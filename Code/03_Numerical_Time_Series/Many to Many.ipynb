{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date Time</th>\n",
       "      <th>p (mbar)</th>\n",
       "      <th>T (degC)</th>\n",
       "      <th>Tpot (K)</th>\n",
       "      <th>Tdew (degC)</th>\n",
       "      <th>rh (%)</th>\n",
       "      <th>VPmax (mbar)</th>\n",
       "      <th>VPact (mbar)</th>\n",
       "      <th>VPdef (mbar)</th>\n",
       "      <th>sh (g/kg)</th>\n",
       "      <th>H2OC (mmol/mol)</th>\n",
       "      <th>rho (g/m**3)</th>\n",
       "      <th>wv (m/s)</th>\n",
       "      <th>max. wv (m/s)</th>\n",
       "      <th>wd (deg)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>01.01.2009 00:10:00</td>\n",
       "      <td>996.52</td>\n",
       "      <td>-8.02</td>\n",
       "      <td>265.40</td>\n",
       "      <td>-8.90</td>\n",
       "      <td>93.3</td>\n",
       "      <td>3.33</td>\n",
       "      <td>3.11</td>\n",
       "      <td>0.22</td>\n",
       "      <td>1.94</td>\n",
       "      <td>3.12</td>\n",
       "      <td>1307.75</td>\n",
       "      <td>1.03</td>\n",
       "      <td>1.75</td>\n",
       "      <td>152.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>01.01.2009 00:20:00</td>\n",
       "      <td>996.57</td>\n",
       "      <td>-8.41</td>\n",
       "      <td>265.01</td>\n",
       "      <td>-9.28</td>\n",
       "      <td>93.4</td>\n",
       "      <td>3.23</td>\n",
       "      <td>3.02</td>\n",
       "      <td>0.21</td>\n",
       "      <td>1.89</td>\n",
       "      <td>3.03</td>\n",
       "      <td>1309.80</td>\n",
       "      <td>0.72</td>\n",
       "      <td>1.50</td>\n",
       "      <td>136.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>01.01.2009 00:30:00</td>\n",
       "      <td>996.53</td>\n",
       "      <td>-8.51</td>\n",
       "      <td>264.91</td>\n",
       "      <td>-9.31</td>\n",
       "      <td>93.9</td>\n",
       "      <td>3.21</td>\n",
       "      <td>3.01</td>\n",
       "      <td>0.20</td>\n",
       "      <td>1.88</td>\n",
       "      <td>3.02</td>\n",
       "      <td>1310.24</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.63</td>\n",
       "      <td>171.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>01.01.2009 00:40:00</td>\n",
       "      <td>996.51</td>\n",
       "      <td>-8.31</td>\n",
       "      <td>265.12</td>\n",
       "      <td>-9.07</td>\n",
       "      <td>94.2</td>\n",
       "      <td>3.26</td>\n",
       "      <td>3.07</td>\n",
       "      <td>0.19</td>\n",
       "      <td>1.92</td>\n",
       "      <td>3.08</td>\n",
       "      <td>1309.19</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.50</td>\n",
       "      <td>198.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>01.01.2009 00:50:00</td>\n",
       "      <td>996.51</td>\n",
       "      <td>-8.27</td>\n",
       "      <td>265.15</td>\n",
       "      <td>-9.04</td>\n",
       "      <td>94.1</td>\n",
       "      <td>3.27</td>\n",
       "      <td>3.08</td>\n",
       "      <td>0.19</td>\n",
       "      <td>1.92</td>\n",
       "      <td>3.09</td>\n",
       "      <td>1309.00</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.63</td>\n",
       "      <td>214.3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Date Time  p (mbar)  T (degC)  Tpot (K)  Tdew (degC)  rh (%)  \\\n",
       "0  01.01.2009 00:10:00    996.52     -8.02    265.40        -8.90    93.3   \n",
       "1  01.01.2009 00:20:00    996.57     -8.41    265.01        -9.28    93.4   \n",
       "2  01.01.2009 00:30:00    996.53     -8.51    264.91        -9.31    93.9   \n",
       "3  01.01.2009 00:40:00    996.51     -8.31    265.12        -9.07    94.2   \n",
       "4  01.01.2009 00:50:00    996.51     -8.27    265.15        -9.04    94.1   \n",
       "\n",
       "   VPmax (mbar)  VPact (mbar)  VPdef (mbar)  sh (g/kg)  H2OC (mmol/mol)  \\\n",
       "0          3.33          3.11          0.22       1.94             3.12   \n",
       "1          3.23          3.02          0.21       1.89             3.03   \n",
       "2          3.21          3.01          0.20       1.88             3.02   \n",
       "3          3.26          3.07          0.19       1.92             3.08   \n",
       "4          3.27          3.08          0.19       1.92             3.09   \n",
       "\n",
       "   rho (g/m**3)  wv (m/s)  max. wv (m/s)  wd (deg)  \n",
       "0       1307.75      1.03           1.75     152.3  \n",
       "1       1309.80      0.72           1.50     136.1  \n",
       "2       1310.24      0.19           0.63     171.6  \n",
       "3       1309.19      0.34           0.50     198.0  \n",
       "4       1309.00      0.32           0.63     214.3  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('temperature.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>p (mbar)</th>\n",
       "      <th>T (degC)</th>\n",
       "      <th>Tpot (K)</th>\n",
       "      <th>Tdew (degC)</th>\n",
       "      <th>rh (%)</th>\n",
       "      <th>VPmax (mbar)</th>\n",
       "      <th>VPact (mbar)</th>\n",
       "      <th>VPdef (mbar)</th>\n",
       "      <th>sh (g/kg)</th>\n",
       "      <th>H2OC (mmol/mol)</th>\n",
       "      <th>rho (g/m**3)</th>\n",
       "      <th>wv (m/s)</th>\n",
       "      <th>max. wv (m/s)</th>\n",
       "      <th>wd (deg)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.900080</td>\n",
       "      <td>-1.931243</td>\n",
       "      <td>-1.982008</td>\n",
       "      <td>-1.862774</td>\n",
       "      <td>1.073091</td>\n",
       "      <td>-1.307358</td>\n",
       "      <td>-1.473784</td>\n",
       "      <td>-0.798768</td>\n",
       "      <td>-1.476283</td>\n",
       "      <td>-1.478172</td>\n",
       "      <td>2.123687</td>\n",
       "      <td>-0.727723</td>\n",
       "      <td>-0.779684</td>\n",
       "      <td>-0.277693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.897721</td>\n",
       "      <td>-1.886062</td>\n",
       "      <td>-1.936218</td>\n",
       "      <td>-1.779038</td>\n",
       "      <td>1.162756</td>\n",
       "      <td>-1.293054</td>\n",
       "      <td>-1.438047</td>\n",
       "      <td>-0.807030</td>\n",
       "      <td>-1.438763</td>\n",
       "      <td>-1.442890</td>\n",
       "      <td>2.074970</td>\n",
       "      <td>-1.281252</td>\n",
       "      <td>-1.260773</td>\n",
       "      <td>-0.114206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.913050</td>\n",
       "      <td>-2.024996</td>\n",
       "      <td>-2.074707</td>\n",
       "      <td>-1.974423</td>\n",
       "      <td>1.085046</td>\n",
       "      <td>-1.334667</td>\n",
       "      <td>-1.519050</td>\n",
       "      <td>-0.802899</td>\n",
       "      <td>-1.521308</td>\n",
       "      <td>-1.522864</td>\n",
       "      <td>2.226299</td>\n",
       "      <td>-1.294276</td>\n",
       "      <td>-1.316614</td>\n",
       "      <td>-0.208614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.941348</td>\n",
       "      <td>-2.023866</td>\n",
       "      <td>-2.075824</td>\n",
       "      <td>-1.973027</td>\n",
       "      <td>1.085046</td>\n",
       "      <td>-1.333367</td>\n",
       "      <td>-1.519050</td>\n",
       "      <td>-0.802899</td>\n",
       "      <td>-1.517556</td>\n",
       "      <td>-1.522864</td>\n",
       "      <td>2.232418</td>\n",
       "      <td>-1.352885</td>\n",
       "      <td>-1.424000</td>\n",
       "      <td>-0.542494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.962572</td>\n",
       "      <td>-2.067919</td>\n",
       "      <td>-2.121614</td>\n",
       "      <td>-2.051181</td>\n",
       "      <td>1.007336</td>\n",
       "      <td>-1.346371</td>\n",
       "      <td>-1.550021</td>\n",
       "      <td>-0.794637</td>\n",
       "      <td>-1.551324</td>\n",
       "      <td>-1.553442</td>\n",
       "      <td>2.285372</td>\n",
       "      <td>-1.333349</td>\n",
       "      <td>-1.368159</td>\n",
       "      <td>0.316385</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    p (mbar)  T (degC)  Tpot (K)  Tdew (degC)    rh (%)  VPmax (mbar)  \\\n",
       "0   0.900080 -1.931243 -1.982008    -1.862774  1.073091     -1.307358   \n",
       "6   0.897721 -1.886062 -1.936218    -1.779038  1.162756     -1.293054   \n",
       "12  0.913050 -2.024996 -2.074707    -1.974423  1.085046     -1.334667   \n",
       "18  0.941348 -2.023866 -2.075824    -1.973027  1.085046     -1.333367   \n",
       "24  0.962572 -2.067919 -2.121614    -2.051181  1.007336     -1.346371   \n",
       "\n",
       "    VPact (mbar)  VPdef (mbar)  sh (g/kg)  H2OC (mmol/mol)  rho (g/m**3)  \\\n",
       "0      -1.473784     -0.798768  -1.476283        -1.478172      2.123687   \n",
       "6      -1.438047     -0.807030  -1.438763        -1.442890      2.074970   \n",
       "12     -1.519050     -0.802899  -1.521308        -1.522864      2.226299   \n",
       "18     -1.519050     -0.802899  -1.517556        -1.522864      2.232418   \n",
       "24     -1.550021     -0.794637  -1.551324        -1.553442      2.285372   \n",
       "\n",
       "    wv (m/s)  max. wv (m/s)  wd (deg)  \n",
       "0  -0.727723      -0.779684 -0.277693  \n",
       "6  -1.281252      -1.260773 -0.114206  \n",
       "12 -1.294276      -1.316614 -0.208614  \n",
       "18 -1.352885      -1.424000 -0.542494  \n",
       "24 -1.333349      -1.368159  0.316385  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.iloc[:200000:6]\n",
    "df = df.drop(columns=['Date Time'])\n",
    "\n",
    "mean = df.mean()\n",
    "std = df.std()\n",
    "df = (df-mean)/std\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(33314, 10, 14) (33314, 10)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "pastDay=10\n",
    "futureDay=10\n",
    "\n",
    "X, y = [], []\n",
    "for i in range(df.shape[0]-futureDay-pastDay):\n",
    "    X.append(np.array(df.iloc[i:i+pastDay]))\n",
    "    y.append(np.array(df.iloc[i+pastDay:i+pastDay+futureDay][\"T (degC)\"]))\n",
    "\n",
    "X, y = np.array(X), np.array(y)\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(29982, 10, 14) (3332, 10, 14) (29982, 10) (3332, 10)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.1, random_state=1)\n",
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(29982, 10, 1) (3332, 10, 1)\n"
     ]
    }
   ],
   "source": [
    "y_train = y_train[:,:,np.newaxis]\n",
    "y_test = y_test[:,:,np.newaxis]\n",
    "print(y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "config = tf.compat.v1.ConfigProto()\n",
    "config.gpu_options.allocator_type = 'BFC' #A \"Best-fit with coalescing\" algorithm, simplified from a version of dlmalloc.\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.8\n",
    "config.gpu_options.allow_growth =True\n",
    "\n",
    "#set_session(tf.compat.v1.Session(config=config)) \n",
    "tf.compat.v1.keras.backend.set_session(tf.compat.v1.Session(config=config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 10, 10)            1000      \n",
      "_________________________________________________________________\n",
      "time_distributed_1 (TimeDist (None, 10, 1)             11        \n",
      "=================================================================\n",
      "Total params: 1,011\n",
      "Trainable params: 1,011\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, TimeDistributed\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(10, input_shape=(X_train.shape[1], X_train.shape[2]),return_sequences=True))\n",
    "model.add(TimeDistributed(Dense(1)))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 26983 samples, validate on 2999 samples\n",
      "Epoch 1/1000\n",
      "26983/26983 [==============================] - 3s 122us/step - loss: 0.5150 - val_loss: 0.2809\n",
      "Epoch 2/1000\n",
      "26983/26983 [==============================] - 3s 96us/step - loss: 0.2562 - val_loss: 0.2390\n",
      "Epoch 3/1000\n",
      "26983/26983 [==============================] - 3s 93us/step - loss: 0.2274 - val_loss: 0.2202\n",
      "Epoch 4/1000\n",
      "26983/26983 [==============================] - 3s 94us/step - loss: 0.2122 - val_loss: 0.2096\n",
      "Epoch 5/1000\n",
      "26983/26983 [==============================] - 3s 99us/step - loss: 0.2029 - val_loss: 0.2028\n",
      "Epoch 6/1000\n",
      "26983/26983 [==============================] - 3s 99us/step - loss: 0.1970 - val_loss: 0.1982\n",
      "Epoch 7/1000\n",
      "26983/26983 [==============================] - 3s 107us/step - loss: 0.1929 - val_loss: 0.1939\n",
      "Epoch 8/1000\n",
      "26983/26983 [==============================] - 3s 96us/step - loss: 0.1899 - val_loss: 0.1910\n",
      "Epoch 9/1000\n",
      "26983/26983 [==============================] - 3s 94us/step - loss: 0.1877 - val_loss: 0.1890\n",
      "Epoch 10/1000\n",
      "26983/26983 [==============================] - 3s 93us/step - loss: 0.1856 - val_loss: 0.1869\n",
      "Epoch 11/1000\n",
      "26983/26983 [==============================] - 2s 91us/step - loss: 0.1838 - val_loss: 0.1851\n",
      "Epoch 12/1000\n",
      "26983/26983 [==============================] - 2s 92us/step - loss: 0.1823 - val_loss: 0.1834\n",
      "Epoch 13/1000\n",
      "26983/26983 [==============================] - 2s 92us/step - loss: 0.1806 - val_loss: 0.1818\n",
      "Epoch 14/1000\n",
      "26983/26983 [==============================] - 3s 96us/step - loss: 0.1790 - val_loss: 0.1796\n",
      "Epoch 15/1000\n",
      "26983/26983 [==============================] - 3s 113us/step - loss: 0.1774 - val_loss: 0.1798\n",
      "Epoch 16/1000\n",
      "26983/26983 [==============================] - 3s 105us/step - loss: 0.1757 - val_loss: 0.1764\n",
      "Epoch 17/1000\n",
      "26983/26983 [==============================] - 3s 101us/step - loss: 0.1739 - val_loss: 0.1745\n",
      "Epoch 18/1000\n",
      "26983/26983 [==============================] - 3s 109us/step - loss: 0.1724 - val_loss: 0.1735\n",
      "Epoch 19/1000\n",
      "26983/26983 [==============================] - 3s 93us/step - loss: 0.1711 - val_loss: 0.1724\n",
      "Epoch 20/1000\n",
      "26983/26983 [==============================] - 3s 98us/step - loss: 0.1697 - val_loss: 0.1713\n",
      "Epoch 21/1000\n",
      "26983/26983 [==============================] - 3s 94us/step - loss: 0.1687 - val_loss: 0.1705\n",
      "Epoch 22/1000\n",
      "26983/26983 [==============================] - 2s 92us/step - loss: 0.1677 - val_loss: 0.1684\n",
      "Epoch 23/1000\n",
      "26983/26983 [==============================] - 3s 94us/step - loss: 0.1669 - val_loss: 0.1690\n",
      "Epoch 24/1000\n",
      "26983/26983 [==============================] - 2s 90us/step - loss: 0.1662 - val_loss: 0.1675\n",
      "Epoch 25/1000\n",
      "26983/26983 [==============================] - 2s 89us/step - loss: 0.1655 - val_loss: 0.1672\n",
      "Epoch 26/1000\n",
      "26983/26983 [==============================] - 2s 89us/step - loss: 0.1648 - val_loss: 0.1666\n",
      "Epoch 27/1000\n",
      "26983/26983 [==============================] - 2s 91us/step - loss: 0.1641 - val_loss: 0.1657\n",
      "Epoch 28/1000\n",
      "26983/26983 [==============================] - 3s 93us/step - loss: 0.1637 - val_loss: 0.1647\n",
      "Epoch 29/1000\n",
      "26983/26983 [==============================] - 2s 92us/step - loss: 0.1631 - val_loss: 0.1669\n",
      "Epoch 30/1000\n",
      "26983/26983 [==============================] - 3s 94us/step - loss: 0.1630 - val_loss: 0.1642\n",
      "Epoch 31/1000\n",
      "26983/26983 [==============================] - 3s 94us/step - loss: 0.1621 - val_loss: 0.1636\n",
      "Epoch 32/1000\n",
      "26983/26983 [==============================] - 3s 94us/step - loss: 0.1620 - val_loss: 0.1635\n",
      "Epoch 33/1000\n",
      "26983/26983 [==============================] - 2s 91us/step - loss: 0.1616 - val_loss: 0.1632\n",
      "Epoch 34/1000\n",
      "26983/26983 [==============================] - 2s 90us/step - loss: 0.1611 - val_loss: 0.1644\n",
      "Epoch 35/1000\n",
      "26983/26983 [==============================] - 2s 91us/step - loss: 0.1608 - val_loss: 0.1624\n",
      "Epoch 36/1000\n",
      "26983/26983 [==============================] - 3s 95us/step - loss: 0.1609 - val_loss: 0.1618\n",
      "Epoch 37/1000\n",
      "26983/26983 [==============================] - 3s 94us/step - loss: 0.1604 - val_loss: 0.1617\n",
      "Epoch 38/1000\n",
      "26983/26983 [==============================] - 3s 95us/step - loss: 0.1603 - val_loss: 0.1610\n",
      "Epoch 39/1000\n",
      "26983/26983 [==============================] - 3s 94us/step - loss: 0.1597 - val_loss: 0.1640\n",
      "Epoch 40/1000\n",
      "26983/26983 [==============================] - 3s 95us/step - loss: 0.1597 - val_loss: 0.1612\n",
      "Epoch 41/1000\n",
      "26983/26983 [==============================] - 2s 91us/step - loss: 0.1592 - val_loss: 0.1618\n",
      "Epoch 42/1000\n",
      "26983/26983 [==============================] - 3s 93us/step - loss: 0.1589 - val_loss: 0.1609\n",
      "Epoch 43/1000\n",
      "26983/26983 [==============================] - 2s 92us/step - loss: 0.1587 - val_loss: 0.1601\n",
      "Epoch 44/1000\n",
      "26983/26983 [==============================] - 2s 90us/step - loss: 0.1584 - val_loss: 0.1600\n",
      "Epoch 45/1000\n",
      "26983/26983 [==============================] - 2s 90us/step - loss: 0.1583 - val_loss: 0.1595\n",
      "Epoch 46/1000\n",
      "26983/26983 [==============================] - 2s 90us/step - loss: 0.1578 - val_loss: 0.1589\n",
      "Epoch 47/1000\n",
      "26983/26983 [==============================] - 2s 89us/step - loss: 0.1581 - val_loss: 0.1590\n",
      "Epoch 48/1000\n",
      "26983/26983 [==============================] - 2s 90us/step - loss: 0.1575 - val_loss: 0.1581\n",
      "Epoch 49/1000\n",
      "26983/26983 [==============================] - 2s 92us/step - loss: 0.1573 - val_loss: 0.1592\n",
      "Epoch 50/1000\n",
      "26983/26983 [==============================] - 3s 94us/step - loss: 0.1571 - val_loss: 0.1584\n",
      "Epoch 51/1000\n",
      "26983/26983 [==============================] - 3s 97us/step - loss: 0.1569 - val_loss: 0.1580\n",
      "Epoch 52/1000\n",
      "26983/26983 [==============================] - 3s 95us/step - loss: 0.1567 - val_loss: 0.1576\n",
      "Epoch 53/1000\n",
      "26983/26983 [==============================] - 2s 90us/step - loss: 0.1565 - val_loss: 0.1574\n",
      "Epoch 54/1000\n",
      "26983/26983 [==============================] - 3s 93us/step - loss: 0.1565 - val_loss: 0.1581\n",
      "Epoch 55/1000\n",
      "26983/26983 [==============================] - 2s 91us/step - loss: 0.1561 - val_loss: 0.1577\n",
      "Epoch 56/1000\n",
      "26983/26983 [==============================] - 3s 95us/step - loss: 0.1560 - val_loss: 0.1572\n",
      "Epoch 57/1000\n",
      "26983/26983 [==============================] - 3s 94us/step - loss: 0.1557 - val_loss: 0.1566\n",
      "Epoch 58/1000\n",
      "26983/26983 [==============================] - 2s 91us/step - loss: 0.1556 - val_loss: 0.1587\n",
      "Epoch 59/1000\n",
      "26983/26983 [==============================] - 3s 93us/step - loss: 0.1554 - val_loss: 0.1575\n",
      "Epoch 60/1000\n",
      "26983/26983 [==============================] - 2s 91us/step - loss: 0.1553 - val_loss: 0.1561\n",
      "Epoch 61/1000\n",
      "26983/26983 [==============================] - 3s 94us/step - loss: 0.1551 - val_loss: 0.1581\n",
      "Epoch 62/1000\n",
      "26983/26983 [==============================] - 3s 96us/step - loss: 0.1554 - val_loss: 0.1571\n",
      "Epoch 63/1000\n",
      "26983/26983 [==============================] - 3s 93us/step - loss: 0.1547 - val_loss: 0.1558\n",
      "Epoch 64/1000\n",
      "26983/26983 [==============================] - 2s 91us/step - loss: 0.1548 - val_loss: 0.1562\n",
      "Epoch 65/1000\n",
      "26983/26983 [==============================] - 3s 95us/step - loss: 0.1547 - val_loss: 0.1568\n",
      "Epoch 66/1000\n",
      "26983/26983 [==============================] - 3s 100us/step - loss: 0.1544 - val_loss: 0.1561\n",
      "Epoch 67/1000\n",
      "26983/26983 [==============================] - 3s 102us/step - loss: 0.1545 - val_loss: 0.1555\n",
      "Epoch 68/1000\n",
      "26983/26983 [==============================] - 3s 110us/step - loss: 0.1544 - val_loss: 0.1553\n",
      "Epoch 69/1000\n",
      "26983/26983 [==============================] - 2s 92us/step - loss: 0.1541 - val_loss: 0.1556\n",
      "Epoch 70/1000\n",
      "26983/26983 [==============================] - 3s 93us/step - loss: 0.1539 - val_loss: 0.1554\n",
      "Epoch 71/1000\n",
      "26983/26983 [==============================] - 3s 96us/step - loss: 0.1535 - val_loss: 0.1547\n",
      "Epoch 72/1000\n",
      "26983/26983 [==============================] - 3s 93us/step - loss: 0.1538 - val_loss: 0.1575\n",
      "Epoch 73/1000\n",
      "26983/26983 [==============================] - 3s 94us/step - loss: 0.1536 - val_loss: 0.1545\n",
      "Epoch 74/1000\n",
      "26983/26983 [==============================] - 3s 94us/step - loss: 0.1533 - val_loss: 0.1545\n",
      "Epoch 75/1000\n",
      "26983/26983 [==============================] - 2s 90us/step - loss: 0.1532 - val_loss: 0.1542\n",
      "Epoch 76/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26983/26983 [==============================] - 2s 90us/step - loss: 0.1532 - val_loss: 0.1544\n",
      "Epoch 77/1000\n",
      "26983/26983 [==============================] - 2s 92us/step - loss: 0.1533 - val_loss: 0.1549\n",
      "Epoch 78/1000\n",
      "26983/26983 [==============================] - 2s 91us/step - loss: 0.1530 - val_loss: 0.1536\n",
      "Epoch 79/1000\n",
      "26983/26983 [==============================] - 3s 99us/step - loss: 0.1531 - val_loss: 0.1538\n",
      "Epoch 80/1000\n",
      "26983/26983 [==============================] - 3s 94us/step - loss: 0.1528 - val_loss: 0.1534\n",
      "Epoch 81/1000\n",
      "26983/26983 [==============================] - 3s 97us/step - loss: 0.1527 - val_loss: 0.1530\n",
      "Epoch 82/1000\n",
      "26983/26983 [==============================] - 2s 90us/step - loss: 0.1525 - val_loss: 0.1530\n",
      "Epoch 83/1000\n",
      "26983/26983 [==============================] - 2s 93us/step - loss: 0.1526 - val_loss: 0.1527\n",
      "Epoch 84/1000\n",
      "26983/26983 [==============================] - 3s 97us/step - loss: 0.1522 - val_loss: 0.1531\n",
      "Epoch 85/1000\n",
      "26983/26983 [==============================] - 2s 90us/step - loss: 0.1520 - val_loss: 0.1533\n",
      "Epoch 86/1000\n",
      "26983/26983 [==============================] - 2s 90us/step - loss: 0.1523 - val_loss: 0.1526\n",
      "Epoch 87/1000\n",
      "26983/26983 [==============================] - 3s 97us/step - loss: 0.1521 - val_loss: 0.1531\n",
      "Epoch 88/1000\n",
      "26983/26983 [==============================] - 2s 91us/step - loss: 0.1519 - val_loss: 0.1525\n",
      "Epoch 89/1000\n",
      "26983/26983 [==============================] - 3s 93us/step - loss: 0.1519 - val_loss: 0.1526\n",
      "Epoch 90/1000\n",
      "26983/26983 [==============================] - 3s 93us/step - loss: 0.1518 - val_loss: 0.1530\n",
      "Epoch 91/1000\n",
      "26983/26983 [==============================] - 3s 95us/step - loss: 0.1516 - val_loss: 0.1522\n",
      "Epoch 92/1000\n",
      "26983/26983 [==============================] - 2s 91us/step - loss: 0.1514 - val_loss: 0.1528\n",
      "Epoch 93/1000\n",
      "26983/26983 [==============================] - 3s 101us/step - loss: 0.1517 - val_loss: 0.1528\n",
      "Epoch 94/1000\n",
      "26983/26983 [==============================] - 3s 96us/step - loss: 0.1512 - val_loss: 0.1525\n",
      "Epoch 95/1000\n",
      "26983/26983 [==============================] - 2s 90us/step - loss: 0.1514 - val_loss: 0.1531\n",
      "Epoch 96/1000\n",
      "26983/26983 [==============================] - 2s 90us/step - loss: 0.1514 - val_loss: 0.1522\n",
      "Epoch 97/1000\n",
      "26983/26983 [==============================] - 2s 91us/step - loss: 0.1512 - val_loss: 0.1534\n",
      "Epoch 98/1000\n",
      "26983/26983 [==============================] - 3s 96us/step - loss: 0.1513 - val_loss: 0.1530\n",
      "Epoch 99/1000\n",
      "26983/26983 [==============================] - 3s 102us/step - loss: 0.1508 - val_loss: 0.1527\n",
      "Epoch 100/1000\n",
      "26983/26983 [==============================] - 3s 97us/step - loss: 0.1511 - val_loss: 0.1515\n",
      "Epoch 101/1000\n",
      "26983/26983 [==============================] - 3s 98us/step - loss: 0.1510 - val_loss: 0.1521\n",
      "Epoch 102/1000\n",
      "26983/26983 [==============================] - 3s 95us/step - loss: 0.1509 - val_loss: 0.1521\n",
      "Epoch 103/1000\n",
      "26983/26983 [==============================] - 3s 98us/step - loss: 0.1507 - val_loss: 0.1516\n",
      "Epoch 104/1000\n",
      "26983/26983 [==============================] - 3s 95us/step - loss: 0.1504 - val_loss: 0.1514\n",
      "Epoch 105/1000\n",
      "26983/26983 [==============================] - 3s 93us/step - loss: 0.1506 - val_loss: 0.1521\n",
      "Epoch 106/1000\n",
      "26983/26983 [==============================] - 3s 99us/step - loss: 0.1507 - val_loss: 0.1523\n",
      "Epoch 107/1000\n",
      "26983/26983 [==============================] - 2s 90us/step - loss: 0.1505 - val_loss: 0.1511\n",
      "Epoch 108/1000\n",
      "26983/26983 [==============================] - 2s 90us/step - loss: 0.1504 - val_loss: 0.1508\n",
      "Epoch 109/1000\n",
      "26983/26983 [==============================] - 2s 92us/step - loss: 0.1504 - val_loss: 0.1518\n",
      "Epoch 110/1000\n",
      "26983/26983 [==============================] - 2s 91us/step - loss: 0.1504 - val_loss: 0.1516\n",
      "Epoch 111/1000\n",
      "26983/26983 [==============================] - 2s 91us/step - loss: 0.1503 - val_loss: 0.1514\n",
      "Epoch 112/1000\n",
      "26983/26983 [==============================] - 3s 93us/step - loss: 0.1504 - val_loss: 0.1514\n",
      "Epoch 113/1000\n",
      "26983/26983 [==============================] - 2s 91us/step - loss: 0.1503 - val_loss: 0.1519\n",
      "Epoch 114/1000\n",
      "26983/26983 [==============================] - 2s 92us/step - loss: 0.1501 - val_loss: 0.1514\n",
      "Epoch 115/1000\n",
      "26983/26983 [==============================] - 2s 91us/step - loss: 0.1499 - val_loss: 0.1503\n",
      "Epoch 116/1000\n",
      "26983/26983 [==============================] - 3s 102us/step - loss: 0.1498 - val_loss: 0.1509\n",
      "Epoch 117/1000\n",
      "26983/26983 [==============================] - 3s 94us/step - loss: 0.1500 - val_loss: 0.1512\n",
      "Epoch 118/1000\n",
      "26983/26983 [==============================] - 2s 92us/step - loss: 0.1499 - val_loss: 0.1508\n",
      "Epoch 119/1000\n",
      "26983/26983 [==============================] - 2s 90us/step - loss: 0.1497 - val_loss: 0.1523\n",
      "Epoch 120/1000\n",
      "26983/26983 [==============================] - 2s 90us/step - loss: 0.1498 - val_loss: 0.1517\n",
      "Epoch 121/1000\n",
      "26983/26983 [==============================] - 3s 102us/step - loss: 0.1497 - val_loss: 0.1514\n",
      "Epoch 122/1000\n",
      "26983/26983 [==============================] - 3s 104us/step - loss: 0.1496 - val_loss: 0.1506\n",
      "Epoch 123/1000\n",
      "26983/26983 [==============================] - 3s 106us/step - loss: 0.1495 - val_loss: 0.1510\n",
      "Epoch 124/1000\n",
      "26983/26983 [==============================] - 4s 135us/step - loss: 0.1495 - val_loss: 0.1498\n",
      "Epoch 125/1000\n",
      "26983/26983 [==============================] - 3s 107us/step - loss: 0.1495 - val_loss: 0.1505\n",
      "Epoch 126/1000\n",
      "26983/26983 [==============================] - 3s 97us/step - loss: 0.1496 - val_loss: 0.1534\n",
      "Epoch 127/1000\n",
      "26983/26983 [==============================] - 3s 100us/step - loss: 0.1495 - val_loss: 0.1515\n",
      "Epoch 128/1000\n",
      "26983/26983 [==============================] - 3s 95us/step - loss: 0.1496 - val_loss: 0.1504\n",
      "Epoch 129/1000\n",
      "26983/26983 [==============================] - 3s 93us/step - loss: 0.1496 - val_loss: 0.1502\n",
      "Epoch 130/1000\n",
      "26983/26983 [==============================] - 3s 112us/step - loss: 0.1493 - val_loss: 0.1507\n",
      "Epoch 131/1000\n",
      "26983/26983 [==============================] - 3s 101us/step - loss: 0.1494 - val_loss: 0.1506\n",
      "Epoch 132/1000\n",
      "26983/26983 [==============================] - 3s 96us/step - loss: 0.1492 - val_loss: 0.1508\n",
      "Epoch 133/1000\n",
      "26983/26983 [==============================] - 3s 99us/step - loss: 0.1491 - val_loss: 0.1508\n",
      "Epoch 134/1000\n",
      "26983/26983 [==============================] - 3s 100us/step - loss: 0.1492 - val_loss: 0.1501\n",
      "Epoch 135/1000\n",
      "26983/26983 [==============================] - 3s 99us/step - loss: 0.1493 - val_loss: 0.1525\n",
      "Epoch 136/1000\n",
      "26983/26983 [==============================] - 3s 105us/step - loss: 0.1492 - val_loss: 0.1499\n",
      "Epoch 137/1000\n",
      "26983/26983 [==============================] - 3s 94us/step - loss: 0.1491 - val_loss: 0.1500\n",
      "Epoch 138/1000\n",
      "26983/26983 [==============================] - 3s 100us/step - loss: 0.1493 - val_loss: 0.1516\n",
      "Epoch 139/1000\n",
      "26983/26983 [==============================] - 3s 97us/step - loss: 0.1490 - val_loss: 0.1504\n",
      "Epoch 140/1000\n",
      "26983/26983 [==============================] - 3s 99us/step - loss: 0.1490 - val_loss: 0.1503\n",
      "Epoch 141/1000\n",
      "26983/26983 [==============================] - 3s 99us/step - loss: 0.1489 - val_loss: 0.1494\n",
      "Epoch 142/1000\n",
      "26983/26983 [==============================] - 3s 107us/step - loss: 0.1488 - val_loss: 0.1509\n",
      "Epoch 143/1000\n",
      "26983/26983 [==============================] - 3s 99us/step - loss: 0.1490 - val_loss: 0.1497\n",
      "Epoch 144/1000\n",
      "26983/26983 [==============================] - 3s 99us/step - loss: 0.1488 - val_loss: 0.1496\n",
      "Epoch 145/1000\n",
      "26983/26983 [==============================] - 3s 99us/step - loss: 0.1490 - val_loss: 0.1503\n",
      "Epoch 146/1000\n",
      "26983/26983 [==============================] - 2s 92us/step - loss: 0.1488 - val_loss: 0.1505\n",
      "Epoch 147/1000\n",
      "26983/26983 [==============================] - 3s 104us/step - loss: 0.1487 - val_loss: 0.1497\n",
      "Epoch 148/1000\n",
      "26983/26983 [==============================] - 3s 100us/step - loss: 0.1486 - val_loss: 0.1502\n",
      "Epoch 149/1000\n",
      "26983/26983 [==============================] - 3s 101us/step - loss: 0.1487 - val_loss: 0.1492\n",
      "Epoch 150/1000\n",
      "26983/26983 [==============================] - 3s 107us/step - loss: 0.1486 - val_loss: 0.1497\n",
      "Epoch 151/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26983/26983 [==============================] - 3s 115us/step - loss: 0.1487 - val_loss: 0.1505\n",
      "Epoch 152/1000\n",
      "26983/26983 [==============================] - 3s 113us/step - loss: 0.1486 - val_loss: 0.1494\n",
      "Epoch 153/1000\n",
      "26983/26983 [==============================] - 4s 131us/step - loss: 0.1484 - val_loss: 0.1501\n",
      "Epoch 154/1000\n",
      "26983/26983 [==============================] - 3s 105us/step - loss: 0.1484 - val_loss: 0.1505\n",
      "Epoch 155/1000\n",
      "26983/26983 [==============================] - 3s 98us/step - loss: 0.1483 - val_loss: 0.1485\n",
      "Epoch 156/1000\n",
      "26983/26983 [==============================] - 3s 95us/step - loss: 0.1485 - val_loss: 0.1498\n",
      "Epoch 157/1000\n",
      "26983/26983 [==============================] - 3s 95us/step - loss: 0.1485 - val_loss: 0.1491\n",
      "Epoch 158/1000\n",
      "26983/26983 [==============================] - 2s 91us/step - loss: 0.1485 - val_loss: 0.1492\n",
      "Epoch 159/1000\n",
      "26983/26983 [==============================] - 3s 98us/step - loss: 0.1487 - val_loss: 0.1487\n",
      "Epoch 160/1000\n",
      "26983/26983 [==============================] - 3s 97us/step - loss: 0.1483 - val_loss: 0.1497\n",
      "Epoch 161/1000\n",
      "26983/26983 [==============================] - 3s 101us/step - loss: 0.1483 - val_loss: 0.1489\n",
      "Epoch 162/1000\n",
      "26983/26983 [==============================] - 3s 100us/step - loss: 0.1481 - val_loss: 0.1495\n",
      "Epoch 163/1000\n",
      "26983/26983 [==============================] - 2s 91us/step - loss: 0.1484 - val_loss: 0.1486\n",
      "Epoch 164/1000\n",
      "26983/26983 [==============================] - 2s 92us/step - loss: 0.1480 - val_loss: 0.1501\n",
      "Epoch 165/1000\n",
      "26983/26983 [==============================] - 3s 94us/step - loss: 0.1481 - val_loss: 0.1499\n",
      "Epoch 166/1000\n",
      "26983/26983 [==============================] - 2s 92us/step - loss: 0.1482 - val_loss: 0.1497\n",
      "Epoch 167/1000\n",
      "26983/26983 [==============================] - 3s 98us/step - loss: 0.1481 - val_loss: 0.1485\n",
      "Epoch 168/1000\n",
      "26983/26983 [==============================] - 2s 90us/step - loss: 0.1480 - val_loss: 0.1490\n",
      "Epoch 169/1000\n",
      "26983/26983 [==============================] - 2s 90us/step - loss: 0.1481 - val_loss: 0.1496\n",
      "Epoch 170/1000\n",
      "26983/26983 [==============================] - 2s 91us/step - loss: 0.1478 - val_loss: 0.1503\n",
      "Epoch 171/1000\n",
      "26983/26983 [==============================] - 3s 93us/step - loss: 0.1484 - val_loss: 0.1482\n",
      "Epoch 172/1000\n",
      "26983/26983 [==============================] - 3s 98us/step - loss: 0.1479 - val_loss: 0.1491\n",
      "Epoch 173/1000\n",
      "26983/26983 [==============================] - 3s 97us/step - loss: 0.1477 - val_loss: 0.1486\n",
      "Epoch 174/1000\n",
      "26983/26983 [==============================] - 2s 91us/step - loss: 0.1480 - val_loss: 0.1491\n",
      "Epoch 175/1000\n",
      "26983/26983 [==============================] - 3s 94us/step - loss: 0.1479 - val_loss: 0.1492\n",
      "Epoch 176/1000\n",
      "26983/26983 [==============================] - 2s 91us/step - loss: 0.1479 - val_loss: 0.1482\n",
      "Epoch 177/1000\n",
      "26983/26983 [==============================] - 3s 97us/step - loss: 0.1478 - val_loss: 0.1502\n",
      "Epoch 178/1000\n",
      "26983/26983 [==============================] - 3s 95us/step - loss: 0.1478 - val_loss: 0.1484\n",
      "Epoch 179/1000\n",
      "26983/26983 [==============================] - 3s 94us/step - loss: 0.1477 - val_loss: 0.1487\n",
      "Epoch 180/1000\n",
      "26983/26983 [==============================] - 3s 100us/step - loss: 0.1481 - val_loss: 0.1489\n",
      "Epoch 181/1000\n",
      "26983/26983 [==============================] - 3s 104us/step - loss: 0.1479 - val_loss: 0.1483\n",
      "Epoch 182/1000\n",
      "26983/26983 [==============================] - 3s 95us/step - loss: 0.1476 - val_loss: 0.1486\n",
      "Epoch 183/1000\n",
      "26983/26983 [==============================] - 3s 96us/step - loss: 0.1477 - val_loss: 0.1487\n",
      "Epoch 184/1000\n",
      "26983/26983 [==============================] - 3s 94us/step - loss: 0.1476 - val_loss: 0.1483\n",
      "Epoch 185/1000\n",
      "26983/26983 [==============================] - 3s 98us/step - loss: 0.1476 - val_loss: 0.1489\n",
      "Epoch 186/1000\n",
      "26983/26983 [==============================] - 3s 104us/step - loss: 0.1477 - val_loss: 0.1489\n",
      "Epoch 187/1000\n",
      "26983/26983 [==============================] - 3s 103us/step - loss: 0.1476 - val_loss: 0.1486\n",
      "Epoch 188/1000\n",
      "26983/26983 [==============================] - 3s 102us/step - loss: 0.1476 - val_loss: 0.1479\n",
      "Epoch 189/1000\n",
      "26983/26983 [==============================] - 3s 108us/step - loss: 0.1475 - val_loss: 0.1482\n",
      "Epoch 190/1000\n",
      "26983/26983 [==============================] - 3s 108us/step - loss: 0.1475 - val_loss: 0.1489\n",
      "Epoch 191/1000\n",
      "26983/26983 [==============================] - 3s 93us/step - loss: 0.1475 - val_loss: 0.1481\n",
      "Epoch 192/1000\n",
      "26983/26983 [==============================] - 3s 98us/step - loss: 0.1476 - val_loss: 0.1480\n",
      "Epoch 193/1000\n",
      "26983/26983 [==============================] - 2s 91us/step - loss: 0.1475 - val_loss: 0.1483\n",
      "Epoch 194/1000\n",
      "26983/26983 [==============================] - 2s 90us/step - loss: 0.1475 - val_loss: 0.1490\n",
      "Epoch 195/1000\n",
      "26983/26983 [==============================] - 2s 90us/step - loss: 0.1473 - val_loss: 0.1474\n",
      "Epoch 196/1000\n",
      "26983/26983 [==============================] - 2s 92us/step - loss: 0.1473 - val_loss: 0.1482\n",
      "Epoch 197/1000\n",
      "26983/26983 [==============================] - 2s 90us/step - loss: 0.1472 - val_loss: 0.1476\n",
      "Epoch 198/1000\n",
      "26983/26983 [==============================] - 2s 91us/step - loss: 0.1475 - val_loss: 0.1493\n",
      "Epoch 199/1000\n",
      "26983/26983 [==============================] - 2s 90us/step - loss: 0.1473 - val_loss: 0.1509\n",
      "Epoch 200/1000\n",
      "26983/26983 [==============================] - 2s 91us/step - loss: 0.1474 - val_loss: 0.1478\n",
      "Epoch 201/1000\n",
      "26983/26983 [==============================] - 2s 90us/step - loss: 0.1471 - val_loss: 0.1485\n",
      "Epoch 202/1000\n",
      "26983/26983 [==============================] - 2s 91us/step - loss: 0.1472 - val_loss: 0.1477\n",
      "Epoch 203/1000\n",
      "26983/26983 [==============================] - 2s 92us/step - loss: 0.1471 - val_loss: 0.1477\n",
      "Epoch 204/1000\n",
      "26983/26983 [==============================] - 2s 91us/step - loss: 0.1474 - val_loss: 0.1472\n",
      "Epoch 205/1000\n",
      "26983/26983 [==============================] - 3s 94us/step - loss: 0.1468 - val_loss: 0.1476\n",
      "Epoch 206/1000\n",
      "26983/26983 [==============================] - 2s 92us/step - loss: 0.1474 - val_loss: 0.1478\n",
      "Epoch 207/1000\n",
      "26983/26983 [==============================] - 3s 94us/step - loss: 0.1472 - val_loss: 0.1478\n",
      "Epoch 208/1000\n",
      "26983/26983 [==============================] - 3s 99us/step - loss: 0.1470 - val_loss: 0.1482\n",
      "Epoch 209/1000\n",
      "26983/26983 [==============================] - 3s 95us/step - loss: 0.1471 - val_loss: 0.1477\n",
      "Epoch 210/1000\n",
      "26983/26983 [==============================] - 3s 101us/step - loss: 0.1469 - val_loss: 0.1479\n",
      "Epoch 211/1000\n",
      "26983/26983 [==============================] - 2s 91us/step - loss: 0.1469 - val_loss: 0.1476\n",
      "Epoch 212/1000\n",
      "26983/26983 [==============================] - 3s 103us/step - loss: 0.1471 - val_loss: 0.1469\n",
      "Epoch 213/1000\n",
      "26983/26983 [==============================] - 3s 94us/step - loss: 0.1472 - val_loss: 0.1472\n",
      "Epoch 214/1000\n",
      "26983/26983 [==============================] - 3s 112us/step - loss: 0.1470 - val_loss: 0.1486\n",
      "Epoch 215/1000\n",
      "26983/26983 [==============================] - 3s 107us/step - loss: 0.1468 - val_loss: 0.1482\n",
      "Epoch 216/1000\n",
      "26983/26983 [==============================] - 3s 105us/step - loss: 0.1468 - val_loss: 0.1480\n",
      "Epoch 217/1000\n",
      "26983/26983 [==============================] - 3s 107us/step - loss: 0.1470 - val_loss: 0.1471\n",
      "Epoch 218/1000\n",
      "26983/26983 [==============================] - 3s 114us/step - loss: 0.1470 - val_loss: 0.1473\n",
      "Epoch 219/1000\n",
      "26983/26983 [==============================] - 3s 98us/step - loss: 0.1467 - val_loss: 0.1486\n",
      "Epoch 220/1000\n",
      "26983/26983 [==============================] - 2s 90us/step - loss: 0.1468 - val_loss: 0.1480\n",
      "Epoch 221/1000\n",
      "26983/26983 [==============================] - 2s 90us/step - loss: 0.1469 - val_loss: 0.1479\n",
      "Epoch 222/1000\n",
      "26983/26983 [==============================] - 2s 92us/step - loss: 0.1471 - val_loss: 0.1480\n",
      "Epoch 223/1000\n",
      "26983/26983 [==============================] - 2s 90us/step - loss: 0.1467 - val_loss: 0.1475\n",
      "Epoch 224/1000\n",
      "26983/26983 [==============================] - 2s 89us/step - loss: 0.1466 - val_loss: 0.1476\n",
      "Epoch 225/1000\n",
      "26983/26983 [==============================] - 2s 90us/step - loss: 0.1468 - val_loss: 0.1475\n",
      "Epoch 226/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26983/26983 [==============================] - 3s 95us/step - loss: 0.1466 - val_loss: 0.1470\n",
      "Epoch 227/1000\n",
      "26983/26983 [==============================] - 3s 102us/step - loss: 0.1467 - val_loss: 0.1474\n",
      "Epoch 228/1000\n",
      "26983/26983 [==============================] - 2s 89us/step - loss: 0.1467 - val_loss: 0.1472\n",
      "Epoch 229/1000\n",
      "26983/26983 [==============================] - 2s 91us/step - loss: 0.1466 - val_loss: 0.1476\n",
      "Epoch 230/1000\n",
      "26983/26983 [==============================] - 3s 95us/step - loss: 0.1467 - val_loss: 0.1478\n",
      "Epoch 231/1000\n",
      "26983/26983 [==============================] - 3s 94us/step - loss: 0.1467 - val_loss: 0.1472\n",
      "Epoch 232/1000\n",
      "26983/26983 [==============================] - 2s 90us/step - loss: 0.1466 - val_loss: 0.1472\n",
      "Epoch 233/1000\n",
      "26983/26983 [==============================] - 3s 99us/step - loss: 0.1466 - val_loss: 0.1480\n",
      "Epoch 234/1000\n",
      "26983/26983 [==============================] - 2s 91us/step - loss: 0.1467 - val_loss: 0.1473\n",
      "Epoch 235/1000\n",
      "26983/26983 [==============================] - 3s 95us/step - loss: 0.1464 - val_loss: 0.1476\n",
      "Epoch 236/1000\n",
      "26983/26983 [==============================] - 3s 95us/step - loss: 0.1465 - val_loss: 0.1469\n",
      "Epoch 237/1000\n",
      "26983/26983 [==============================] - 3s 94us/step - loss: 0.1465 - val_loss: 0.1475\n",
      "Epoch 238/1000\n",
      "26983/26983 [==============================] - 2s 91us/step - loss: 0.1463 - val_loss: 0.1466\n",
      "Epoch 239/1000\n",
      "26983/26983 [==============================] - 3s 98us/step - loss: 0.1465 - val_loss: 0.1473\n",
      "Epoch 240/1000\n",
      "26983/26983 [==============================] - 2s 91us/step - loss: 0.1466 - val_loss: 0.1465\n",
      "Epoch 241/1000\n",
      "26983/26983 [==============================] - 2s 91us/step - loss: 0.1466 - val_loss: 0.1467\n",
      "Epoch 242/1000\n",
      "26983/26983 [==============================] - 2s 90us/step - loss: 0.1464 - val_loss: 0.1460\n",
      "Epoch 243/1000\n",
      "26983/26983 [==============================] - 2s 89us/step - loss: 0.1462 - val_loss: 0.1471\n",
      "Epoch 244/1000\n",
      "26983/26983 [==============================] - 2s 91us/step - loss: 0.1464 - val_loss: 0.1469\n",
      "Epoch 245/1000\n",
      "26983/26983 [==============================] - 2s 89us/step - loss: 0.1463 - val_loss: 0.1465\n",
      "Epoch 246/1000\n",
      "26983/26983 [==============================] - 2s 91us/step - loss: 0.1465 - val_loss: 0.1466\n",
      "Epoch 247/1000\n",
      "26983/26983 [==============================] - 2s 90us/step - loss: 0.1462 - val_loss: 0.1479\n",
      "Epoch 248/1000\n",
      "26983/26983 [==============================] - 2s 88us/step - loss: 0.1463 - val_loss: 0.1465\n",
      "Epoch 249/1000\n",
      "26983/26983 [==============================] - 3s 95us/step - loss: 0.1463 - val_loss: 0.1479\n",
      "Epoch 250/1000\n",
      "26983/26983 [==============================] - 2s 92us/step - loss: 0.1463 - val_loss: 0.1485\n",
      "Epoch 251/1000\n",
      "26983/26983 [==============================] - 3s 98us/step - loss: 0.1462 - val_loss: 0.1466\n",
      "Epoch 252/1000\n",
      "26983/26983 [==============================] - 3s 98us/step - loss: 0.1466 - val_loss: 0.1477\n",
      "Epoch 253/1000\n",
      "26983/26983 [==============================] - 3s 98us/step - loss: 0.1463 - val_loss: 0.1473\n",
      "Epoch 254/1000\n",
      "26983/26983 [==============================] - 2s 92us/step - loss: 0.1461 - val_loss: 0.1467\n",
      "Epoch 255/1000\n",
      "26983/26983 [==============================] - 3s 99us/step - loss: 0.1461 - val_loss: 0.1463\n",
      "Epoch 256/1000\n",
      "26983/26983 [==============================] - 3s 100us/step - loss: 0.1461 - val_loss: 0.1473\n",
      "Epoch 257/1000\n",
      "26983/26983 [==============================] - 3s 96us/step - loss: 0.1463 - val_loss: 0.1462\n",
      "Epoch 258/1000\n",
      "26983/26983 [==============================] - 2s 89us/step - loss: 0.1462 - val_loss: 0.1472\n",
      "Epoch 259/1000\n",
      "26983/26983 [==============================] - 2s 90us/step - loss: 0.1462 - val_loss: 0.1475\n",
      "Epoch 260/1000\n",
      "26983/26983 [==============================] - 2s 88us/step - loss: 0.1461 - val_loss: 0.1466\n",
      "Epoch 261/1000\n",
      "26983/26983 [==============================] - 2s 89us/step - loss: 0.1457 - val_loss: 0.1471\n",
      "Epoch 262/1000\n",
      "26983/26983 [==============================] - 2s 89us/step - loss: 0.1462 - val_loss: 0.1461\n",
      "Epoch 263/1000\n",
      "26983/26983 [==============================] - 2s 88us/step - loss: 0.1460 - val_loss: 0.1464\n",
      "Epoch 264/1000\n",
      "26983/26983 [==============================] - 2s 90us/step - loss: 0.1460 - val_loss: 0.1461\n",
      "Epoch 265/1000\n",
      "26983/26983 [==============================] - 2s 89us/step - loss: 0.1459 - val_loss: 0.1464\n",
      "Epoch 266/1000\n",
      "26983/26983 [==============================] - 3s 93us/step - loss: 0.1460 - val_loss: 0.1457\n",
      "Epoch 267/1000\n",
      "26983/26983 [==============================] - 2s 92us/step - loss: 0.1458 - val_loss: 0.1466\n",
      "Epoch 268/1000\n",
      "26983/26983 [==============================] - 3s 104us/step - loss: 0.1458 - val_loss: 0.1478\n",
      "Epoch 269/1000\n",
      "26983/26983 [==============================] - 3s 105us/step - loss: 0.1458 - val_loss: 0.1468\n",
      "Epoch 270/1000\n",
      "26983/26983 [==============================] - 3s 114us/step - loss: 0.1460 - val_loss: 0.1457\n",
      "Epoch 271/1000\n",
      "26983/26983 [==============================] - 3s 108us/step - loss: 0.1457 - val_loss: 0.1469\n",
      "Epoch 00271: early stopping\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss=\"mse\", optimizer=\"adam\")\n",
    "callback = EarlyStopping(monitor=\"loss\", patience=10, verbose=1, mode=\"auto\")\n",
    "history = model.fit(X_train, y_train, epochs=1000, batch_size=128, validation_split=0.1, callbacks=[callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3332/3332 [==============================] - 1s 223us/step\n",
      "0.1466791589184254\n"
     ]
    }
   ],
   "source": [
    "results = model.evaluate(X_test, y_test)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['val_loss', 'loss'])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "history_dict = history.history\n",
    "print(history_dict.keys())\n",
    "\n",
    "loss_values = history_dict['loss']\n",
    "val_loss_values = history_dict['val_loss']\n",
    "\n",
    "epochs = range(1, len(loss_values) + 1)\n",
    "\n",
    "plt.plot(epochs, loss_values, 'b', color='k', label='Training loss')\n",
    "plt.plot(epochs, val_loss_values, 'b', color='b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
