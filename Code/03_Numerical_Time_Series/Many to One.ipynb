{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             Date Time  p (mbar)  T (degC)  Tpot (K)  Tdew (degC)  rh (%)  \\\n",
      "0  01.01.2009 00:10:00    996.52     -8.02    265.40        -8.90    93.3   \n",
      "1  01.01.2009 00:20:00    996.57     -8.41    265.01        -9.28    93.4   \n",
      "2  01.01.2009 00:30:00    996.53     -8.51    264.91        -9.31    93.9   \n",
      "3  01.01.2009 00:40:00    996.51     -8.31    265.12        -9.07    94.2   \n",
      "4  01.01.2009 00:50:00    996.51     -8.27    265.15        -9.04    94.1   \n",
      "\n",
      "   VPmax (mbar)  VPact (mbar)  VPdef (mbar)  sh (g/kg)  H2OC (mmol/mol)  \\\n",
      "0          3.33          3.11          0.22       1.94             3.12   \n",
      "1          3.23          3.02          0.21       1.89             3.03   \n",
      "2          3.21          3.01          0.20       1.88             3.02   \n",
      "3          3.26          3.07          0.19       1.92             3.08   \n",
      "4          3.27          3.08          0.19       1.92             3.09   \n",
      "\n",
      "   rho (g/m**3)  wv (m/s)  max. wv (m/s)  wd (deg)  \n",
      "0       1307.75      1.03           1.75     152.3  \n",
      "1       1309.80      0.72           1.50     136.1  \n",
      "2       1310.24      0.19           0.63     171.6  \n",
      "3       1309.19      0.34           0.50     198.0  \n",
      "4       1309.00      0.32           0.63     214.3  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('temperature.csv')\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    p (mbar)  T (degC)  Tpot (K)  Tdew (degC)    rh (%)  VPmax (mbar)  \\\n",
      "0   0.900080 -1.931243 -1.982008    -1.862774  1.073091     -1.307358   \n",
      "6   0.897721 -1.886062 -1.936218    -1.779038  1.162756     -1.293054   \n",
      "12  0.913050 -2.024996 -2.074707    -1.974423  1.085046     -1.334667   \n",
      "18  0.941348 -2.023866 -2.075824    -1.973027  1.085046     -1.333367   \n",
      "24  0.962572 -2.067919 -2.121614    -2.051181  1.007336     -1.346371   \n",
      "\n",
      "    VPact (mbar)  VPdef (mbar)  sh (g/kg)  H2OC (mmol/mol)  rho (g/m**3)  \\\n",
      "0      -1.473784     -0.798768  -1.476283        -1.478172      2.123687   \n",
      "6      -1.438047     -0.807030  -1.438763        -1.442890      2.074970   \n",
      "12     -1.519050     -0.802899  -1.521308        -1.522864      2.226299   \n",
      "18     -1.519050     -0.802899  -1.517556        -1.522864      2.232418   \n",
      "24     -1.550021     -0.794637  -1.551324        -1.553442      2.285372   \n",
      "\n",
      "    wv (m/s)  max. wv (m/s)  wd (deg)  \n",
      "0  -0.727723      -0.779684 -0.277693  \n",
      "6  -1.281252      -1.260773 -0.114206  \n",
      "12 -1.294276      -1.316614 -0.208614  \n",
      "18 -1.352885      -1.424000 -0.542494  \n",
      "24 -1.333349      -1.368159  0.316385  \n"
     ]
    }
   ],
   "source": [
    "df = df.iloc[:200000:6]\n",
    "df = df.drop(columns=['Date Time'])\n",
    "\n",
    "mean = df.mean()\n",
    "std = df.std()\n",
    "df = (df-mean)/std\n",
    "\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(33303, 30, 14) (33303, 1)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "pastDay=30\n",
    "futureDay=1\n",
    "\n",
    "X, y = [], []\n",
    "for i in range(df.shape[0]-futureDay-pastDay):\n",
    "    X.append(np.array(df.iloc[i:i+pastDay]))\n",
    "    y.append(np.array(df.iloc[i+pastDay:i+pastDay+futureDay][\"T (degC)\"]))\n",
    "\n",
    "X, y = np.array(X), np.array(y)\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(29972, 30, 14) (3331, 30, 14) (29972, 1) (3331, 1)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.1, random_state=1)\n",
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "config = tf.compat.v1.ConfigProto()\n",
    "config.gpu_options.allocator_type = 'BFC' #A \"Best-fit with coalescing\" algorithm, simplified from a version of dlmalloc.\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.8\n",
    "config.gpu_options.allow_growth =True\n",
    "\n",
    "#set_session(tf.compat.v1.Session(config=config)) \n",
    "tf.compat.v1.keras.backend.set_session(tf.compat.v1.Session(config=config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 10)                1000      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 1,011\n",
      "Trainable params: 1,011\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, TimeDistributed\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(10, input_shape=(X_train.shape[1], X_train.shape[2]),return_sequences=False))\n",
    "model.add(Dense(1))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 26974 samples, validate on 2998 samples\n",
      "Epoch 1/1000\n",
      "26974/26974 [==============================] - 6s 233us/step - loss: 0.2194 - val_loss: 0.0662\n",
      "Epoch 2/1000\n",
      "26974/26974 [==============================] - 6s 216us/step - loss: 0.0415 - val_loss: 0.0302\n",
      "Epoch 3/1000\n",
      "26974/26974 [==============================] - 6s 219us/step - loss: 0.0224 - val_loss: 0.0198\n",
      "Epoch 4/1000\n",
      "26974/26974 [==============================] - 6s 211us/step - loss: 0.0159 - val_loss: 0.0151\n",
      "Epoch 5/1000\n",
      "26974/26974 [==============================] - 6s 209us/step - loss: 0.0128 - val_loss: 0.0123\n",
      "Epoch 6/1000\n",
      "26974/26974 [==============================] - 6s 215us/step - loss: 0.0110 - val_loss: 0.0109\n",
      "Epoch 7/1000\n",
      "26974/26974 [==============================] - 6s 226us/step - loss: 0.0099 - val_loss: 0.0101\n",
      "Epoch 8/1000\n",
      "26974/26974 [==============================] - 7s 260us/step - loss: 0.0093 - val_loss: 0.0094\n",
      "Epoch 9/1000\n",
      "26974/26974 [==============================] - 6s 240us/step - loss: 0.0088 - val_loss: 0.0089\n",
      "Epoch 10/1000\n",
      "26974/26974 [==============================] - 6s 231us/step - loss: 0.0085 - val_loss: 0.0090\n",
      "Epoch 11/1000\n",
      "26974/26974 [==============================] - 6s 239us/step - loss: 0.0083 - val_loss: 0.0086\n",
      "Epoch 12/1000\n",
      "26974/26974 [==============================] - 6s 222us/step - loss: 0.0082 - val_loss: 0.0084\n",
      "Epoch 13/1000\n",
      "26974/26974 [==============================] - 6s 218us/step - loss: 0.0080 - val_loss: 0.0083\n",
      "Epoch 14/1000\n",
      "26974/26974 [==============================] - 6s 208us/step - loss: 0.0078 - val_loss: 0.0081\n",
      "Epoch 15/1000\n",
      "26974/26974 [==============================] - 6s 217us/step - loss: 0.0077 - val_loss: 0.0082\n",
      "Epoch 16/1000\n",
      "26974/26974 [==============================] - 6s 213us/step - loss: 0.0077 - val_loss: 0.0079\n",
      "Epoch 17/1000\n",
      "26974/26974 [==============================] - 6s 210us/step - loss: 0.0076 - val_loss: 0.0079\n",
      "Epoch 18/1000\n",
      "26974/26974 [==============================] - 6s 207us/step - loss: 0.0075 - val_loss: 0.0078\n",
      "Epoch 19/1000\n",
      "26974/26974 [==============================] - 6s 209us/step - loss: 0.0074 - val_loss: 0.0077\n",
      "Epoch 20/1000\n",
      "26974/26974 [==============================] - 6s 210us/step - loss: 0.0074 - val_loss: 0.0078\n",
      "Epoch 21/1000\n",
      "26974/26974 [==============================] - 6s 220us/step - loss: 0.0073 - val_loss: 0.0075\n",
      "Epoch 22/1000\n",
      "26974/26974 [==============================] - 6s 209us/step - loss: 0.0073 - val_loss: 0.0075\n",
      "Epoch 23/1000\n",
      "26974/26974 [==============================] - 6s 208us/step - loss: 0.0072 - val_loss: 0.0075\n",
      "Epoch 24/1000\n",
      "26974/26974 [==============================] - 6s 215us/step - loss: 0.0071 - val_loss: 0.0074\n",
      "Epoch 25/1000\n",
      "26974/26974 [==============================] - 6s 209us/step - loss: 0.0071 - val_loss: 0.0074\n",
      "Epoch 26/1000\n",
      "26974/26974 [==============================] - 6s 206us/step - loss: 0.0070 - val_loss: 0.0075\n",
      "Epoch 27/1000\n",
      "26974/26974 [==============================] - 6s 208us/step - loss: 0.0070 - val_loss: 0.0072\n",
      "Epoch 28/1000\n",
      "26974/26974 [==============================] - 6s 205us/step - loss: 0.0070 - val_loss: 0.0073\n",
      "Epoch 29/1000\n",
      "26974/26974 [==============================] - 5s 202us/step - loss: 0.0069 - val_loss: 0.0071\n",
      "Epoch 30/1000\n",
      "26974/26974 [==============================] - 6s 209us/step - loss: 0.0069 - val_loss: 0.0071\n",
      "Epoch 31/1000\n",
      "26974/26974 [==============================] - 6s 209us/step - loss: 0.0068 - val_loss: 0.0071\n",
      "Epoch 32/1000\n",
      "26974/26974 [==============================] - 6s 212us/step - loss: 0.0068 - val_loss: 0.0071\n",
      "Epoch 33/1000\n",
      "26974/26974 [==============================] - 6s 217us/step - loss: 0.0068 - val_loss: 0.0071\n",
      "Epoch 34/1000\n",
      "26974/26974 [==============================] - 6s 227us/step - loss: 0.0068 - val_loss: 0.0070\n",
      "Epoch 35/1000\n",
      "26974/26974 [==============================] - 6s 223us/step - loss: 0.0067 - val_loss: 0.0070\n",
      "Epoch 36/1000\n",
      "26974/26974 [==============================] - 6s 217us/step - loss: 0.0067 - val_loss: 0.0071\n",
      "Epoch 37/1000\n",
      "26974/26974 [==============================] - 6s 215us/step - loss: 0.0067 - val_loss: 0.0069\n",
      "Epoch 38/1000\n",
      "26974/26974 [==============================] - 6s 218us/step - loss: 0.0067 - val_loss: 0.0070\n",
      "Epoch 39/1000\n",
      "26974/26974 [==============================] - 6s 210us/step - loss: 0.0067 - val_loss: 0.0069\n",
      "Epoch 40/1000\n",
      "26974/26974 [==============================] - 7s 242us/step - loss: 0.0066 - val_loss: 0.0070\n",
      "Epoch 41/1000\n",
      "26974/26974 [==============================] - 6s 219us/step - loss: 0.0066 - val_loss: 0.0071\n",
      "Epoch 42/1000\n",
      "26974/26974 [==============================] - 6s 223us/step - loss: 0.0066 - val_loss: 0.0069\n",
      "Epoch 43/1000\n",
      "26974/26974 [==============================] - 6s 217us/step - loss: 0.0066 - val_loss: 0.0071\n",
      "Epoch 44/1000\n",
      "26974/26974 [==============================] - 7s 252us/step - loss: 0.0066 - val_loss: 0.0068\n",
      "Epoch 45/1000\n",
      "26974/26974 [==============================] - 6s 232us/step - loss: 0.0066 - val_loss: 0.0069\n",
      "Epoch 46/1000\n",
      "26974/26974 [==============================] - 6s 227us/step - loss: 0.0065 - val_loss: 0.0068\n",
      "Epoch 47/1000\n",
      "26974/26974 [==============================] - 6s 234us/step - loss: 0.0066 - val_loss: 0.0069\n",
      "Epoch 48/1000\n",
      "26974/26974 [==============================] - 6s 238us/step - loss: 0.0065 - val_loss: 0.0068\n",
      "Epoch 49/1000\n",
      "26974/26974 [==============================] - 6s 238us/step - loss: 0.0065 - val_loss: 0.0069\n",
      "Epoch 50/1000\n",
      "26974/26974 [==============================] - 7s 247us/step - loss: 0.0065 - val_loss: 0.0067\n",
      "Epoch 51/1000\n",
      "26974/26974 [==============================] - 6s 220us/step - loss: 0.0065 - val_loss: 0.0068\n",
      "Epoch 52/1000\n",
      "26974/26974 [==============================] - 6s 214us/step - loss: 0.0065 - val_loss: 0.0067\n",
      "Epoch 53/1000\n",
      "26974/26974 [==============================] - 6s 213us/step - loss: 0.0065 - val_loss: 0.0068\n",
      "Epoch 54/1000\n",
      "26974/26974 [==============================] - 6s 218us/step - loss: 0.0064 - val_loss: 0.0067\n",
      "Epoch 55/1000\n",
      "26974/26974 [==============================] - 6s 212us/step - loss: 0.0065 - val_loss: 0.0069\n",
      "Epoch 56/1000\n",
      "26974/26974 [==============================] - 6s 234us/step - loss: 0.0065 - val_loss: 0.0068\n",
      "Epoch 57/1000\n",
      "26974/26974 [==============================] - 6s 213us/step - loss: 0.0065 - val_loss: 0.0068\n",
      "Epoch 58/1000\n",
      "26974/26974 [==============================] - 6s 223us/step - loss: 0.0065 - val_loss: 0.0067\n",
      "Epoch 59/1000\n",
      "26974/26974 [==============================] - 6s 228us/step - loss: 0.0064 - val_loss: 0.0068\n",
      "Epoch 60/1000\n",
      "26974/26974 [==============================] - 6s 240us/step - loss: 0.0065 - val_loss: 0.0067\n",
      "Epoch 61/1000\n",
      "26974/26974 [==============================] - 6s 212us/step - loss: 0.0065 - val_loss: 0.0067\n",
      "Epoch 62/1000\n",
      "26974/26974 [==============================] - 6s 221us/step - loss: 0.0064 - val_loss: 0.0068\n",
      "Epoch 63/1000\n",
      "26974/26974 [==============================] - 6s 229us/step - loss: 0.0064 - val_loss: 0.0067\n",
      "Epoch 64/1000\n",
      "26974/26974 [==============================] - 7s 246us/step - loss: 0.0064 - val_loss: 0.0067\n",
      "Epoch 65/1000\n",
      "26974/26974 [==============================] - 6s 221us/step - loss: 0.0064 - val_loss: 0.0067\n",
      "Epoch 66/1000\n",
      "26974/26974 [==============================] - 6s 228us/step - loss: 0.0064 - val_loss: 0.0067\n",
      "Epoch 67/1000\n",
      "26974/26974 [==============================] - 6s 223us/step - loss: 0.0064 - val_loss: 0.0067\n",
      "Epoch 68/1000\n",
      "26974/26974 [==============================] - 6s 215us/step - loss: 0.0064 - val_loss: 0.0074\n",
      "Epoch 69/1000\n",
      "26974/26974 [==============================] - 6s 206us/step - loss: 0.0064 - val_loss: 0.0066\n",
      "Epoch 70/1000\n",
      "26974/26974 [==============================] - 6s 218us/step - loss: 0.0064 - val_loss: 0.0068\n",
      "Epoch 71/1000\n",
      "26974/26974 [==============================] - 6s 216us/step - loss: 0.0064 - val_loss: 0.0066\n",
      "Epoch 72/1000\n",
      "26974/26974 [==============================] - 6s 225us/step - loss: 0.0063 - val_loss: 0.0066\n",
      "Epoch 73/1000\n",
      "26974/26974 [==============================] - 6s 222us/step - loss: 0.0064 - val_loss: 0.0069\n",
      "Epoch 74/1000\n",
      "26974/26974 [==============================] - 6s 218us/step - loss: 0.0064 - val_loss: 0.0069\n",
      "Epoch 75/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26974/26974 [==============================] - 6s 214us/step - loss: 0.0063 - val_loss: 0.0066\n",
      "Epoch 76/1000\n",
      "26974/26974 [==============================] - 6s 212us/step - loss: 0.0064 - val_loss: 0.0067\n",
      "Epoch 77/1000\n",
      "26974/26974 [==============================] - 6s 216us/step - loss: 0.0063 - val_loss: 0.0067\n",
      "Epoch 78/1000\n",
      "26974/26974 [==============================] - 6s 212us/step - loss: 0.0063 - val_loss: 0.0067\n",
      "Epoch 79/1000\n",
      "26974/26974 [==============================] - 6s 211us/step - loss: 0.0063 - val_loss: 0.0067\n",
      "Epoch 80/1000\n",
      "26974/26974 [==============================] - 6s 227us/step - loss: 0.0064 - val_loss: 0.0066\n",
      "Epoch 81/1000\n",
      "26974/26974 [==============================] - 6s 230us/step - loss: 0.0063 - val_loss: 0.0066\n",
      "Epoch 82/1000\n",
      "26974/26974 [==============================] - 6s 217us/step - loss: 0.0063 - val_loss: 0.0067\n",
      "Epoch 83/1000\n",
      "26974/26974 [==============================] - 6s 223us/step - loss: 0.0063 - val_loss: 0.0065\n",
      "Epoch 84/1000\n",
      "26974/26974 [==============================] - 6s 226us/step - loss: 0.0063 - val_loss: 0.0067\n",
      "Epoch 85/1000\n",
      "26974/26974 [==============================] - 6s 219us/step - loss: 0.0063 - val_loss: 0.0066\n",
      "Epoch 86/1000\n",
      "26974/26974 [==============================] - 6s 205us/step - loss: 0.0063 - val_loss: 0.0066\n",
      "Epoch 87/1000\n",
      "26974/26974 [==============================] - 5s 204us/step - loss: 0.0063 - val_loss: 0.0066\n",
      "Epoch 88/1000\n",
      "26974/26974 [==============================] - 6s 212us/step - loss: 0.0063 - val_loss: 0.0066\n",
      "Epoch 89/1000\n",
      "26974/26974 [==============================] - 6s 220us/step - loss: 0.0063 - val_loss: 0.0066\n",
      "Epoch 90/1000\n",
      "26974/26974 [==============================] - 6s 205us/step - loss: 0.0063 - val_loss: 0.0066\n",
      "Epoch 91/1000\n",
      "26974/26974 [==============================] - 5s 202us/step - loss: 0.0063 - val_loss: 0.0066\n",
      "Epoch 92/1000\n",
      "26974/26974 [==============================] - 5s 202us/step - loss: 0.0063 - val_loss: 0.0066\n",
      "Epoch 93/1000\n",
      "26974/26974 [==============================] - 6s 204us/step - loss: 0.0063 - val_loss: 0.0067\n",
      "Epoch 94/1000\n",
      "26974/26974 [==============================] - 6s 213us/step - loss: 0.0063 - val_loss: 0.0066\n",
      "Epoch 95/1000\n",
      "26974/26974 [==============================] - 6s 221us/step - loss: 0.0063 - val_loss: 0.0066\n",
      "Epoch 96/1000\n",
      "26974/26974 [==============================] - 5s 201us/step - loss: 0.0063 - val_loss: 0.0066\n",
      "Epoch 97/1000\n",
      "26974/26974 [==============================] - 6s 205us/step - loss: 0.0063 - val_loss: 0.0067\n",
      "Epoch 98/1000\n",
      "26974/26974 [==============================] - 5s 203us/step - loss: 0.0062 - val_loss: 0.0065\n",
      "Epoch 99/1000\n",
      "26974/26974 [==============================] - 6s 204us/step - loss: 0.0063 - val_loss: 0.0065\n",
      "Epoch 100/1000\n",
      "26974/26974 [==============================] - 6s 213us/step - loss: 0.0063 - val_loss: 0.0066\n",
      "Epoch 101/1000\n",
      "26974/26974 [==============================] - 6s 209us/step - loss: 0.0062 - val_loss: 0.0065\n",
      "Epoch 102/1000\n",
      "26974/26974 [==============================] - 7s 242us/step - loss: 0.0063 - val_loss: 0.0065\n",
      "Epoch 103/1000\n",
      "26974/26974 [==============================] - 6s 241us/step - loss: 0.0063 - val_loss: 0.0065\n",
      "Epoch 104/1000\n",
      "26974/26974 [==============================] - 6s 221us/step - loss: 0.0062 - val_loss: 0.0065\n",
      "Epoch 105/1000\n",
      "26974/26974 [==============================] - 6s 231us/step - loss: 0.0062 - val_loss: 0.0065\n",
      "Epoch 106/1000\n",
      "26974/26974 [==============================] - 6s 230us/step - loss: 0.0062 - val_loss: 0.0066\n",
      "Epoch 107/1000\n",
      "26974/26974 [==============================] - 6s 206us/step - loss: 0.0063 - val_loss: 0.0066\n",
      "Epoch 108/1000\n",
      "26974/26974 [==============================] - 7s 248us/step - loss: 0.0062 - val_loss: 0.0066\n",
      "Epoch 109/1000\n",
      "26974/26974 [==============================] - 6s 224us/step - loss: 0.0062 - val_loss: 0.0068\n",
      "Epoch 110/1000\n",
      "26974/26974 [==============================] - 6s 221us/step - loss: 0.0062 - val_loss: 0.0067\n",
      "Epoch 111/1000\n",
      "26974/26974 [==============================] - 6s 220us/step - loss: 0.0062 - val_loss: 0.0065\n",
      "Epoch 112/1000\n",
      "26974/26974 [==============================] - 6s 211us/step - loss: 0.0063 - val_loss: 0.0065\n",
      "Epoch 113/1000\n",
      "26974/26974 [==============================] - 6s 212us/step - loss: 0.0062 - val_loss: 0.0065\n",
      "Epoch 114/1000\n",
      "26974/26974 [==============================] - 6s 211us/step - loss: 0.0062 - val_loss: 0.0067\n",
      "Epoch 115/1000\n",
      "26974/26974 [==============================] - 6s 212us/step - loss: 0.0062 - val_loss: 0.0066\n",
      "Epoch 116/1000\n",
      "26974/26974 [==============================] - 6s 216us/step - loss: 0.0062 - val_loss: 0.0067\n",
      "Epoch 117/1000\n",
      "26974/26974 [==============================] - 6s 213us/step - loss: 0.0062 - val_loss: 0.0065\n",
      "Epoch 118/1000\n",
      "26974/26974 [==============================] - 6s 218us/step - loss: 0.0062 - val_loss: 0.0066\n",
      "Epoch 119/1000\n",
      "26974/26974 [==============================] - 5s 201us/step - loss: 0.0062 - val_loss: 0.0065\n",
      "Epoch 120/1000\n",
      "26974/26974 [==============================] - 5s 199us/step - loss: 0.0062 - val_loss: 0.0065\n",
      "Epoch 121/1000\n",
      "26974/26974 [==============================] - 5s 198us/step - loss: 0.0062 - val_loss: 0.0065\n",
      "Epoch 122/1000\n",
      "26974/26974 [==============================] - 6s 213us/step - loss: 0.0062 - val_loss: 0.0066\n",
      "Epoch 123/1000\n",
      "26974/26974 [==============================] - 6s 211us/step - loss: 0.0062 - val_loss: 0.0066\n",
      "Epoch 124/1000\n",
      "26974/26974 [==============================] - 6s 210us/step - loss: 0.0062 - val_loss: 0.0065\n",
      "Epoch 125/1000\n",
      "26974/26974 [==============================] - 6s 211us/step - loss: 0.0062 - val_loss: 0.0066\n",
      "Epoch 126/1000\n",
      "26974/26974 [==============================] - 6s 214us/step - loss: 0.0062 - val_loss: 0.0066\n",
      "Epoch 127/1000\n",
      "26974/26974 [==============================] - 6s 209us/step - loss: 0.0062 - val_loss: 0.0065\n",
      "Epoch 128/1000\n",
      "26974/26974 [==============================] - 6s 205us/step - loss: 0.0062 - val_loss: 0.0065\n",
      "Epoch 129/1000\n",
      "26974/26974 [==============================] - 6s 206us/step - loss: 0.0062 - val_loss: 0.0065\n",
      "Epoch 130/1000\n",
      "26974/26974 [==============================] - 6s 213us/step - loss: 0.0061 - val_loss: 0.0065\n",
      "Epoch 131/1000\n",
      "26974/26974 [==============================] - 6s 222us/step - loss: 0.0062 - val_loss: 0.0065\n",
      "Epoch 132/1000\n",
      "26974/26974 [==============================] - 6s 227us/step - loss: 0.0062 - val_loss: 0.0066\n",
      "Epoch 133/1000\n",
      "26974/26974 [==============================] - 6s 213us/step - loss: 0.0062 - val_loss: 0.0065\n",
      "Epoch 134/1000\n",
      "26974/26974 [==============================] - 6s 223us/step - loss: 0.0061 - val_loss: 0.0066\n",
      "Epoch 135/1000\n",
      "26974/26974 [==============================] - 6s 207us/step - loss: 0.0062 - val_loss: 0.0065\n",
      "Epoch 136/1000\n",
      "26974/26974 [==============================] - 6s 205us/step - loss: 0.0062 - val_loss: 0.0066\n",
      "Epoch 137/1000\n",
      "26974/26974 [==============================] - 6s 213us/step - loss: 0.0061 - val_loss: 0.0066\n",
      "Epoch 138/1000\n",
      "26974/26974 [==============================] - 6s 206us/step - loss: 0.0062 - val_loss: 0.0065\n",
      "Epoch 139/1000\n",
      "26974/26974 [==============================] - 6s 209us/step - loss: 0.0062 - val_loss: 0.0067\n",
      "Epoch 140/1000\n",
      "26974/26974 [==============================] - 6s 208us/step - loss: 0.0061 - val_loss: 0.0067\n",
      "Epoch 141/1000\n",
      "26974/26974 [==============================] - 6s 215us/step - loss: 0.0062 - val_loss: 0.0066\n",
      "Epoch 142/1000\n",
      "26974/26974 [==============================] - 6s 210us/step - loss: 0.0062 - val_loss: 0.0064\n",
      "Epoch 143/1000\n",
      "26974/26974 [==============================] - 6s 207us/step - loss: 0.0062 - val_loss: 0.0065\n",
      "Epoch 144/1000\n",
      "26974/26974 [==============================] - 6s 206us/step - loss: 0.0061 - val_loss: 0.0065\n",
      "Epoch 145/1000\n",
      "26974/26974 [==============================] - 6s 206us/step - loss: 0.0061 - val_loss: 0.0065\n",
      "Epoch 146/1000\n",
      "26974/26974 [==============================] - 6s 206us/step - loss: 0.0061 - val_loss: 0.0065\n",
      "Epoch 147/1000\n",
      "26974/26974 [==============================] - 6s 205us/step - loss: 0.0061 - val_loss: 0.0065\n",
      "Epoch 148/1000\n",
      "26974/26974 [==============================] - 6s 207us/step - loss: 0.0061 - val_loss: 0.0065\n",
      "Epoch 149/1000\n",
      "26974/26974 [==============================] - 6s 215us/step - loss: 0.0061 - val_loss: 0.0066\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 150/1000\n",
      "26974/26974 [==============================] - 6s 218us/step - loss: 0.0061 - val_loss: 0.0064\n",
      "Epoch 151/1000\n",
      "26974/26974 [==============================] - 6s 208us/step - loss: 0.0061 - val_loss: 0.0065\n",
      "Epoch 152/1000\n",
      "26974/26974 [==============================] - 6s 213us/step - loss: 0.0062 - val_loss: 0.0064\n",
      "Epoch 153/1000\n",
      "26974/26974 [==============================] - 6s 215us/step - loss: 0.0061 - val_loss: 0.0065\n",
      "Epoch 154/1000\n",
      "26974/26974 [==============================] - 6s 224us/step - loss: 0.0061 - val_loss: 0.0065\n",
      "Epoch 155/1000\n",
      "26974/26974 [==============================] - 6s 210us/step - loss: 0.0061 - val_loss: 0.0066\n",
      "Epoch 156/1000\n",
      "26974/26974 [==============================] - 6s 211us/step - loss: 0.0061 - val_loss: 0.0065\n",
      "Epoch 157/1000\n",
      "26974/26974 [==============================] - 6s 219us/step - loss: 0.0061 - val_loss: 0.0067\n",
      "Epoch 158/1000\n",
      "26974/26974 [==============================] - 6s 206us/step - loss: 0.0061 - val_loss: 0.0068\n",
      "Epoch 159/1000\n",
      "26974/26974 [==============================] - 6s 210us/step - loss: 0.0061 - val_loss: 0.0066\n",
      "Epoch 160/1000\n",
      "26974/26974 [==============================] - 6s 216us/step - loss: 0.0061 - val_loss: 0.0065\n",
      "Epoch 161/1000\n",
      "26974/26974 [==============================] - 6s 214us/step - loss: 0.0061 - val_loss: 0.0065\n",
      "Epoch 162/1000\n",
      "26974/26974 [==============================] - 6s 213us/step - loss: 0.0061 - val_loss: 0.0064\n",
      "Epoch 163/1000\n",
      "26974/26974 [==============================] - 6s 216us/step - loss: 0.0061 - val_loss: 0.0064\n",
      "Epoch 164/1000\n",
      "26974/26974 [==============================] - 6s 207us/step - loss: 0.0061 - val_loss: 0.0064\n",
      "Epoch 165/1000\n",
      "26974/26974 [==============================] - 7s 242us/step - loss: 0.0061 - val_loss: 0.0066\n",
      "Epoch 166/1000\n",
      "26974/26974 [==============================] - 7s 241us/step - loss: 0.0061 - val_loss: 0.0064\n",
      "Epoch 167/1000\n",
      "26974/26974 [==============================] - 6s 213us/step - loss: 0.0061 - val_loss: 0.0066\n",
      "Epoch 168/1000\n",
      "26974/26974 [==============================] - 6s 222us/step - loss: 0.0061 - val_loss: 0.0065\n",
      "Epoch 169/1000\n",
      "26974/26974 [==============================] - 6s 214us/step - loss: 0.0061 - val_loss: 0.0065\n",
      "Epoch 170/1000\n",
      "26974/26974 [==============================] - 6s 216us/step - loss: 0.0061 - val_loss: 0.0064\n",
      "Epoch 171/1000\n",
      "26974/26974 [==============================] - 6s 216us/step - loss: 0.0061 - val_loss: 0.0065\n",
      "Epoch 172/1000\n",
      "26974/26974 [==============================] - 6s 214us/step - loss: 0.0061 - val_loss: 0.0065\n",
      "Epoch 173/1000\n",
      "26974/26974 [==============================] - 6s 224us/step - loss: 0.0061 - val_loss: 0.0065\n",
      "Epoch 174/1000\n",
      "26974/26974 [==============================] - 6s 227us/step - loss: 0.0061 - val_loss: 0.0065\n",
      "Epoch 175/1000\n",
      "26974/26974 [==============================] - 6s 226us/step - loss: 0.0061 - val_loss: 0.0066\n",
      "Epoch 176/1000\n",
      "26974/26974 [==============================] - 6s 209us/step - loss: 0.0061 - val_loss: 0.0064\n",
      "Epoch 177/1000\n",
      "26974/26974 [==============================] - 6s 214us/step - loss: 0.0061 - val_loss: 0.0064\n",
      "Epoch 178/1000\n",
      "26974/26974 [==============================] - 6s 212us/step - loss: 0.0061 - val_loss: 0.0065\n",
      "Epoch 179/1000\n",
      "26974/26974 [==============================] - 6s 217us/step - loss: 0.0061 - val_loss: 0.0067\n",
      "Epoch 180/1000\n",
      "26974/26974 [==============================] - 7s 244us/step - loss: 0.0061 - val_loss: 0.0065\n",
      "Epoch 181/1000\n",
      "26974/26974 [==============================] - 6s 227us/step - loss: 0.0061 - val_loss: 0.0064\n",
      "Epoch 182/1000\n",
      "26974/26974 [==============================] - 6s 215us/step - loss: 0.0061 - val_loss: 0.0065\n",
      "Epoch 183/1000\n",
      "26974/26974 [==============================] - 6s 216us/step - loss: 0.0061 - val_loss: 0.0064\n",
      "Epoch 184/1000\n",
      "26974/26974 [==============================] - 6s 213us/step - loss: 0.0060 - val_loss: 0.0065\n",
      "Epoch 185/1000\n",
      "26974/26974 [==============================] - 6s 219us/step - loss: 0.0061 - val_loss: 0.0064\n",
      "Epoch 186/1000\n",
      "26974/26974 [==============================] - 6s 215us/step - loss: 0.0061 - val_loss: 0.0067\n",
      "Epoch 187/1000\n",
      "26974/26974 [==============================] - 6s 217us/step - loss: 0.0061 - val_loss: 0.0064\n",
      "Epoch 188/1000\n",
      "26974/26974 [==============================] - 6s 215us/step - loss: 0.0061 - val_loss: 0.0065\n",
      "Epoch 189/1000\n",
      "26974/26974 [==============================] - 6s 215us/step - loss: 0.0061 - val_loss: 0.0064\n",
      "Epoch 190/1000\n",
      "26974/26974 [==============================] - 6s 207us/step - loss: 0.0060 - val_loss: 0.0064\n",
      "Epoch 191/1000\n",
      "26974/26974 [==============================] - 6s 210us/step - loss: 0.0061 - val_loss: 0.0065\n",
      "Epoch 192/1000\n",
      "26974/26974 [==============================] - 6s 206us/step - loss: 0.0061 - val_loss: 0.0065\n",
      "Epoch 193/1000\n",
      "26974/26974 [==============================] - 6s 215us/step - loss: 0.0061 - val_loss: 0.0065\n",
      "Epoch 194/1000\n",
      "26974/26974 [==============================] - 6s 210us/step - loss: 0.0061 - val_loss: 0.0065\n",
      "Epoch 00194: early stopping\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss=\"mse\", optimizer=\"adam\")\n",
    "callback = EarlyStopping(monitor=\"loss\", patience=10, verbose=1, mode=\"auto\")\n",
    "history = model.fit(X_train, y_train, epochs=1000, batch_size=128, validation_split=0.1, callbacks=[callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3331/3331 [==============================] - 2s 490us/step\n",
      "0.0063161847442478\n"
     ]
    }
   ],
   "source": [
    "results = model.evaluate(X_test, y_test)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['val_loss', 'loss'])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "history_dict = history.history\n",
    "print(history_dict.keys())\n",
    "\n",
    "loss_values = history_dict['loss']\n",
    "val_loss_values = history_dict['val_loss']\n",
    "\n",
    "epochs = range(1, len(loss_values) + 1)\n",
    "\n",
    "plt.plot(epochs, loss_values, 'b', color='k', label='Training loss')\n",
    "plt.plot(epochs, val_loss_values, 'b', color='b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
