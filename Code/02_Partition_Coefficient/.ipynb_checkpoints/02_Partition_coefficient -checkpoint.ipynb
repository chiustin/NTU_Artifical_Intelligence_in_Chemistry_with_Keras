{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>smiles</th>\n",
       "      <th>logP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C[C@H]([C@@H](C)Cl)Cl</td>\n",
       "      <td>2.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C(C=CBr)N</td>\n",
       "      <td>0.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CCC(CO)Br</td>\n",
       "      <td>1.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[13CH3][13CH2][13CH2][13CH2][13CH2][13CH2]O</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CCCOCCP</td>\n",
       "      <td>0.6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        smiles  logP\n",
       "0                        C[C@H]([C@@H](C)Cl)Cl   2.3\n",
       "1                                    C(C=CBr)N   0.3\n",
       "2                                    CCC(CO)Br   1.3\n",
       "3  [13CH3][13CH2][13CH2][13CH2][13CH2][13CH2]O   2.0\n",
       "4                                      CCCOCCP   0.6"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df= pd.read_csv('logP_dataset.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdkit import Chem \n",
    "df['mol'] = df['smiles'].apply(lambda x: Chem.MolFromSmiles(x))\n",
    "df['mol'] = df['mol'].apply(lambda x: Chem.AddHs(x))\n",
    "df['num_of_atoms'] = df['mol'].apply(lambda x: x.GetNumAtoms())\n",
    "df['num_of_heavy_atoms'] = df['mol'].apply(lambda x: x.GetNumHeavyAtoms())\n",
    "\n",
    "def number_of_atoms(atom_list, df):\n",
    "    for i in atom_list:\n",
    "        df['num_of_{}_atoms'.format(i)] = df['mol'].apply(lambda x: len(x.GetSubstructMatches(Chem.MolFromSmiles(i))))\n",
    "\n",
    "number_of_atoms(['C','O', 'N', 'Cl'], df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.3 0.3 1.3 ... 0.4 3.4 1. ]\n"
     ]
    }
   ],
   "source": [
    "y = df['logP'].values\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       num_of_atoms  num_of_heavy_atoms  num_of_C_atoms  num_of_O_atoms  \\\n",
      "0         -0.092957            0.086835        0.274572       -0.698667   \n",
      "1         -0.733774           -0.811013       -0.345443       -0.698667   \n",
      "2          0.120648            0.086835        0.274572        0.388860   \n",
      "3          1.402281            0.984684        1.514603        0.388860   \n",
      "4          1.188676            0.984684        0.894588        0.388860   \n",
      "...             ...                 ...             ...             ...   \n",
      "14605      0.120648            0.086835        0.274572       -0.698667   \n",
      "14606     -0.092957            0.086835       -0.345443        2.563914   \n",
      "14607      0.761465            0.086835        0.274572       -0.698667   \n",
      "14608      0.547859            0.086835        1.514603       -0.698667   \n",
      "14609     -1.588195           -0.811013       -0.965459        0.388860   \n",
      "\n",
      "       num_of_N_atoms  num_of_Cl_atoms  \n",
      "0           -0.639604         3.352682  \n",
      "1            0.588497        -0.373654  \n",
      "2           -0.639604        -0.373654  \n",
      "3           -0.639604        -0.373654  \n",
      "4           -0.639604        -0.373654  \n",
      "...               ...              ...  \n",
      "14605       -0.639604        -0.373654  \n",
      "14606       -0.639604        -0.373654  \n",
      "14607        1.816598        -0.373654  \n",
      "14608       -0.639604        -0.373654  \n",
      "14609       -0.639604        -0.373654  \n",
      "\n",
      "[14610 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "train_df = df.drop(columns=['smiles', 'mol', 'logP'])\n",
    "\n",
    "mean = train_df.mean(axis=0)\n",
    "std  = train_df.std(axis=0)\n",
    "\n",
    "train_df = (train_df - mean) / std\n",
    "print(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_df, y, test_size=.1, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#InternalError: Blas GEMM launch failed\n",
    "import tensorflow as tf\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "config = tf.compat.v1.ConfigProto()\n",
    "config.gpu_options.allocator_type = 'BFC' #A \"Best-fit with coalescing\" algorithm, simplified from a version of dlmalloc.\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.8\n",
    "config.gpu_options.allow_growth =True\n",
    "\n",
    "#set_session(tf.compat.v1.Session(config=config)) \n",
    "tf.compat.v1.keras.backend.set_session(tf.compat.v1.Session(config=config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 96)                672       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 96)                9312      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 97        \n",
      "=================================================================\n",
      "Total params: 10,081\n",
      "Trainable params: 10,081\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras import optimizers\n",
    "from keras.layers import Dropout\n",
    "from tensorflow.keras import initializers\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(96, input_dim=6, activation='relu'))\n",
    "model.add(Dense(96, activation='relu'))\n",
    "model.add(Dense(1))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 11834 samples, validate on 1315 samples\n",
      "Epoch 1/300\n",
      "11834/11834 [==============================] - 0s 34us/step - loss: 0.9156 - mae: 0.7271 - val_loss: 0.5536 - val_mae: 0.5684\n",
      "Epoch 2/300\n",
      "11834/11834 [==============================] - 0s 9us/step - loss: 0.4605 - mae: 0.5154 - val_loss: 0.5186 - val_mae: 0.5369\n",
      "Epoch 3/300\n",
      "11834/11834 [==============================] - 0s 9us/step - loss: 0.4336 - mae: 0.4965 - val_loss: 0.5009 - val_mae: 0.5265\n",
      "Epoch 4/300\n",
      "11834/11834 [==============================] - 0s 9us/step - loss: 0.4181 - mae: 0.4875 - val_loss: 0.4878 - val_mae: 0.5203\n",
      "Epoch 5/300\n",
      "11834/11834 [==============================] - 0s 9us/step - loss: 0.4088 - mae: 0.4814 - val_loss: 0.4763 - val_mae: 0.5098\n",
      "Epoch 6/300\n",
      "11834/11834 [==============================] - 0s 9us/step - loss: 0.4023 - mae: 0.4774 - val_loss: 0.4669 - val_mae: 0.5084\n",
      "Epoch 7/300\n",
      "11834/11834 [==============================] - 0s 9us/step - loss: 0.3934 - mae: 0.4718 - val_loss: 0.4599 - val_mae: 0.5027\n",
      "Epoch 8/300\n",
      "11834/11834 [==============================] - 0s 9us/step - loss: 0.3881 - mae: 0.4677 - val_loss: 0.4522 - val_mae: 0.4968\n",
      "Epoch 9/300\n",
      "11834/11834 [==============================] - 0s 9us/step - loss: 0.3824 - mae: 0.4644 - val_loss: 0.4466 - val_mae: 0.4938\n",
      "Epoch 10/300\n",
      "11834/11834 [==============================] - 0s 9us/step - loss: 0.3816 - mae: 0.4628 - val_loss: 0.4413 - val_mae: 0.4944\n",
      "Epoch 11/300\n",
      "11834/11834 [==============================] - 0s 9us/step - loss: 0.3780 - mae: 0.4606 - val_loss: 0.4382 - val_mae: 0.4932\n",
      "Epoch 12/300\n",
      "11834/11834 [==============================] - 0s 10us/step - loss: 0.3796 - mae: 0.4635 - val_loss: 0.4357 - val_mae: 0.4844\n",
      "Epoch 13/300\n",
      "11834/11834 [==============================] - 0s 9us/step - loss: 0.3741 - mae: 0.4587 - val_loss: 0.4345 - val_mae: 0.4874\n",
      "Epoch 14/300\n",
      "11834/11834 [==============================] - 0s 9us/step - loss: 0.3722 - mae: 0.4575 - val_loss: 0.4470 - val_mae: 0.4909\n",
      "Epoch 15/300\n",
      "11834/11834 [==============================] - 0s 10us/step - loss: 0.3705 - mae: 0.4550 - val_loss: 0.4293 - val_mae: 0.4852\n",
      "Epoch 16/300\n",
      "11834/11834 [==============================] - 0s 10us/step - loss: 0.3710 - mae: 0.4568 - val_loss: 0.4259 - val_mae: 0.4835\n",
      "Epoch 17/300\n",
      "11834/11834 [==============================] - 0s 9us/step - loss: 0.3680 - mae: 0.4538 - val_loss: 0.4287 - val_mae: 0.4842\n",
      "Epoch 18/300\n",
      "11834/11834 [==============================] - 0s 9us/step - loss: 0.3665 - mae: 0.4526 - val_loss: 0.4255 - val_mae: 0.4840\n",
      "Epoch 19/300\n",
      "11834/11834 [==============================] - 0s 10us/step - loss: 0.3654 - mae: 0.4528 - val_loss: 0.4239 - val_mae: 0.4838\n",
      "Epoch 20/300\n",
      "11834/11834 [==============================] - 0s 17us/step - loss: 0.3636 - mae: 0.4510 - val_loss: 0.4271 - val_mae: 0.4885\n",
      "Epoch 21/300\n",
      "11834/11834 [==============================] - 0s 9us/step - loss: 0.3631 - mae: 0.4518 - val_loss: 0.4243 - val_mae: 0.4815\n",
      "Epoch 22/300\n",
      "11834/11834 [==============================] - 0s 9us/step - loss: 0.3643 - mae: 0.4527 - val_loss: 0.4310 - val_mae: 0.4956\n",
      "Epoch 23/300\n",
      "11834/11834 [==============================] - 0s 9us/step - loss: 0.3661 - mae: 0.4544 - val_loss: 0.4323 - val_mae: 0.4843\n",
      "Epoch 24/300\n",
      "11834/11834 [==============================] - 0s 9us/step - loss: 0.3630 - mae: 0.4518 - val_loss: 0.4211 - val_mae: 0.4807\n",
      "Epoch 25/300\n",
      "11834/11834 [==============================] - 0s 9us/step - loss: 0.3618 - mae: 0.4505 - val_loss: 0.4317 - val_mae: 0.4888\n",
      "Epoch 26/300\n",
      "11834/11834 [==============================] - 0s 9us/step - loss: 0.3616 - mae: 0.4501 - val_loss: 0.4257 - val_mae: 0.4822\n",
      "Epoch 27/300\n",
      "11834/11834 [==============================] - 0s 9us/step - loss: 0.3604 - mae: 0.4502 - val_loss: 0.4190 - val_mae: 0.4799\n",
      "Epoch 28/300\n",
      "11834/11834 [==============================] - 0s 10us/step - loss: 0.3609 - mae: 0.4503 - val_loss: 0.4177 - val_mae: 0.4813\n",
      "Epoch 29/300\n",
      "11834/11834 [==============================] - 0s 9us/step - loss: 0.3593 - mae: 0.4489 - val_loss: 0.4223 - val_mae: 0.4845\n",
      "Epoch 30/300\n",
      "11834/11834 [==============================] - 0s 10us/step - loss: 0.3580 - mae: 0.4495 - val_loss: 0.4204 - val_mae: 0.4782\n",
      "Epoch 31/300\n",
      "11834/11834 [==============================] - 0s 10us/step - loss: 0.3580 - mae: 0.4479 - val_loss: 0.4207 - val_mae: 0.4885\n",
      "Epoch 32/300\n",
      "11834/11834 [==============================] - 0s 9us/step - loss: 0.3577 - mae: 0.4476 - val_loss: 0.4167 - val_mae: 0.4821\n",
      "Epoch 33/300\n",
      "11834/11834 [==============================] - 0s 10us/step - loss: 0.3562 - mae: 0.4473 - val_loss: 0.4176 - val_mae: 0.4763\n",
      "Epoch 34/300\n",
      "11834/11834 [==============================] - 0s 10us/step - loss: 0.3562 - mae: 0.4477 - val_loss: 0.4147 - val_mae: 0.4780\n",
      "Epoch 35/300\n",
      "11834/11834 [==============================] - 0s 9us/step - loss: 0.3548 - mae: 0.4462 - val_loss: 0.4198 - val_mae: 0.4802\n",
      "Epoch 36/300\n",
      "11834/11834 [==============================] - 0s 9us/step - loss: 0.3548 - mae: 0.4472 - val_loss: 0.4159 - val_mae: 0.4765\n",
      "Epoch 37/300\n",
      "11834/11834 [==============================] - 0s 10us/step - loss: 0.3572 - mae: 0.4488 - val_loss: 0.4138 - val_mae: 0.4767\n",
      "Epoch 38/300\n",
      "11834/11834 [==============================] - 0s 10us/step - loss: 0.3544 - mae: 0.4457 - val_loss: 0.4125 - val_mae: 0.4777\n",
      "Epoch 39/300\n",
      "11834/11834 [==============================] - 0s 9us/step - loss: 0.3549 - mae: 0.4467 - val_loss: 0.4144 - val_mae: 0.4800\n",
      "Epoch 40/300\n",
      "11834/11834 [==============================] - 0s 9us/step - loss: 0.3558 - mae: 0.4478 - val_loss: 0.4245 - val_mae: 0.4876\n",
      "Epoch 41/300\n",
      "11834/11834 [==============================] - 0s 9us/step - loss: 0.3534 - mae: 0.4452 - val_loss: 0.4222 - val_mae: 0.4881\n",
      "Epoch 42/300\n",
      "11834/11834 [==============================] - 0s 8us/step - loss: 0.3549 - mae: 0.4473 - val_loss: 0.4104 - val_mae: 0.4787\n",
      "Epoch 43/300\n",
      "11834/11834 [==============================] - 0s 9us/step - loss: 0.3535 - mae: 0.4468 - val_loss: 0.4160 - val_mae: 0.4843\n",
      "Epoch 44/300\n",
      "11834/11834 [==============================] - 0s 9us/step - loss: 0.3541 - mae: 0.4461 - val_loss: 0.4162 - val_mae: 0.4818\n",
      "Epoch 45/300\n",
      "11834/11834 [==============================] - 0s 10us/step - loss: 0.3523 - mae: 0.4451 - val_loss: 0.4101 - val_mae: 0.4777\n",
      "Epoch 46/300\n",
      "11834/11834 [==============================] - 0s 12us/step - loss: 0.3528 - mae: 0.4449 - val_loss: 0.4151 - val_mae: 0.4762\n",
      "Epoch 47/300\n",
      "11834/11834 [==============================] - 0s 10us/step - loss: 0.3514 - mae: 0.4446 - val_loss: 0.4122 - val_mae: 0.4729\n",
      "Epoch 48/300\n",
      "11834/11834 [==============================] - 0s 10us/step - loss: 0.3508 - mae: 0.4439 - val_loss: 0.4103 - val_mae: 0.4791\n",
      "Epoch 49/300\n",
      "11834/11834 [==============================] - 0s 9us/step - loss: 0.3506 - mae: 0.4446 - val_loss: 0.4139 - val_mae: 0.4745\n",
      "Epoch 50/300\n",
      "11834/11834 [==============================] - 0s 9us/step - loss: 0.3500 - mae: 0.4434 - val_loss: 0.4153 - val_mae: 0.4792\n",
      "Epoch 51/300\n",
      "11834/11834 [==============================] - 0s 11us/step - loss: 0.3516 - mae: 0.4452 - val_loss: 0.4094 - val_mae: 0.4746\n",
      "Epoch 52/300\n",
      "11834/11834 [==============================] - 0s 17us/step - loss: 0.3499 - mae: 0.4438 - val_loss: 0.4152 - val_mae: 0.4773\n",
      "Epoch 53/300\n",
      "11834/11834 [==============================] - 0s 11us/step - loss: 0.3507 - mae: 0.4447 - val_loss: 0.4246 - val_mae: 0.4763\n",
      "Epoch 54/300\n",
      "11834/11834 [==============================] - 0s 10us/step - loss: 0.3519 - mae: 0.4446 - val_loss: 0.4102 - val_mae: 0.4728\n",
      "Epoch 55/300\n",
      "11834/11834 [==============================] - 0s 9us/step - loss: 0.3487 - mae: 0.4430 - val_loss: 0.4142 - val_mae: 0.4761\n",
      "Epoch 56/300\n",
      "11834/11834 [==============================] - 0s 10us/step - loss: 0.3483 - mae: 0.4424 - val_loss: 0.4062 - val_mae: 0.4731\n",
      "Epoch 57/300\n",
      "11834/11834 [==============================] - 0s 9us/step - loss: 0.3487 - mae: 0.4426 - val_loss: 0.4071 - val_mae: 0.4732\n",
      "Epoch 58/300\n",
      "11834/11834 [==============================] - 0s 10us/step - loss: 0.3486 - mae: 0.4430 - val_loss: 0.4086 - val_mae: 0.4729\n",
      "Epoch 59/300\n",
      "11834/11834 [==============================] - 0s 10us/step - loss: 0.3483 - mae: 0.4425 - val_loss: 0.4088 - val_mae: 0.4728\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 60/300\n",
      "11834/11834 [==============================] - 0s 11us/step - loss: 0.3474 - mae: 0.4421 - val_loss: 0.4087 - val_mae: 0.4728\n",
      "Epoch 61/300\n",
      "11834/11834 [==============================] - 0s 11us/step - loss: 0.3477 - mae: 0.4425 - val_loss: 0.4124 - val_mae: 0.4776\n",
      "Epoch 62/300\n",
      "11834/11834 [==============================] - 0s 11us/step - loss: 0.3472 - mae: 0.4423 - val_loss: 0.4148 - val_mae: 0.4773\n",
      "Epoch 63/300\n",
      "11834/11834 [==============================] - 0s 10us/step - loss: 0.3471 - mae: 0.4426 - val_loss: 0.4095 - val_mae: 0.4753\n",
      "Epoch 64/300\n",
      "11834/11834 [==============================] - 0s 10us/step - loss: 0.3489 - mae: 0.4442 - val_loss: 0.4119 - val_mae: 0.4803\n",
      "Epoch 65/300\n",
      "11834/11834 [==============================] - 0s 10us/step - loss: 0.3477 - mae: 0.4429 - val_loss: 0.4167 - val_mae: 0.4752\n",
      "Epoch 66/300\n",
      "11834/11834 [==============================] - 0s 10us/step - loss: 0.3465 - mae: 0.4425 - val_loss: 0.4063 - val_mae: 0.4739\n",
      "Epoch 67/300\n",
      "11834/11834 [==============================] - 0s 10us/step - loss: 0.3448 - mae: 0.4405 - val_loss: 0.4060 - val_mae: 0.4750\n",
      "Epoch 68/300\n",
      "11834/11834 [==============================] - 0s 9us/step - loss: 0.3458 - mae: 0.4416 - val_loss: 0.4086 - val_mae: 0.4779\n",
      "Epoch 69/300\n",
      "11834/11834 [==============================] - 0s 9us/step - loss: 0.3459 - mae: 0.4420 - val_loss: 0.4072 - val_mae: 0.4721\n",
      "Epoch 70/300\n",
      "11834/11834 [==============================] - 0s 9us/step - loss: 0.3458 - mae: 0.4416 - val_loss: 0.4057 - val_mae: 0.4730\n",
      "Epoch 71/300\n",
      "11834/11834 [==============================] - 0s 9us/step - loss: 0.3439 - mae: 0.4397 - val_loss: 0.4053 - val_mae: 0.4769\n",
      "Epoch 72/300\n",
      "11834/11834 [==============================] - 0s 9us/step - loss: 0.3478 - mae: 0.4438 - val_loss: 0.4073 - val_mae: 0.4761\n",
      "Epoch 73/300\n",
      "11834/11834 [==============================] - 0s 9us/step - loss: 0.3454 - mae: 0.4424 - val_loss: 0.4080 - val_mae: 0.4698\n",
      "Epoch 74/300\n",
      "11834/11834 [==============================] - 0s 9us/step - loss: 0.3466 - mae: 0.4416 - val_loss: 0.4063 - val_mae: 0.4714\n",
      "Epoch 75/300\n",
      "11834/11834 [==============================] - 0s 14us/step - loss: 0.3440 - mae: 0.4400 - val_loss: 0.4048 - val_mae: 0.4743\n",
      "Epoch 76/300\n",
      "11834/11834 [==============================] - 0s 9us/step - loss: 0.3470 - mae: 0.4429 - val_loss: 0.4144 - val_mae: 0.4790\n",
      "Epoch 77/300\n",
      "11834/11834 [==============================] - 0s 9us/step - loss: 0.3440 - mae: 0.4398 - val_loss: 0.4080 - val_mae: 0.4738\n",
      "Epoch 78/300\n",
      "11834/11834 [==============================] - 0s 9us/step - loss: 0.3440 - mae: 0.4409 - val_loss: 0.4043 - val_mae: 0.4783\n",
      "Epoch 79/300\n",
      "11834/11834 [==============================] - 0s 9us/step - loss: 0.3431 - mae: 0.4408 - val_loss: 0.4031 - val_mae: 0.4736\n",
      "Epoch 80/300\n",
      "11834/11834 [==============================] - 0s 10us/step - loss: 0.3436 - mae: 0.4403 - val_loss: 0.4043 - val_mae: 0.4752\n",
      "Epoch 81/300\n",
      "11834/11834 [==============================] - 0s 9us/step - loss: 0.3435 - mae: 0.4396 - val_loss: 0.4115 - val_mae: 0.4770\n",
      "Epoch 82/300\n",
      "11834/11834 [==============================] - 0s 9us/step - loss: 0.3441 - mae: 0.4411 - val_loss: 0.4060 - val_mae: 0.4782\n",
      "Epoch 83/300\n",
      "11834/11834 [==============================] - 0s 9us/step - loss: 0.3435 - mae: 0.4401 - val_loss: 0.4064 - val_mae: 0.4735\n",
      "Epoch 84/300\n",
      "11834/11834 [==============================] - 0s 9us/step - loss: 0.3420 - mae: 0.4387 - val_loss: 0.4077 - val_mae: 0.4752\n",
      "Epoch 85/300\n",
      "11834/11834 [==============================] - 0s 9us/step - loss: 0.3425 - mae: 0.4391 - val_loss: 0.4023 - val_mae: 0.4745\n",
      "Epoch 86/300\n",
      "11834/11834 [==============================] - 0s 9us/step - loss: 0.3435 - mae: 0.4404 - val_loss: 0.4064 - val_mae: 0.4778\n",
      "Epoch 87/300\n",
      "11834/11834 [==============================] - 0s 9us/step - loss: 0.3449 - mae: 0.4416 - val_loss: 0.4090 - val_mae: 0.4749\n",
      "Epoch 88/300\n",
      "11834/11834 [==============================] - 0s 9us/step - loss: 0.3416 - mae: 0.4389 - val_loss: 0.4041 - val_mae: 0.4730\n",
      "Epoch 89/300\n",
      "11834/11834 [==============================] - 0s 9us/step - loss: 0.3431 - mae: 0.4399 - val_loss: 0.4025 - val_mae: 0.4717\n",
      "Epoch 90/300\n",
      "11834/11834 [==============================] - 0s 11us/step - loss: 0.3428 - mae: 0.4400 - val_loss: 0.4047 - val_mae: 0.4738\n",
      "Epoch 91/300\n",
      "11834/11834 [==============================] - 0s 11us/step - loss: 0.3431 - mae: 0.4401 - val_loss: 0.4073 - val_mae: 0.4723\n",
      "Epoch 92/300\n",
      "11834/11834 [==============================] - 0s 10us/step - loss: 0.3419 - mae: 0.4387 - val_loss: 0.4019 - val_mae: 0.4711\n",
      "Epoch 93/300\n",
      "11834/11834 [==============================] - 0s 10us/step - loss: 0.3427 - mae: 0.4400 - val_loss: 0.4017 - val_mae: 0.4700\n",
      "Epoch 94/300\n",
      "11834/11834 [==============================] - 0s 11us/step - loss: 0.3426 - mae: 0.4405 - val_loss: 0.4064 - val_mae: 0.4719\n",
      "Epoch 95/300\n",
      "11834/11834 [==============================] - 0s 9us/step - loss: 0.3424 - mae: 0.4400 - val_loss: 0.4055 - val_mae: 0.4758\n",
      "Epoch 96/300\n",
      "11834/11834 [==============================] - 0s 9us/step - loss: 0.3425 - mae: 0.4408 - val_loss: 0.4100 - val_mae: 0.4749\n",
      "Epoch 97/300\n",
      "11834/11834 [==============================] - 0s 9us/step - loss: 0.3409 - mae: 0.4392 - val_loss: 0.4051 - val_mae: 0.4706\n",
      "Epoch 98/300\n",
      "11834/11834 [==============================] - 0s 9us/step - loss: 0.3433 - mae: 0.4410 - val_loss: 0.4013 - val_mae: 0.4681\n",
      "Epoch 99/300\n",
      "11834/11834 [==============================] - 0s 9us/step - loss: 0.3426 - mae: 0.4406 - val_loss: 0.4024 - val_mae: 0.4734\n",
      "Epoch 100/300\n",
      "11834/11834 [==============================] - 0s 9us/step - loss: 0.3416 - mae: 0.4390 - val_loss: 0.4083 - val_mae: 0.4764\n",
      "Epoch 101/300\n",
      "11834/11834 [==============================] - 0s 9us/step - loss: 0.3402 - mae: 0.4384 - val_loss: 0.4017 - val_mae: 0.4685\n",
      "Epoch 102/300\n",
      "11834/11834 [==============================] - 0s 9us/step - loss: 0.3411 - mae: 0.4394 - val_loss: 0.4016 - val_mae: 0.4735\n",
      "Epoch 103/300\n",
      "11834/11834 [==============================] - 0s 9us/step - loss: 0.3407 - mae: 0.4393 - val_loss: 0.4026 - val_mae: 0.4736\n",
      "Epoch 104/300\n",
      "11834/11834 [==============================] - 0s 9us/step - loss: 0.3393 - mae: 0.4380 - val_loss: 0.4071 - val_mae: 0.4723\n",
      "Epoch 105/300\n",
      "11834/11834 [==============================] - 0s 9us/step - loss: 0.3415 - mae: 0.4392 - val_loss: 0.4009 - val_mae: 0.4686\n",
      "Epoch 106/300\n",
      "11834/11834 [==============================] - 0s 10us/step - loss: 0.3399 - mae: 0.4383 - val_loss: 0.4092 - val_mae: 0.4744\n",
      "Epoch 107/300\n",
      "11834/11834 [==============================] - 0s 9us/step - loss: 0.3403 - mae: 0.4383 - val_loss: 0.4031 - val_mae: 0.4722\n",
      "Epoch 108/300\n",
      "11834/11834 [==============================] - 0s 9us/step - loss: 0.3421 - mae: 0.4398 - val_loss: 0.4046 - val_mae: 0.4752\n",
      "Epoch 109/300\n",
      "11834/11834 [==============================] - 0s 9us/step - loss: 0.3394 - mae: 0.4382 - val_loss: 0.4022 - val_mae: 0.4725\n",
      "Epoch 110/300\n",
      "11834/11834 [==============================] - 0s 11us/step - loss: 0.3416 - mae: 0.4399 - val_loss: 0.3990 - val_mae: 0.4699\n",
      "Epoch 111/300\n",
      "11834/11834 [==============================] - 0s 10us/step - loss: 0.3401 - mae: 0.4380 - val_loss: 0.4057 - val_mae: 0.4740\n",
      "Epoch 112/300\n",
      "11834/11834 [==============================] - 0s 9us/step - loss: 0.3420 - mae: 0.4399 - val_loss: 0.4018 - val_mae: 0.4710\n",
      "Epoch 113/300\n",
      "11834/11834 [==============================] - 0s 9us/step - loss: 0.3386 - mae: 0.4366 - val_loss: 0.4013 - val_mae: 0.4742\n",
      "Epoch 114/300\n",
      "11834/11834 [==============================] - 0s 9us/step - loss: 0.3404 - mae: 0.4395 - val_loss: 0.4089 - val_mae: 0.4751\n",
      "Epoch 115/300\n",
      "11834/11834 [==============================] - 0s 9us/step - loss: 0.3411 - mae: 0.4392 - val_loss: 0.4024 - val_mae: 0.4674\n",
      "Epoch 116/300\n",
      "11834/11834 [==============================] - 0s 9us/step - loss: 0.3418 - mae: 0.4399 - val_loss: 0.4045 - val_mae: 0.4705\n",
      "Epoch 117/300\n",
      "11834/11834 [==============================] - 0s 9us/step - loss: 0.3398 - mae: 0.4387 - val_loss: 0.3994 - val_mae: 0.4719\n",
      "Epoch 118/300\n",
      "11834/11834 [==============================] - 0s 9us/step - loss: 0.3384 - mae: 0.4370 - val_loss: 0.4002 - val_mae: 0.4677\n",
      "Epoch 119/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11834/11834 [==============================] - 0s 9us/step - loss: 0.3387 - mae: 0.4377 - val_loss: 0.4096 - val_mae: 0.4770\n",
      "Epoch 120/300\n",
      "11834/11834 [==============================] - 0s 9us/step - loss: 0.3405 - mae: 0.4388 - val_loss: 0.4047 - val_mae: 0.4765\n",
      "Epoch 121/300\n",
      "11834/11834 [==============================] - 0s 9us/step - loss: 0.3397 - mae: 0.4380 - val_loss: 0.4025 - val_mae: 0.4737\n",
      "Epoch 122/300\n",
      "11834/11834 [==============================] - 0s 9us/step - loss: 0.3400 - mae: 0.4382 - val_loss: 0.3983 - val_mae: 0.4726\n",
      "Epoch 123/300\n",
      "11834/11834 [==============================] - 0s 10us/step - loss: 0.3388 - mae: 0.4380 - val_loss: 0.4002 - val_mae: 0.4707\n",
      "Epoch 124/300\n",
      "11834/11834 [==============================] - 0s 10us/step - loss: 0.3365 - mae: 0.4359 - val_loss: 0.4022 - val_mae: 0.4685\n",
      "Epoch 125/300\n",
      "11834/11834 [==============================] - 0s 9us/step - loss: 0.3364 - mae: 0.4364 - val_loss: 0.4013 - val_mae: 0.4728\n",
      "Epoch 126/300\n",
      "11834/11834 [==============================] - 0s 9us/step - loss: 0.3389 - mae: 0.4375 - val_loss: 0.4072 - val_mae: 0.4742\n",
      "Epoch 127/300\n",
      "11834/11834 [==============================] - 0s 9us/step - loss: 0.3392 - mae: 0.4378 - val_loss: 0.4001 - val_mae: 0.4724\n",
      "Epoch 128/300\n",
      "11834/11834 [==============================] - 0s 9us/step - loss: 0.3389 - mae: 0.4378 - val_loss: 0.4002 - val_mae: 0.4711\n",
      "Epoch 129/300\n",
      "11834/11834 [==============================] - 0s 9us/step - loss: 0.3377 - mae: 0.4361 - val_loss: 0.4059 - val_mae: 0.4695\n",
      "Epoch 130/300\n",
      "11834/11834 [==============================] - 0s 10us/step - loss: 0.3369 - mae: 0.4360 - val_loss: 0.4022 - val_mae: 0.4683\n",
      "Epoch 131/300\n",
      "11834/11834 [==============================] - 0s 10us/step - loss: 0.3382 - mae: 0.4379 - val_loss: 0.4080 - val_mae: 0.4723\n",
      "Epoch 132/300\n",
      "11834/11834 [==============================] - 0s 9us/step - loss: 0.3376 - mae: 0.4366 - val_loss: 0.4016 - val_mae: 0.4731\n",
      "Epoch 133/300\n",
      "11834/11834 [==============================] - 0s 9us/step - loss: 0.3374 - mae: 0.4371 - val_loss: 0.3980 - val_mae: 0.4674\n",
      "Epoch 134/300\n",
      "11834/11834 [==============================] - 0s 9us/step - loss: 0.3376 - mae: 0.4371 - val_loss: 0.4048 - val_mae: 0.4736\n",
      "Epoch 135/300\n",
      "11834/11834 [==============================] - 0s 10us/step - loss: 0.3386 - mae: 0.4391 - val_loss: 0.4019 - val_mae: 0.4682\n",
      "Epoch 136/300\n",
      "11834/11834 [==============================] - 0s 9us/step - loss: 0.3386 - mae: 0.4383 - val_loss: 0.4018 - val_mae: 0.4674\n",
      "Epoch 137/300\n",
      "11834/11834 [==============================] - 0s 9us/step - loss: 0.3374 - mae: 0.4361 - val_loss: 0.3999 - val_mae: 0.4697\n",
      "Epoch 138/300\n",
      "11834/11834 [==============================] - 0s 9us/step - loss: 0.3383 - mae: 0.4380 - val_loss: 0.4012 - val_mae: 0.4718\n",
      "Epoch 139/300\n",
      "11834/11834 [==============================] - 0s 9us/step - loss: 0.3370 - mae: 0.4372 - val_loss: 0.4008 - val_mae: 0.4693\n",
      "Epoch 140/300\n",
      "11834/11834 [==============================] - 0s 8us/step - loss: 0.3369 - mae: 0.4358 - val_loss: 0.4032 - val_mae: 0.4742\n",
      "Epoch 141/300\n",
      "11834/11834 [==============================] - 0s 9us/step - loss: 0.3370 - mae: 0.4358 - val_loss: 0.3978 - val_mae: 0.4701\n",
      "Epoch 142/300\n",
      "11834/11834 [==============================] - 0s 10us/step - loss: 0.3372 - mae: 0.4372 - val_loss: 0.3980 - val_mae: 0.4690\n",
      "Epoch 143/300\n",
      "11834/11834 [==============================] - 0s 9us/step - loss: 0.3389 - mae: 0.4375 - val_loss: 0.4025 - val_mae: 0.4703\n",
      "Epoch 144/300\n",
      "11834/11834 [==============================] - 0s 9us/step - loss: 0.3360 - mae: 0.4351 - val_loss: 0.3974 - val_mae: 0.4672\n",
      "Epoch 145/300\n",
      "11834/11834 [==============================] - 0s 16us/step - loss: 0.3366 - mae: 0.4359 - val_loss: 0.4069 - val_mae: 0.4719\n",
      "Epoch 146/300\n",
      "11834/11834 [==============================] - 0s 9us/step - loss: 0.3368 - mae: 0.4368 - val_loss: 0.4087 - val_mae: 0.4748\n",
      "Epoch 147/300\n",
      "11834/11834 [==============================] - 0s 9us/step - loss: 0.3358 - mae: 0.4356 - val_loss: 0.4009 - val_mae: 0.4737\n",
      "Epoch 148/300\n",
      "11834/11834 [==============================] - 0s 9us/step - loss: 0.3349 - mae: 0.4356 - val_loss: 0.4011 - val_mae: 0.4724\n",
      "Epoch 149/300\n",
      "11834/11834 [==============================] - 0s 9us/step - loss: 0.3366 - mae: 0.4356 - val_loss: 0.4034 - val_mae: 0.4778\n",
      "Epoch 150/300\n",
      "11834/11834 [==============================] - 0s 9us/step - loss: 0.3352 - mae: 0.4356 - val_loss: 0.3992 - val_mae: 0.4659\n",
      "Epoch 151/300\n",
      "11834/11834 [==============================] - 0s 9us/step - loss: 0.3363 - mae: 0.4362 - val_loss: 0.4008 - val_mae: 0.4702\n",
      "Epoch 152/300\n",
      "11834/11834 [==============================] - 0s 9us/step - loss: 0.3360 - mae: 0.4361 - val_loss: 0.4032 - val_mae: 0.4704\n",
      "Epoch 153/300\n",
      "11834/11834 [==============================] - 0s 9us/step - loss: 0.3362 - mae: 0.4364 - val_loss: 0.4085 - val_mae: 0.4740\n",
      "Epoch 154/300\n",
      "11834/11834 [==============================] - 0s 10us/step - loss: 0.3369 - mae: 0.4357 - val_loss: 0.3960 - val_mae: 0.4672\n",
      "Epoch 155/300\n",
      "11834/11834 [==============================] - 0s 11us/step - loss: 0.3342 - mae: 0.4343 - val_loss: 0.4007 - val_mae: 0.4729\n",
      "Epoch 156/300\n",
      "11834/11834 [==============================] - 0s 10us/step - loss: 0.3362 - mae: 0.4364 - val_loss: 0.4094 - val_mae: 0.4723\n",
      "Epoch 157/300\n",
      "11834/11834 [==============================] - 0s 10us/step - loss: 0.3350 - mae: 0.4352 - val_loss: 0.3988 - val_mae: 0.4745\n",
      "Epoch 158/300\n",
      "11834/11834 [==============================] - 0s 9us/step - loss: 0.3355 - mae: 0.4351 - val_loss: 0.4045 - val_mae: 0.4739\n",
      "Epoch 159/300\n",
      "11834/11834 [==============================] - 0s 10us/step - loss: 0.3347 - mae: 0.4352 - val_loss: 0.4014 - val_mae: 0.4691\n",
      "Epoch 160/300\n",
      "11834/11834 [==============================] - 0s 9us/step - loss: 0.3332 - mae: 0.4342 - val_loss: 0.3962 - val_mae: 0.4677\n",
      "Epoch 161/300\n",
      "11834/11834 [==============================] - 0s 9us/step - loss: 0.3338 - mae: 0.4344 - val_loss: 0.3967 - val_mae: 0.4652\n",
      "Epoch 162/300\n",
      "11834/11834 [==============================] - 0s 10us/step - loss: 0.3334 - mae: 0.4343 - val_loss: 0.3978 - val_mae: 0.4675\n",
      "Epoch 163/300\n",
      "11834/11834 [==============================] - 0s 9us/step - loss: 0.3338 - mae: 0.4341 - val_loss: 0.4011 - val_mae: 0.4703\n",
      "Epoch 164/300\n",
      "11834/11834 [==============================] - 0s 9us/step - loss: 0.3329 - mae: 0.4335 - val_loss: 0.3998 - val_mae: 0.4702\n",
      "Epoch 165/300\n",
      "11834/11834 [==============================] - 0s 9us/step - loss: 0.3333 - mae: 0.4346 - val_loss: 0.4057 - val_mae: 0.4714\n",
      "Epoch 166/300\n",
      "11834/11834 [==============================] - 0s 9us/step - loss: 0.3337 - mae: 0.4338 - val_loss: 0.3974 - val_mae: 0.4690\n",
      "Epoch 167/300\n",
      "11834/11834 [==============================] - 0s 9us/step - loss: 0.3353 - mae: 0.4360 - val_loss: 0.4004 - val_mae: 0.4703\n",
      "Epoch 168/300\n",
      "11834/11834 [==============================] - 0s 9us/step - loss: 0.3333 - mae: 0.4337 - val_loss: 0.3965 - val_mae: 0.4684\n",
      "Epoch 169/300\n",
      "11834/11834 [==============================] - 0s 10us/step - loss: 0.3324 - mae: 0.4329 - val_loss: 0.3979 - val_mae: 0.4684\n",
      "Epoch 170/300\n",
      "11834/11834 [==============================] - 0s 9us/step - loss: 0.3340 - mae: 0.4348 - val_loss: 0.3992 - val_mae: 0.4722\n",
      "Epoch 171/300\n",
      "11834/11834 [==============================] - 0s 8us/step - loss: 0.3338 - mae: 0.4347 - val_loss: 0.4006 - val_mae: 0.4695\n",
      "Epoch 172/300\n",
      "11834/11834 [==============================] - 0s 9us/step - loss: 0.3346 - mae: 0.4351 - val_loss: 0.3947 - val_mae: 0.4679\n",
      "Epoch 173/300\n",
      "11834/11834 [==============================] - 0s 8us/step - loss: 0.3326 - mae: 0.4342 - val_loss: 0.3991 - val_mae: 0.4689\n",
      "Epoch 174/300\n",
      "11834/11834 [==============================] - 0s 9us/step - loss: 0.3337 - mae: 0.4354 - val_loss: 0.4070 - val_mae: 0.4711\n",
      "Epoch 175/300\n",
      "11834/11834 [==============================] - 0s 9us/step - loss: 0.3350 - mae: 0.4364 - val_loss: 0.3984 - val_mae: 0.4684\n",
      "Epoch 176/300\n",
      "11834/11834 [==============================] - 0s 8us/step - loss: 0.3330 - mae: 0.4333 - val_loss: 0.3980 - val_mae: 0.4708\n",
      "Epoch 177/300\n",
      "11834/11834 [==============================] - 0s 8us/step - loss: 0.3334 - mae: 0.4352 - val_loss: 0.3958 - val_mae: 0.4679\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 178/300\n",
      "11834/11834 [==============================] - 0s 9us/step - loss: 0.3331 - mae: 0.4343 - val_loss: 0.3993 - val_mae: 0.4704\n",
      "Epoch 179/300\n",
      "11834/11834 [==============================] - 0s 8us/step - loss: 0.3335 - mae: 0.4344 - val_loss: 0.4013 - val_mae: 0.4739\n",
      "Epoch 180/300\n",
      "11834/11834 [==============================] - 0s 9us/step - loss: 0.3339 - mae: 0.4349 - val_loss: 0.3987 - val_mae: 0.4683\n",
      "Epoch 181/300\n",
      "11834/11834 [==============================] - 0s 8us/step - loss: 0.3324 - mae: 0.4326 - val_loss: 0.4018 - val_mae: 0.4728\n",
      "Epoch 182/300\n",
      "11834/11834 [==============================] - 0s 9us/step - loss: 0.3340 - mae: 0.4344 - val_loss: 0.3958 - val_mae: 0.4655\n",
      "Epoch 183/300\n",
      "11834/11834 [==============================] - 0s 8us/step - loss: 0.3322 - mae: 0.4333 - val_loss: 0.3967 - val_mae: 0.4662\n",
      "Epoch 184/300\n",
      "11834/11834 [==============================] - 0s 9us/step - loss: 0.3327 - mae: 0.4331 - val_loss: 0.3980 - val_mae: 0.4636\n",
      "Epoch 185/300\n",
      "11834/11834 [==============================] - 0s 10us/step - loss: 0.3316 - mae: 0.4325 - val_loss: 0.3942 - val_mae: 0.4698\n",
      "Epoch 186/300\n",
      "11834/11834 [==============================] - 0s 9us/step - loss: 0.3325 - mae: 0.4346 - val_loss: 0.3975 - val_mae: 0.4661\n",
      "Epoch 187/300\n",
      "11834/11834 [==============================] - 0s 9us/step - loss: 0.3322 - mae: 0.4331 - val_loss: 0.4028 - val_mae: 0.4742\n",
      "Epoch 188/300\n",
      "11834/11834 [==============================] - 0s 9us/step - loss: 0.3327 - mae: 0.4342 - val_loss: 0.4006 - val_mae: 0.4741\n",
      "Epoch 189/300\n",
      "11834/11834 [==============================] - 0s 9us/step - loss: 0.3321 - mae: 0.4339 - val_loss: 0.4006 - val_mae: 0.4706\n",
      "Epoch 190/300\n",
      "11834/11834 [==============================] - 0s 10us/step - loss: 0.3332 - mae: 0.4344 - val_loss: 0.3997 - val_mae: 0.4695\n",
      "Epoch 191/300\n",
      "11834/11834 [==============================] - 0s 9us/step - loss: 0.3320 - mae: 0.4329 - val_loss: 0.3940 - val_mae: 0.4672\n",
      "Epoch 192/300\n",
      "11834/11834 [==============================] - 0s 9us/step - loss: 0.3307 - mae: 0.4326 - val_loss: 0.3931 - val_mae: 0.4639\n",
      "Epoch 193/300\n",
      "11834/11834 [==============================] - 0s 9us/step - loss: 0.3320 - mae: 0.4328 - val_loss: 0.3983 - val_mae: 0.4685\n",
      "Epoch 194/300\n",
      "11834/11834 [==============================] - 0s 11us/step - loss: 0.3317 - mae: 0.4335 - val_loss: 0.3953 - val_mae: 0.4664\n",
      "Epoch 195/300\n",
      "11834/11834 [==============================] - 0s 10us/step - loss: 0.3336 - mae: 0.4346 - val_loss: 0.4005 - val_mae: 0.4676\n",
      "Epoch 196/300\n",
      "11834/11834 [==============================] - 0s 11us/step - loss: 0.3308 - mae: 0.4326 - val_loss: 0.3977 - val_mae: 0.4707\n",
      "Epoch 197/300\n",
      "11834/11834 [==============================] - 0s 11us/step - loss: 0.3319 - mae: 0.4334 - val_loss: 0.3936 - val_mae: 0.4653\n",
      "Epoch 198/300\n",
      "11834/11834 [==============================] - 0s 10us/step - loss: 0.3304 - mae: 0.4321 - val_loss: 0.3946 - val_mae: 0.4672\n",
      "Epoch 199/300\n",
      "11834/11834 [==============================] - 0s 10us/step - loss: 0.3312 - mae: 0.4330 - val_loss: 0.3961 - val_mae: 0.4673\n",
      "Epoch 200/300\n",
      "11834/11834 [==============================] - 0s 13us/step - loss: 0.3314 - mae: 0.4330 - val_loss: 0.3946 - val_mae: 0.4648\n",
      "Epoch 201/300\n",
      "11834/11834 [==============================] - 0s 11us/step - loss: 0.3317 - mae: 0.4327 - val_loss: 0.3979 - val_mae: 0.4686\n",
      "Epoch 202/300\n",
      "11834/11834 [==============================] - 0s 10us/step - loss: 0.3310 - mae: 0.4330 - val_loss: 0.3960 - val_mae: 0.4675\n",
      "Epoch 203/300\n",
      "11834/11834 [==============================] - 0s 9us/step - loss: 0.3304 - mae: 0.4325 - val_loss: 0.3957 - val_mae: 0.4656\n",
      "Epoch 204/300\n",
      "11834/11834 [==============================] - 0s 9us/step - loss: 0.3318 - mae: 0.4332 - val_loss: 0.3930 - val_mae: 0.4680\n",
      "Epoch 205/300\n",
      "11834/11834 [==============================] - 0s 9us/step - loss: 0.3316 - mae: 0.4327 - val_loss: 0.3954 - val_mae: 0.4704\n",
      "Epoch 206/300\n",
      "11834/11834 [==============================] - 0s 9us/step - loss: 0.3318 - mae: 0.4336 - val_loss: 0.3961 - val_mae: 0.4646\n",
      "Epoch 207/300\n",
      "11834/11834 [==============================] - 0s 9us/step - loss: 0.3298 - mae: 0.4316 - val_loss: 0.3931 - val_mae: 0.4660\n",
      "Epoch 208/300\n",
      "11834/11834 [==============================] - 0s 11us/step - loss: 0.3309 - mae: 0.4331 - val_loss: 0.3982 - val_mae: 0.4683\n",
      "Epoch 209/300\n",
      "11834/11834 [==============================] - 0s 10us/step - loss: 0.3295 - mae: 0.4314 - val_loss: 0.3950 - val_mae: 0.4682\n",
      "Epoch 210/300\n",
      "11834/11834 [==============================] - 0s 9us/step - loss: 0.3299 - mae: 0.4308 - val_loss: 0.4019 - val_mae: 0.4751\n",
      "Epoch 211/300\n",
      "11834/11834 [==============================] - 0s 9us/step - loss: 0.3308 - mae: 0.4326 - val_loss: 0.3951 - val_mae: 0.4644\n",
      "Epoch 212/300\n",
      "11834/11834 [==============================] - 0s 9us/step - loss: 0.3307 - mae: 0.4328 - val_loss: 0.4011 - val_mae: 0.4686\n",
      "Epoch 213/300\n",
      "11834/11834 [==============================] - 0s 10us/step - loss: 0.3304 - mae: 0.4319 - val_loss: 0.3919 - val_mae: 0.4680\n",
      "Epoch 214/300\n",
      "11834/11834 [==============================] - 0s 9us/step - loss: 0.3290 - mae: 0.4313 - val_loss: 0.3996 - val_mae: 0.4671\n",
      "Epoch 215/300\n",
      "11834/11834 [==============================] - 0s 9us/step - loss: 0.3288 - mae: 0.4312 - val_loss: 0.3982 - val_mae: 0.4745\n",
      "Epoch 216/300\n",
      "11834/11834 [==============================] - 0s 9us/step - loss: 0.3287 - mae: 0.4315 - val_loss: 0.3948 - val_mae: 0.4672\n",
      "Epoch 217/300\n",
      "11834/11834 [==============================] - 0s 9us/step - loss: 0.3299 - mae: 0.4314 - val_loss: 0.3957 - val_mae: 0.4689\n",
      "Epoch 218/300\n",
      "11834/11834 [==============================] - 0s 10us/step - loss: 0.3282 - mae: 0.4314 - val_loss: 0.3973 - val_mae: 0.4663\n",
      "Epoch 219/300\n",
      "11834/11834 [==============================] - 0s 9us/step - loss: 0.3295 - mae: 0.4311 - val_loss: 0.3950 - val_mae: 0.4684\n",
      "Epoch 220/300\n",
      "11834/11834 [==============================] - 0s 9us/step - loss: 0.3289 - mae: 0.4313 - val_loss: 0.3940 - val_mae: 0.4644\n",
      "Epoch 221/300\n",
      "11834/11834 [==============================] - 0s 9us/step - loss: 0.3302 - mae: 0.4319 - val_loss: 0.3940 - val_mae: 0.4644\n",
      "Epoch 222/300\n",
      "11834/11834 [==============================] - 0s 9us/step - loss: 0.3288 - mae: 0.4312 - val_loss: 0.3938 - val_mae: 0.4667\n",
      "Epoch 223/300\n",
      "11834/11834 [==============================] - 0s 9us/step - loss: 0.3281 - mae: 0.4302 - val_loss: 0.3895 - val_mae: 0.4649\n",
      "Epoch 224/300\n",
      "11834/11834 [==============================] - 0s 9us/step - loss: 0.3273 - mae: 0.4298 - val_loss: 0.3952 - val_mae: 0.4694\n",
      "Epoch 225/300\n",
      "11834/11834 [==============================] - 0s 9us/step - loss: 0.3298 - mae: 0.4331 - val_loss: 0.4045 - val_mae: 0.4693\n",
      "Epoch 226/300\n",
      "11834/11834 [==============================] - 0s 9us/step - loss: 0.3304 - mae: 0.4322 - val_loss: 0.3917 - val_mae: 0.4652\n",
      "Epoch 227/300\n",
      "11834/11834 [==============================] - 0s 9us/step - loss: 0.3274 - mae: 0.4306 - val_loss: 0.3912 - val_mae: 0.4639\n",
      "Epoch 228/300\n",
      "11834/11834 [==============================] - 0s 11us/step - loss: 0.3286 - mae: 0.4321 - val_loss: 0.3939 - val_mae: 0.4676\n",
      "Epoch 229/300\n",
      "11834/11834 [==============================] - 0s 9us/step - loss: 0.3285 - mae: 0.4311 - val_loss: 0.3943 - val_mae: 0.4667\n",
      "Epoch 230/300\n",
      "11834/11834 [==============================] - 0s 9us/step - loss: 0.3265 - mae: 0.4301 - val_loss: 0.4013 - val_mae: 0.4678\n",
      "Epoch 231/300\n",
      "11834/11834 [==============================] - 0s 9us/step - loss: 0.3326 - mae: 0.4346 - val_loss: 0.3902 - val_mae: 0.4643\n",
      "Epoch 232/300\n",
      "11834/11834 [==============================] - 0s 10us/step - loss: 0.3285 - mae: 0.4310 - val_loss: 0.3998 - val_mae: 0.4668\n",
      "Epoch 233/300\n",
      "11834/11834 [==============================] - 0s 12us/step - loss: 0.3289 - mae: 0.4315 - val_loss: 0.3895 - val_mae: 0.4650\n",
      "Epoch 234/300\n",
      "11834/11834 [==============================] - 0s 11us/step - loss: 0.3273 - mae: 0.4300 - val_loss: 0.3932 - val_mae: 0.4684\n",
      "Epoch 235/300\n",
      "11834/11834 [==============================] - 0s 11us/step - loss: 0.3270 - mae: 0.4292 - val_loss: 0.3952 - val_mae: 0.4692\n",
      "Epoch 236/300\n",
      "11834/11834 [==============================] - 0s 16us/step - loss: 0.3283 - mae: 0.4304 - val_loss: 0.3882 - val_mae: 0.4620\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 237/300\n",
      "11834/11834 [==============================] - 0s 11us/step - loss: 0.3270 - mae: 0.4295 - val_loss: 0.3923 - val_mae: 0.4700\n",
      "Epoch 238/300\n",
      "11834/11834 [==============================] - 0s 10us/step - loss: 0.3288 - mae: 0.4315 - val_loss: 0.3938 - val_mae: 0.4641\n",
      "Epoch 239/300\n",
      "11834/11834 [==============================] - 0s 9us/step - loss: 0.3269 - mae: 0.4297 - val_loss: 0.3963 - val_mae: 0.4645\n",
      "Epoch 240/300\n",
      "11834/11834 [==============================] - 0s 9us/step - loss: 0.3282 - mae: 0.4305 - val_loss: 0.3912 - val_mae: 0.4636\n",
      "Epoch 241/300\n",
      "11834/11834 [==============================] - 0s 9us/step - loss: 0.3258 - mae: 0.4288 - val_loss: 0.3904 - val_mae: 0.4661\n",
      "Epoch 242/300\n",
      "11834/11834 [==============================] - 0s 9us/step - loss: 0.3267 - mae: 0.4300 - val_loss: 0.3990 - val_mae: 0.4697\n",
      "Epoch 243/300\n",
      "11834/11834 [==============================] - 0s 8us/step - loss: 0.3288 - mae: 0.4318 - val_loss: 0.3894 - val_mae: 0.4680\n",
      "Epoch 244/300\n",
      "11834/11834 [==============================] - 0s 9us/step - loss: 0.3276 - mae: 0.4306 - val_loss: 0.4098 - val_mae: 0.4731\n",
      "Epoch 245/300\n",
      "11834/11834 [==============================] - 0s 9us/step - loss: 0.3273 - mae: 0.4311 - val_loss: 0.3900 - val_mae: 0.4648\n",
      "Epoch 246/300\n",
      "11834/11834 [==============================] - 0s 9us/step - loss: 0.3261 - mae: 0.4297 - val_loss: 0.3945 - val_mae: 0.4662\n",
      "Epoch 247/300\n",
      "11834/11834 [==============================] - 0s 9us/step - loss: 0.3291 - mae: 0.4315 - val_loss: 0.3956 - val_mae: 0.4672\n",
      "Epoch 248/300\n",
      "11834/11834 [==============================] - 0s 9us/step - loss: 0.3284 - mae: 0.4308 - val_loss: 0.3936 - val_mae: 0.4657\n",
      "Epoch 249/300\n",
      "11834/11834 [==============================] - 0s 9us/step - loss: 0.3257 - mae: 0.4290 - val_loss: 0.3934 - val_mae: 0.4650\n",
      "Epoch 250/300\n",
      "11834/11834 [==============================] - 0s 9us/step - loss: 0.3262 - mae: 0.4300 - val_loss: 0.3926 - val_mae: 0.4657\n",
      "Epoch 251/300\n",
      "11834/11834 [==============================] - 0s 9us/step - loss: 0.3264 - mae: 0.4294 - val_loss: 0.3930 - val_mae: 0.4678\n",
      "Epoch 252/300\n",
      "11834/11834 [==============================] - 0s 10us/step - loss: 0.3255 - mae: 0.4292 - val_loss: 0.3922 - val_mae: 0.4655\n",
      "Epoch 253/300\n",
      "11834/11834 [==============================] - 0s 9us/step - loss: 0.3262 - mae: 0.4301 - val_loss: 0.3931 - val_mae: 0.4681\n",
      "Epoch 254/300\n",
      "11834/11834 [==============================] - 0s 9us/step - loss: 0.3248 - mae: 0.4277 - val_loss: 0.3911 - val_mae: 0.4638\n",
      "Epoch 255/300\n",
      "11834/11834 [==============================] - 0s 9us/step - loss: 0.3252 - mae: 0.4284 - val_loss: 0.3876 - val_mae: 0.4631\n",
      "Epoch 256/300\n",
      "11834/11834 [==============================] - 0s 10us/step - loss: 0.3245 - mae: 0.4276 - val_loss: 0.3910 - val_mae: 0.4682\n",
      "Epoch 257/300\n",
      "11834/11834 [==============================] - 0s 8us/step - loss: 0.3253 - mae: 0.4295 - val_loss: 0.3894 - val_mae: 0.4648\n",
      "Epoch 258/300\n",
      "11834/11834 [==============================] - 0s 9us/step - loss: 0.3242 - mae: 0.4278 - val_loss: 0.3957 - val_mae: 0.4659\n",
      "Epoch 259/300\n",
      "11834/11834 [==============================] - 0s 9us/step - loss: 0.3254 - mae: 0.4286 - val_loss: 0.3906 - val_mae: 0.4674\n",
      "Epoch 260/300\n",
      "11834/11834 [==============================] - 0s 9us/step - loss: 0.3257 - mae: 0.4290 - val_loss: 0.3936 - val_mae: 0.4658\n",
      "Epoch 261/300\n",
      "11834/11834 [==============================] - 0s 9us/step - loss: 0.3258 - mae: 0.4288 - val_loss: 0.3882 - val_mae: 0.4641\n",
      "Epoch 262/300\n",
      "11834/11834 [==============================] - 0s 9us/step - loss: 0.3256 - mae: 0.4286 - val_loss: 0.3990 - val_mae: 0.4663\n",
      "Epoch 263/300\n",
      "11834/11834 [==============================] - 0s 9us/step - loss: 0.3286 - mae: 0.4323 - val_loss: 0.3898 - val_mae: 0.4645\n",
      "Epoch 264/300\n",
      "11834/11834 [==============================] - 0s 10us/step - loss: 0.3256 - mae: 0.4291 - val_loss: 0.3926 - val_mae: 0.4667\n",
      "Epoch 265/300\n",
      "11834/11834 [==============================] - 0s 9us/step - loss: 0.3251 - mae: 0.4289 - val_loss: 0.3924 - val_mae: 0.4659\n",
      "Epoch 266/300\n",
      "11834/11834 [==============================] - 0s 9us/step - loss: 0.3252 - mae: 0.4297 - val_loss: 0.3882 - val_mae: 0.4642\n",
      "Epoch 267/300\n",
      "11834/11834 [==============================] - 0s 9us/step - loss: 0.3263 - mae: 0.4291 - val_loss: 0.3945 - val_mae: 0.4662\n",
      "Epoch 268/300\n",
      "11834/11834 [==============================] - 0s 9us/step - loss: 0.3246 - mae: 0.4286 - val_loss: 0.3954 - val_mae: 0.4679\n",
      "Epoch 269/300\n",
      "11834/11834 [==============================] - 0s 9us/step - loss: 0.3249 - mae: 0.4287 - val_loss: 0.3944 - val_mae: 0.4648\n",
      "Epoch 270/300\n",
      "11834/11834 [==============================] - 0s 9us/step - loss: 0.3247 - mae: 0.4288 - val_loss: 0.3885 - val_mae: 0.4635\n",
      "Epoch 271/300\n",
      "11834/11834 [==============================] - 0s 9us/step - loss: 0.3239 - mae: 0.4273 - val_loss: 0.3932 - val_mae: 0.4682\n",
      "Epoch 272/300\n",
      "11834/11834 [==============================] - 0s 9us/step - loss: 0.3240 - mae: 0.4276 - val_loss: 0.3897 - val_mae: 0.4633\n",
      "Epoch 273/300\n",
      "11834/11834 [==============================] - 0s 10us/step - loss: 0.3268 - mae: 0.4309 - val_loss: 0.3935 - val_mae: 0.4627\n",
      "Epoch 274/300\n",
      "11834/11834 [==============================] - 0s 9us/step - loss: 0.3275 - mae: 0.4306 - val_loss: 0.3866 - val_mae: 0.4610\n",
      "Epoch 275/300\n",
      "11834/11834 [==============================] - 0s 9us/step - loss: 0.3241 - mae: 0.4276 - val_loss: 0.3864 - val_mae: 0.4618\n",
      "Epoch 276/300\n",
      "11834/11834 [==============================] - 0s 9us/step - loss: 0.3264 - mae: 0.4305 - val_loss: 0.3923 - val_mae: 0.4672\n",
      "Epoch 277/300\n",
      "11834/11834 [==============================] - 0s 10us/step - loss: 0.3243 - mae: 0.4276 - val_loss: 0.3881 - val_mae: 0.4652\n",
      "Epoch 278/300\n",
      "11834/11834 [==============================] - 0s 9us/step - loss: 0.3239 - mae: 0.4282 - val_loss: 0.3892 - val_mae: 0.4650\n",
      "Epoch 279/300\n",
      "11834/11834 [==============================] - 0s 9us/step - loss: 0.3260 - mae: 0.4300 - val_loss: 0.3872 - val_mae: 0.4626\n",
      "Epoch 280/300\n",
      "11834/11834 [==============================] - 0s 9us/step - loss: 0.3233 - mae: 0.4271 - val_loss: 0.3894 - val_mae: 0.4618\n",
      "Epoch 281/300\n",
      "11834/11834 [==============================] - 0s 9us/step - loss: 0.3241 - mae: 0.4283 - val_loss: 0.3868 - val_mae: 0.4659\n",
      "Epoch 282/300\n",
      "11834/11834 [==============================] - 0s 9us/step - loss: 0.3242 - mae: 0.4274 - val_loss: 0.3900 - val_mae: 0.4678\n",
      "Epoch 283/300\n",
      "11834/11834 [==============================] - 0s 9us/step - loss: 0.3232 - mae: 0.4271 - val_loss: 0.3868 - val_mae: 0.4631\n",
      "Epoch 284/300\n",
      "11834/11834 [==============================] - 0s 9us/step - loss: 0.3238 - mae: 0.4276 - val_loss: 0.3917 - val_mae: 0.4696\n",
      "Epoch 285/300\n",
      "11834/11834 [==============================] - 0s 9us/step - loss: 0.3231 - mae: 0.4274 - val_loss: 0.3922 - val_mae: 0.4699\n",
      "Epoch 286/300\n",
      "11834/11834 [==============================] - 0s 9us/step - loss: 0.3239 - mae: 0.4270 - val_loss: 0.3931 - val_mae: 0.4642\n",
      "Epoch 287/300\n",
      "11834/11834 [==============================] - 0s 9us/step - loss: 0.3233 - mae: 0.4278 - val_loss: 0.3917 - val_mae: 0.4713\n",
      "Epoch 288/300\n",
      "11834/11834 [==============================] - 0s 10us/step - loss: 0.3257 - mae: 0.4294 - val_loss: 0.3929 - val_mae: 0.4624\n",
      "Epoch 289/300\n",
      "11834/11834 [==============================] - 0s 10us/step - loss: 0.3238 - mae: 0.4269 - val_loss: 0.3869 - val_mae: 0.4623\n",
      "Epoch 290/300\n",
      "11834/11834 [==============================] - 0s 16us/step - loss: 0.3223 - mae: 0.4269 - val_loss: 0.3935 - val_mae: 0.4649\n",
      "Epoch 291/300\n",
      "11834/11834 [==============================] - 0s 13us/step - loss: 0.3242 - mae: 0.4273 - val_loss: 0.3882 - val_mae: 0.4684\n",
      "Epoch 292/300\n",
      "11834/11834 [==============================] - 0s 10us/step - loss: 0.3233 - mae: 0.4277 - val_loss: 0.3878 - val_mae: 0.4630\n",
      "Epoch 293/300\n",
      "11834/11834 [==============================] - 0s 9us/step - loss: 0.3223 - mae: 0.4269 - val_loss: 0.3933 - val_mae: 0.4645\n",
      "Epoch 294/300\n",
      "11834/11834 [==============================] - 0s 9us/step - loss: 0.3235 - mae: 0.4263 - val_loss: 0.3884 - val_mae: 0.4664\n",
      "Epoch 295/300\n",
      "11834/11834 [==============================] - 0s 9us/step - loss: 0.3220 - mae: 0.4262 - val_loss: 0.3912 - val_mae: 0.4668\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 296/300\n",
      "11834/11834 [==============================] - 0s 9us/step - loss: 0.3232 - mae: 0.4268 - val_loss: 0.3926 - val_mae: 0.4647\n",
      "Epoch 297/300\n",
      "11834/11834 [==============================] - 0s 9us/step - loss: 0.3241 - mae: 0.4282 - val_loss: 0.3941 - val_mae: 0.4644\n",
      "Epoch 298/300\n",
      "11834/11834 [==============================] - 0s 9us/step - loss: 0.3221 - mae: 0.4258 - val_loss: 0.3926 - val_mae: 0.4645\n",
      "Epoch 299/300\n",
      "11834/11834 [==============================] - 0s 9us/step - loss: 0.3229 - mae: 0.4270 - val_loss: 0.3866 - val_mae: 0.4642\n",
      "Epoch 300/300\n",
      "11834/11834 [==============================] - 0s 9us/step - loss: 0.3215 - mae: 0.4258 - val_loss: 0.3925 - val_mae: 0.4670\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss=\"mse\", optimizer=\"adam\", metrics=[\"mae\"])\n",
    "history = model.fit(X_train, y_train, epochs=300, batch_size=256, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1461/1461 [==============================] - 0s 38us/step\n",
      "[0.3657560562491825, 0.4585958421230316]\n"
     ]
    }
   ],
   "source": [
    "results = model.evaluate(X_test, y_test)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['val_loss', 'val_mae', 'loss', 'mae'])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "history_dict = history.history\n",
    "print(history_dict.keys())\n",
    "\n",
    "loss_values = history_dict['loss']\n",
    "val_loss_values = history_dict['val_loss']\n",
    "\n",
    "epochs = range(1, len(loss_values) + 1)\n",
    "\n",
    "plt.plot(epochs, loss_values, 'b', color='k', label='Training loss')\n",
    "plt.plot(epochs, val_loss_values, 'b', color='b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_4 (Dense)              (None, 96)                672       \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 96)                0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 96)                9312      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 96)                0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 1)                 97        \n",
      "=================================================================\n",
      "Total params: 10,081\n",
      "Trainable params: 10,081\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras import optimizers\n",
    "from keras.layers import Dropout\n",
    "from tensorflow.keras import initializers\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(96, input_dim=6, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(96, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 11834 samples, validate on 1315 samples\n",
      "Epoch 1/300\n",
      "11834/11834 [==============================] - 0s 24us/step - loss: 1.2504 - mae: 0.8617 - val_loss: 0.5875 - val_mae: 0.5875\n",
      "Epoch 2/300\n",
      "11834/11834 [==============================] - 0s 10us/step - loss: 0.7270 - mae: 0.6617 - val_loss: 0.5652 - val_mae: 0.5711\n",
      "Epoch 3/300\n",
      "11834/11834 [==============================] - 0s 10us/step - loss: 0.6499 - mae: 0.6247 - val_loss: 0.5500 - val_mae: 0.5585\n",
      "Epoch 4/300\n",
      "11834/11834 [==============================] - 0s 10us/step - loss: 0.6173 - mae: 0.6062 - val_loss: 0.5367 - val_mae: 0.5493\n",
      "Epoch 5/300\n",
      "11834/11834 [==============================] - 0s 11us/step - loss: 0.5962 - mae: 0.5956 - val_loss: 0.5308 - val_mae: 0.5472\n",
      "Epoch 6/300\n",
      "11834/11834 [==============================] - 0s 10us/step - loss: 0.5800 - mae: 0.5863 - val_loss: 0.5431 - val_mae: 0.5568\n",
      "Epoch 7/300\n",
      "11834/11834 [==============================] - 0s 10us/step - loss: 0.5619 - mae: 0.5765 - val_loss: 0.5254 - val_mae: 0.5457\n",
      "Epoch 8/300\n",
      "11834/11834 [==============================] - 0s 10us/step - loss: 0.5404 - mae: 0.5672 - val_loss: 0.5137 - val_mae: 0.5380\n",
      "Epoch 9/300\n",
      "11834/11834 [==============================] - 0s 10us/step - loss: 0.5317 - mae: 0.5603 - val_loss: 0.5107 - val_mae: 0.5355\n",
      "Epoch 10/300\n",
      "11834/11834 [==============================] - 0s 10us/step - loss: 0.5278 - mae: 0.5566 - val_loss: 0.5207 - val_mae: 0.5426\n",
      "Epoch 11/300\n",
      "11834/11834 [==============================] - 0s 10us/step - loss: 0.5292 - mae: 0.5576 - val_loss: 0.5008 - val_mae: 0.5294\n",
      "Epoch 12/300\n",
      "11834/11834 [==============================] - 0s 11us/step - loss: 0.5216 - mae: 0.5521 - val_loss: 0.4926 - val_mae: 0.5230\n",
      "Epoch 13/300\n",
      "11834/11834 [==============================] - 0s 11us/step - loss: 0.5123 - mae: 0.5490 - val_loss: 0.4895 - val_mae: 0.5213\n",
      "Epoch 14/300\n",
      "11834/11834 [==============================] - 0s 11us/step - loss: 0.5036 - mae: 0.5401 - val_loss: 0.4926 - val_mae: 0.5248\n",
      "Epoch 15/300\n",
      "11834/11834 [==============================] - 0s 11us/step - loss: 0.4931 - mae: 0.5367 - val_loss: 0.4717 - val_mae: 0.5100\n",
      "Epoch 16/300\n",
      "11834/11834 [==============================] - 0s 10us/step - loss: 0.4973 - mae: 0.5390 - val_loss: 0.4810 - val_mae: 0.5166\n",
      "Epoch 17/300\n",
      "11834/11834 [==============================] - 0s 10us/step - loss: 0.5010 - mae: 0.5428 - val_loss: 0.4702 - val_mae: 0.5086\n",
      "Epoch 18/300\n",
      "11834/11834 [==============================] - 0s 10us/step - loss: 0.4909 - mae: 0.5342 - val_loss: 0.4626 - val_mae: 0.5020\n",
      "Epoch 19/300\n",
      "11834/11834 [==============================] - 0s 10us/step - loss: 0.4904 - mae: 0.5343 - val_loss: 0.4679 - val_mae: 0.5063\n",
      "Epoch 20/300\n",
      "11834/11834 [==============================] - 0s 10us/step - loss: 0.4904 - mae: 0.5344 - val_loss: 0.4649 - val_mae: 0.5038\n",
      "Epoch 21/300\n",
      "11834/11834 [==============================] - 0s 11us/step - loss: 0.4927 - mae: 0.5347 - val_loss: 0.4633 - val_mae: 0.5028\n",
      "Epoch 22/300\n",
      "11834/11834 [==============================] - 0s 10us/step - loss: 0.4877 - mae: 0.5328 - val_loss: 0.4713 - val_mae: 0.5102\n",
      "Epoch 23/300\n",
      "11834/11834 [==============================] - 0s 10us/step - loss: 0.4780 - mae: 0.5276 - val_loss: 0.4637 - val_mae: 0.5041\n",
      "Epoch 24/300\n",
      "11834/11834 [==============================] - 0s 10us/step - loss: 0.4810 - mae: 0.5284 - val_loss: 0.4537 - val_mae: 0.4965\n",
      "Epoch 25/300\n",
      "11834/11834 [==============================] - 0s 10us/step - loss: 0.4820 - mae: 0.5298 - val_loss: 0.4676 - val_mae: 0.5076\n",
      "Epoch 26/300\n",
      "11834/11834 [==============================] - 0s 10us/step - loss: 0.4781 - mae: 0.5277 - val_loss: 0.4725 - val_mae: 0.5109\n",
      "Epoch 27/300\n",
      "11834/11834 [==============================] - 0s 10us/step - loss: 0.4739 - mae: 0.5244 - val_loss: 0.4639 - val_mae: 0.5051\n",
      "Epoch 28/300\n",
      "11834/11834 [==============================] - 0s 10us/step - loss: 0.4767 - mae: 0.5261 - val_loss: 0.4609 - val_mae: 0.5035\n",
      "Epoch 29/300\n",
      "11834/11834 [==============================] - 0s 10us/step - loss: 0.4707 - mae: 0.5206 - val_loss: 0.4579 - val_mae: 0.5024\n",
      "Epoch 30/300\n",
      "11834/11834 [==============================] - 0s 11us/step - loss: 0.4731 - mae: 0.5240 - val_loss: 0.4636 - val_mae: 0.5068\n",
      "Epoch 31/300\n",
      "11834/11834 [==============================] - 0s 10us/step - loss: 0.4689 - mae: 0.5206 - val_loss: 0.4632 - val_mae: 0.5047\n",
      "Epoch 32/300\n",
      "11834/11834 [==============================] - 0s 11us/step - loss: 0.4704 - mae: 0.5213 - val_loss: 0.4585 - val_mae: 0.5010\n",
      "Epoch 33/300\n",
      "11834/11834 [==============================] - 0s 11us/step - loss: 0.4676 - mae: 0.5202 - val_loss: 0.4556 - val_mae: 0.5004\n",
      "Epoch 34/300\n",
      "11834/11834 [==============================] - 0s 12us/step - loss: 0.4677 - mae: 0.5195 - val_loss: 0.4706 - val_mae: 0.5094\n",
      "Epoch 35/300\n",
      "11834/11834 [==============================] - 0s 11us/step - loss: 0.4610 - mae: 0.5165 - val_loss: 0.4650 - val_mae: 0.5054\n",
      "Epoch 36/300\n",
      "11834/11834 [==============================] - 0s 11us/step - loss: 0.4634 - mae: 0.5166 - val_loss: 0.4555 - val_mae: 0.5000\n",
      "Epoch 37/300\n",
      "11834/11834 [==============================] - 0s 11us/step - loss: 0.4623 - mae: 0.5177 - val_loss: 0.4598 - val_mae: 0.5025\n",
      "Epoch 38/300\n",
      "11834/11834 [==============================] - 0s 11us/step - loss: 0.4561 - mae: 0.5160 - val_loss: 0.4539 - val_mae: 0.4992\n",
      "Epoch 39/300\n",
      "11834/11834 [==============================] - 0s 11us/step - loss: 0.4604 - mae: 0.5192 - val_loss: 0.4571 - val_mae: 0.5019\n",
      "Epoch 40/300\n",
      "11834/11834 [==============================] - 0s 11us/step - loss: 0.4585 - mae: 0.5157 - val_loss: 0.4612 - val_mae: 0.5032\n",
      "Epoch 41/300\n",
      "11834/11834 [==============================] - 0s 11us/step - loss: 0.4607 - mae: 0.5149 - val_loss: 0.4665 - val_mae: 0.5071\n",
      "Epoch 42/300\n",
      "11834/11834 [==============================] - 0s 11us/step - loss: 0.4613 - mae: 0.5161 - val_loss: 0.4565 - val_mae: 0.5014\n",
      "Epoch 43/300\n",
      "11834/11834 [==============================] - 0s 11us/step - loss: 0.4577 - mae: 0.5152 - val_loss: 0.4640 - val_mae: 0.5046\n",
      "Epoch 44/300\n",
      "11834/11834 [==============================] - 0s 11us/step - loss: 0.4567 - mae: 0.5139 - val_loss: 0.4536 - val_mae: 0.5009\n",
      "Epoch 45/300\n",
      "11834/11834 [==============================] - 0s 18us/step - loss: 0.4520 - mae: 0.5107 - val_loss: 0.4543 - val_mae: 0.4997\n",
      "Epoch 46/300\n",
      "11834/11834 [==============================] - 0s 11us/step - loss: 0.4502 - mae: 0.5109 - val_loss: 0.4518 - val_mae: 0.4981\n",
      "Epoch 47/300\n",
      "11834/11834 [==============================] - 0s 11us/step - loss: 0.4520 - mae: 0.5084 - val_loss: 0.4523 - val_mae: 0.4999\n",
      "Epoch 48/300\n",
      "11834/11834 [==============================] - 0s 11us/step - loss: 0.4517 - mae: 0.5123 - val_loss: 0.4560 - val_mae: 0.5013\n",
      "Epoch 49/300\n",
      "11834/11834 [==============================] - 0s 11us/step - loss: 0.4542 - mae: 0.5128 - val_loss: 0.4537 - val_mae: 0.4996\n",
      "Epoch 50/300\n",
      "11834/11834 [==============================] - 0s 11us/step - loss: 0.4541 - mae: 0.5128 - val_loss: 0.4541 - val_mae: 0.5006\n",
      "Epoch 51/300\n",
      "11834/11834 [==============================] - 0s 11us/step - loss: 0.4517 - mae: 0.5120 - val_loss: 0.4535 - val_mae: 0.4996\n",
      "Epoch 52/300\n",
      "11834/11834 [==============================] - 0s 12us/step - loss: 0.4528 - mae: 0.5116 - val_loss: 0.4638 - val_mae: 0.5059\n",
      "Epoch 53/300\n",
      "11834/11834 [==============================] - 0s 11us/step - loss: 0.4557 - mae: 0.5129 - val_loss: 0.4581 - val_mae: 0.5036\n",
      "Epoch 54/300\n",
      "11834/11834 [==============================] - 0s 11us/step - loss: 0.4466 - mae: 0.5088 - val_loss: 0.4602 - val_mae: 0.5057\n",
      "Epoch 55/300\n",
      "11834/11834 [==============================] - 0s 10us/step - loss: 0.4446 - mae: 0.5065 - val_loss: 0.4629 - val_mae: 0.5064\n",
      "Epoch 56/300\n",
      "11834/11834 [==============================] - 0s 10us/step - loss: 0.4486 - mae: 0.5087 - val_loss: 0.4565 - val_mae: 0.5031\n",
      "Epoch 57/300\n",
      "11834/11834 [==============================] - 0s 10us/step - loss: 0.4505 - mae: 0.5115 - val_loss: 0.4586 - val_mae: 0.5033\n",
      "Epoch 58/300\n",
      "11834/11834 [==============================] - 0s 11us/step - loss: 0.4432 - mae: 0.5056 - val_loss: 0.4548 - val_mae: 0.5013\n",
      "Epoch 59/300\n",
      "11834/11834 [==============================] - 0s 10us/step - loss: 0.4530 - mae: 0.5105 - val_loss: 0.4530 - val_mae: 0.4995\n",
      "Epoch 60/300\n",
      "11834/11834 [==============================] - 0s 11us/step - loss: 0.4534 - mae: 0.5112 - val_loss: 0.4569 - val_mae: 0.5024\n",
      "Epoch 61/300\n",
      "11834/11834 [==============================] - 0s 12us/step - loss: 0.4450 - mae: 0.5047 - val_loss: 0.4494 - val_mae: 0.4994\n",
      "Epoch 62/300\n",
      "11834/11834 [==============================] - 0s 11us/step - loss: 0.4510 - mae: 0.5096 - val_loss: 0.4472 - val_mae: 0.4966\n",
      "Epoch 63/300\n",
      "11834/11834 [==============================] - 0s 10us/step - loss: 0.4459 - mae: 0.5074 - val_loss: 0.4566 - val_mae: 0.5019\n",
      "Epoch 64/300\n",
      "11834/11834 [==============================] - 0s 10us/step - loss: 0.4469 - mae: 0.5087 - val_loss: 0.4504 - val_mae: 0.4983\n",
      "Epoch 65/300\n",
      "11834/11834 [==============================] - 0s 11us/step - loss: 0.4415 - mae: 0.5057 - val_loss: 0.4419 - val_mae: 0.4945\n",
      "Epoch 66/300\n",
      "11834/11834 [==============================] - 0s 12us/step - loss: 0.4444 - mae: 0.5069 - val_loss: 0.4588 - val_mae: 0.5043\n",
      "Epoch 67/300\n",
      "11834/11834 [==============================] - 0s 15us/step - loss: 0.4433 - mae: 0.5054 - val_loss: 0.4571 - val_mae: 0.5048\n",
      "Epoch 68/300\n",
      "11834/11834 [==============================] - 0s 14us/step - loss: 0.4465 - mae: 0.5072 - val_loss: 0.4474 - val_mae: 0.4985\n",
      "Epoch 69/300\n",
      "11834/11834 [==============================] - 0s 11us/step - loss: 0.4434 - mae: 0.5058 - val_loss: 0.4578 - val_mae: 0.5030\n",
      "Epoch 70/300\n",
      "11834/11834 [==============================] - 0s 11us/step - loss: 0.4428 - mae: 0.5054 - val_loss: 0.4519 - val_mae: 0.4998\n",
      "Epoch 71/300\n",
      "11834/11834 [==============================] - 0s 10us/step - loss: 0.4485 - mae: 0.5104 - val_loss: 0.4438 - val_mae: 0.4948\n",
      "Epoch 72/300\n",
      "11834/11834 [==============================] - 0s 11us/step - loss: 0.4442 - mae: 0.5064 - val_loss: 0.4515 - val_mae: 0.5000\n",
      "Epoch 73/300\n",
      "11834/11834 [==============================] - 0s 11us/step - loss: 0.4469 - mae: 0.5060 - val_loss: 0.4535 - val_mae: 0.5021\n",
      "Epoch 74/300\n",
      "11834/11834 [==============================] - 0s 11us/step - loss: 0.4414 - mae: 0.5026 - val_loss: 0.4436 - val_mae: 0.4944\n",
      "Epoch 75/300\n",
      "11834/11834 [==============================] - 0s 11us/step - loss: 0.4461 - mae: 0.5052 - val_loss: 0.4504 - val_mae: 0.4990\n",
      "Epoch 76/300\n",
      "11834/11834 [==============================] - 0s 11us/step - loss: 0.4381 - mae: 0.5009 - val_loss: 0.4435 - val_mae: 0.4968\n",
      "Epoch 77/300\n",
      "11834/11834 [==============================] - 0s 10us/step - loss: 0.4401 - mae: 0.5056 - val_loss: 0.4466 - val_mae: 0.4974\n",
      "Epoch 78/300\n",
      "11834/11834 [==============================] - 0s 10us/step - loss: 0.4395 - mae: 0.5036 - val_loss: 0.4491 - val_mae: 0.4982\n",
      "Epoch 79/300\n",
      "11834/11834 [==============================] - 0s 11us/step - loss: 0.4465 - mae: 0.5070 - val_loss: 0.4423 - val_mae: 0.4952\n",
      "Epoch 80/300\n",
      "11834/11834 [==============================] - 0s 12us/step - loss: 0.4450 - mae: 0.5059 - val_loss: 0.4422 - val_mae: 0.4950\n",
      "Epoch 81/300\n",
      "11834/11834 [==============================] - 0s 15us/step - loss: 0.4406 - mae: 0.5066 - val_loss: 0.4529 - val_mae: 0.5029\n",
      "Epoch 82/300\n",
      "11834/11834 [==============================] - 0s 13us/step - loss: 0.4434 - mae: 0.5063 - val_loss: 0.4454 - val_mae: 0.4967\n",
      "Epoch 83/300\n",
      "11834/11834 [==============================] - 0s 11us/step - loss: 0.4414 - mae: 0.5045 - val_loss: 0.4503 - val_mae: 0.4996\n",
      "Epoch 84/300\n",
      "11834/11834 [==============================] - 0s 11us/step - loss: 0.4394 - mae: 0.5002 - val_loss: 0.4463 - val_mae: 0.4974\n",
      "Epoch 85/300\n",
      "11834/11834 [==============================] - 0s 10us/step - loss: 0.4425 - mae: 0.5052 - val_loss: 0.4378 - val_mae: 0.4916\n",
      "Epoch 86/300\n",
      "11834/11834 [==============================] - 0s 10us/step - loss: 0.4360 - mae: 0.4994 - val_loss: 0.4388 - val_mae: 0.4934\n",
      "Epoch 87/300\n",
      "11834/11834 [==============================] - 0s 10us/step - loss: 0.4346 - mae: 0.5003 - val_loss: 0.4500 - val_mae: 0.4998\n",
      "Epoch 88/300\n",
      "11834/11834 [==============================] - 0s 10us/step - loss: 0.4388 - mae: 0.5020 - val_loss: 0.4440 - val_mae: 0.4965\n",
      "Epoch 89/300\n",
      "11834/11834 [==============================] - 0s 10us/step - loss: 0.4377 - mae: 0.5004 - val_loss: 0.4437 - val_mae: 0.4976\n",
      "Epoch 90/300\n",
      "11834/11834 [==============================] - 0s 10us/step - loss: 0.4384 - mae: 0.5043 - val_loss: 0.4433 - val_mae: 0.4974\n",
      "Epoch 91/300\n",
      "11834/11834 [==============================] - 0s 10us/step - loss: 0.4434 - mae: 0.5064 - val_loss: 0.4536 - val_mae: 0.5040\n",
      "Epoch 92/300\n",
      "11834/11834 [==============================] - 0s 11us/step - loss: 0.4376 - mae: 0.5033 - val_loss: 0.4393 - val_mae: 0.4922\n",
      "Epoch 93/300\n",
      "11834/11834 [==============================] - 0s 10us/step - loss: 0.4429 - mae: 0.5032 - val_loss: 0.4444 - val_mae: 0.4964\n",
      "Epoch 94/300\n",
      "11834/11834 [==============================] - 0s 10us/step - loss: 0.4374 - mae: 0.5036 - val_loss: 0.4402 - val_mae: 0.4948\n",
      "Epoch 95/300\n",
      "11834/11834 [==============================] - 0s 11us/step - loss: 0.4360 - mae: 0.5014 - val_loss: 0.4397 - val_mae: 0.4940\n",
      "Epoch 96/300\n",
      "11834/11834 [==============================] - 0s 11us/step - loss: 0.4433 - mae: 0.5038 - val_loss: 0.4392 - val_mae: 0.4926\n",
      "Epoch 97/300\n",
      "11834/11834 [==============================] - 0s 11us/step - loss: 0.4404 - mae: 0.5057 - val_loss: 0.4480 - val_mae: 0.4983\n",
      "Epoch 98/300\n",
      "11834/11834 [==============================] - 0s 11us/step - loss: 0.4374 - mae: 0.5031 - val_loss: 0.4420 - val_mae: 0.4937\n",
      "Epoch 99/300\n",
      "11834/11834 [==============================] - 0s 11us/step - loss: 0.4417 - mae: 0.5059 - val_loss: 0.4520 - val_mae: 0.5031\n",
      "Epoch 100/300\n",
      "11834/11834 [==============================] - 0s 12us/step - loss: 0.4369 - mae: 0.5011 - val_loss: 0.4513 - val_mae: 0.5025\n",
      "Epoch 101/300\n",
      "11834/11834 [==============================] - 0s 12us/step - loss: 0.4365 - mae: 0.5022 - val_loss: 0.4531 - val_mae: 0.5037\n",
      "Epoch 102/300\n",
      "11834/11834 [==============================] - 0s 11us/step - loss: 0.4339 - mae: 0.5014 - val_loss: 0.4426 - val_mae: 0.4959\n",
      "Epoch 103/300\n",
      "11834/11834 [==============================] - 0s 10us/step - loss: 0.4361 - mae: 0.4996 - val_loss: 0.4521 - val_mae: 0.5022\n",
      "Epoch 104/300\n",
      "11834/11834 [==============================] - 0s 12us/step - loss: 0.4358 - mae: 0.5004 - val_loss: 0.4430 - val_mae: 0.4972\n",
      "Epoch 105/300\n",
      "11834/11834 [==============================] - 0s 12us/step - loss: 0.4339 - mae: 0.5004 - val_loss: 0.4388 - val_mae: 0.4955\n",
      "Epoch 106/300\n",
      "11834/11834 [==============================] - 0s 12us/step - loss: 0.4346 - mae: 0.5006 - val_loss: 0.4462 - val_mae: 0.4992\n",
      "Epoch 107/300\n",
      "11834/11834 [==============================] - 0s 15us/step - loss: 0.4394 - mae: 0.5043 - val_loss: 0.4501 - val_mae: 0.5019\n",
      "Epoch 108/300\n",
      "11834/11834 [==============================] - 0s 11us/step - loss: 0.4331 - mae: 0.4995 - val_loss: 0.4422 - val_mae: 0.4942\n",
      "Epoch 109/300\n",
      "11834/11834 [==============================] - 0s 10us/step - loss: 0.4387 - mae: 0.5022 - val_loss: 0.4407 - val_mae: 0.4944\n",
      "Epoch 110/300\n",
      "11834/11834 [==============================] - 0s 10us/step - loss: 0.4362 - mae: 0.5017 - val_loss: 0.4441 - val_mae: 0.4960\n",
      "Epoch 111/300\n",
      "11834/11834 [==============================] - 0s 11us/step - loss: 0.4438 - mae: 0.5031 - val_loss: 0.4434 - val_mae: 0.4971\n",
      "Epoch 112/300\n",
      "11834/11834 [==============================] - 0s 10us/step - loss: 0.4352 - mae: 0.5002 - val_loss: 0.4439 - val_mae: 0.4973\n",
      "Epoch 113/300\n",
      "11834/11834 [==============================] - 0s 10us/step - loss: 0.4356 - mae: 0.5009 - val_loss: 0.4400 - val_mae: 0.4957\n",
      "Epoch 114/300\n",
      "11834/11834 [==============================] - 0s 10us/step - loss: 0.4360 - mae: 0.5019 - val_loss: 0.4443 - val_mae: 0.4972\n",
      "Epoch 115/300\n",
      "11834/11834 [==============================] - 0s 10us/step - loss: 0.4318 - mae: 0.4986 - val_loss: 0.4502 - val_mae: 0.5013\n",
      "Epoch 116/300\n",
      "11834/11834 [==============================] - 0s 11us/step - loss: 0.4370 - mae: 0.5023 - val_loss: 0.4424 - val_mae: 0.4980\n",
      "Epoch 117/300\n",
      "11834/11834 [==============================] - 0s 11us/step - loss: 0.4406 - mae: 0.5034 - val_loss: 0.4541 - val_mae: 0.5036\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 118/300\n",
      "11834/11834 [==============================] - 0s 10us/step - loss: 0.4289 - mae: 0.4950 - val_loss: 0.4456 - val_mae: 0.4981\n",
      "Epoch 119/300\n",
      "11834/11834 [==============================] - 0s 10us/step - loss: 0.4297 - mae: 0.4982 - val_loss: 0.4433 - val_mae: 0.4966\n",
      "Epoch 120/300\n",
      "11834/11834 [==============================] - 0s 10us/step - loss: 0.4361 - mae: 0.5021 - val_loss: 0.4441 - val_mae: 0.4970\n",
      "Epoch 121/300\n",
      "11834/11834 [==============================] - 0s 10us/step - loss: 0.4299 - mae: 0.4969 - val_loss: 0.4449 - val_mae: 0.4979\n",
      "Epoch 122/300\n",
      "11834/11834 [==============================] - 0s 10us/step - loss: 0.4376 - mae: 0.5022 - val_loss: 0.4526 - val_mae: 0.5040\n",
      "Epoch 123/300\n",
      "11834/11834 [==============================] - 0s 10us/step - loss: 0.4337 - mae: 0.4993 - val_loss: 0.4438 - val_mae: 0.4980\n",
      "Epoch 124/300\n",
      "11834/11834 [==============================] - 0s 10us/step - loss: 0.4340 - mae: 0.5004 - val_loss: 0.4442 - val_mae: 0.4976\n",
      "Epoch 125/300\n",
      "11834/11834 [==============================] - 0s 10us/step - loss: 0.4409 - mae: 0.5034 - val_loss: 0.4388 - val_mae: 0.4928\n",
      "Epoch 126/300\n",
      "11834/11834 [==============================] - 0s 9us/step - loss: 0.4319 - mae: 0.4985 - val_loss: 0.4429 - val_mae: 0.4967\n",
      "Epoch 127/300\n",
      "11834/11834 [==============================] - 0s 11us/step - loss: 0.4385 - mae: 0.5033 - val_loss: 0.4499 - val_mae: 0.5015\n",
      "Epoch 128/300\n",
      "11834/11834 [==============================] - 0s 10us/step - loss: 0.4326 - mae: 0.4983 - val_loss: 0.4444 - val_mae: 0.4993\n",
      "Epoch 129/300\n",
      "11834/11834 [==============================] - 0s 10us/step - loss: 0.4312 - mae: 0.4989 - val_loss: 0.4423 - val_mae: 0.4958\n",
      "Epoch 130/300\n",
      "11834/11834 [==============================] - 0s 10us/step - loss: 0.4378 - mae: 0.5023 - val_loss: 0.4444 - val_mae: 0.4970\n",
      "Epoch 131/300\n",
      "11834/11834 [==============================] - 0s 10us/step - loss: 0.4334 - mae: 0.5001 - val_loss: 0.4491 - val_mae: 0.5019\n",
      "Epoch 132/300\n",
      "11834/11834 [==============================] - 0s 10us/step - loss: 0.4265 - mae: 0.4965 - val_loss: 0.4415 - val_mae: 0.4963\n",
      "Epoch 133/300\n",
      "11834/11834 [==============================] - 0s 10us/step - loss: 0.4297 - mae: 0.4981 - val_loss: 0.4373 - val_mae: 0.4947\n",
      "Epoch 134/300\n",
      "11834/11834 [==============================] - 0s 10us/step - loss: 0.4294 - mae: 0.4962 - val_loss: 0.4510 - val_mae: 0.5029\n",
      "Epoch 135/300\n",
      "11834/11834 [==============================] - 0s 10us/step - loss: 0.4357 - mae: 0.5010 - val_loss: 0.4531 - val_mae: 0.5042\n",
      "Epoch 136/300\n",
      "11834/11834 [==============================] - 0s 10us/step - loss: 0.4286 - mae: 0.4976 - val_loss: 0.4420 - val_mae: 0.4962\n",
      "Epoch 137/300\n",
      "11834/11834 [==============================] - 0s 10us/step - loss: 0.4340 - mae: 0.4990 - val_loss: 0.4483 - val_mae: 0.5010\n",
      "Epoch 138/300\n",
      "11834/11834 [==============================] - 0s 10us/step - loss: 0.4342 - mae: 0.5014 - val_loss: 0.4437 - val_mae: 0.4976\n",
      "Epoch 139/300\n",
      "11834/11834 [==============================] - 0s 9us/step - loss: 0.4373 - mae: 0.5000 - val_loss: 0.4383 - val_mae: 0.4938\n",
      "Epoch 140/300\n",
      "11834/11834 [==============================] - 0s 10us/step - loss: 0.4300 - mae: 0.4987 - val_loss: 0.4385 - val_mae: 0.4941\n",
      "Epoch 141/300\n",
      "11834/11834 [==============================] - 0s 10us/step - loss: 0.4347 - mae: 0.5012 - val_loss: 0.4409 - val_mae: 0.4948\n",
      "Epoch 142/300\n",
      "11834/11834 [==============================] - 0s 10us/step - loss: 0.4285 - mae: 0.4974 - val_loss: 0.4404 - val_mae: 0.4960\n",
      "Epoch 143/300\n",
      "11834/11834 [==============================] - 0s 9us/step - loss: 0.4284 - mae: 0.4969 - val_loss: 0.4425 - val_mae: 0.4950\n",
      "Epoch 144/300\n",
      "11834/11834 [==============================] - 0s 10us/step - loss: 0.4338 - mae: 0.4988 - val_loss: 0.4480 - val_mae: 0.5021\n",
      "Epoch 145/300\n",
      "11834/11834 [==============================] - 0s 10us/step - loss: 0.4350 - mae: 0.5027 - val_loss: 0.4379 - val_mae: 0.4955\n",
      "Epoch 146/300\n",
      "11834/11834 [==============================] - 0s 10us/step - loss: 0.4324 - mae: 0.4985 - val_loss: 0.4417 - val_mae: 0.4966\n",
      "Epoch 147/300\n",
      "11834/11834 [==============================] - 0s 10us/step - loss: 0.4330 - mae: 0.5001 - val_loss: 0.4409 - val_mae: 0.4951\n",
      "Epoch 148/300\n",
      "11834/11834 [==============================] - 0s 10us/step - loss: 0.4304 - mae: 0.4978 - val_loss: 0.4294 - val_mae: 0.4898\n",
      "Epoch 149/300\n",
      "11834/11834 [==============================] - 0s 10us/step - loss: 0.4280 - mae: 0.4964 - val_loss: 0.4371 - val_mae: 0.4953\n",
      "Epoch 150/300\n",
      "11834/11834 [==============================] - 0s 9us/step - loss: 0.4293 - mae: 0.4991 - val_loss: 0.4392 - val_mae: 0.4959\n",
      "Epoch 151/300\n",
      "11834/11834 [==============================] - 0s 10us/step - loss: 0.4267 - mae: 0.4954 - val_loss: 0.4368 - val_mae: 0.4957\n",
      "Epoch 152/300\n",
      "11834/11834 [==============================] - 0s 10us/step - loss: 0.4313 - mae: 0.4987 - val_loss: 0.4478 - val_mae: 0.5017\n",
      "Epoch 153/300\n",
      "11834/11834 [==============================] - 0s 10us/step - loss: 0.4310 - mae: 0.4979 - val_loss: 0.4374 - val_mae: 0.4952\n",
      "Epoch 154/300\n",
      "11834/11834 [==============================] - 0s 10us/step - loss: 0.4246 - mae: 0.4950 - val_loss: 0.4452 - val_mae: 0.4990\n",
      "Epoch 155/300\n",
      "11834/11834 [==============================] - 0s 10us/step - loss: 0.4280 - mae: 0.4954 - val_loss: 0.4452 - val_mae: 0.5016\n",
      "Epoch 156/300\n",
      "11834/11834 [==============================] - 0s 10us/step - loss: 0.4240 - mae: 0.4949 - val_loss: 0.4337 - val_mae: 0.4930\n",
      "Epoch 157/300\n",
      "11834/11834 [==============================] - 0s 10us/step - loss: 0.4294 - mae: 0.4963 - val_loss: 0.4502 - val_mae: 0.5039\n",
      "Epoch 158/300\n",
      "11834/11834 [==============================] - 0s 11us/step - loss: 0.4315 - mae: 0.4964 - val_loss: 0.4442 - val_mae: 0.4995\n",
      "Epoch 159/300\n",
      "11834/11834 [==============================] - 0s 12us/step - loss: 0.4271 - mae: 0.4958 - val_loss: 0.4415 - val_mae: 0.4980\n",
      "Epoch 160/300\n",
      "11834/11834 [==============================] - 0s 11us/step - loss: 0.4341 - mae: 0.5014 - val_loss: 0.4464 - val_mae: 0.5002\n",
      "Epoch 161/300\n",
      "11834/11834 [==============================] - 0s 12us/step - loss: 0.4290 - mae: 0.4980 - val_loss: 0.4366 - val_mae: 0.4938\n",
      "Epoch 162/300\n",
      "11834/11834 [==============================] - 0s 12us/step - loss: 0.4312 - mae: 0.4990 - val_loss: 0.4398 - val_mae: 0.4967\n",
      "Epoch 163/300\n",
      "11834/11834 [==============================] - 0s 10us/step - loss: 0.4294 - mae: 0.4964 - val_loss: 0.4309 - val_mae: 0.4893\n",
      "Epoch 164/300\n",
      "11834/11834 [==============================] - 0s 10us/step - loss: 0.4311 - mae: 0.4968 - val_loss: 0.4339 - val_mae: 0.4918\n",
      "Epoch 165/300\n",
      "11834/11834 [==============================] - 0s 10us/step - loss: 0.4286 - mae: 0.4957 - val_loss: 0.4429 - val_mae: 0.4970\n",
      "Epoch 166/300\n",
      "11834/11834 [==============================] - 0s 10us/step - loss: 0.4254 - mae: 0.4948 - val_loss: 0.4350 - val_mae: 0.4931\n",
      "Epoch 167/300\n",
      "11834/11834 [==============================] - 0s 11us/step - loss: 0.4332 - mae: 0.4994 - val_loss: 0.4398 - val_mae: 0.4968\n",
      "Epoch 168/300\n",
      "11834/11834 [==============================] - 0s 10us/step - loss: 0.4265 - mae: 0.4970 - val_loss: 0.4358 - val_mae: 0.4931\n",
      "Epoch 169/300\n",
      "11834/11834 [==============================] - 0s 10us/step - loss: 0.4270 - mae: 0.4971 - val_loss: 0.4378 - val_mae: 0.4947\n",
      "Epoch 170/300\n",
      "11834/11834 [==============================] - 0s 10us/step - loss: 0.4237 - mae: 0.4947 - val_loss: 0.4334 - val_mae: 0.4925\n",
      "Epoch 171/300\n",
      "11834/11834 [==============================] - 0s 10us/step - loss: 0.4281 - mae: 0.4974 - val_loss: 0.4369 - val_mae: 0.4960\n",
      "Epoch 172/300\n",
      "11834/11834 [==============================] - 0s 10us/step - loss: 0.4304 - mae: 0.4979 - val_loss: 0.4417 - val_mae: 0.4977\n",
      "Epoch 173/300\n",
      "11834/11834 [==============================] - 0s 10us/step - loss: 0.4241 - mae: 0.4941 - val_loss: 0.4271 - val_mae: 0.4888\n",
      "Epoch 174/300\n",
      "11834/11834 [==============================] - 0s 10us/step - loss: 0.4310 - mae: 0.4974 - val_loss: 0.4409 - val_mae: 0.4976\n",
      "Epoch 175/300\n",
      "11834/11834 [==============================] - 0s 13us/step - loss: 0.4276 - mae: 0.4980 - val_loss: 0.4360 - val_mae: 0.4953\n",
      "Epoch 176/300\n",
      "11834/11834 [==============================] - 0s 12us/step - loss: 0.4317 - mae: 0.4995 - val_loss: 0.4323 - val_mae: 0.4913\n",
      "Epoch 177/300\n",
      "11834/11834 [==============================] - 0s 12us/step - loss: 0.4278 - mae: 0.4987 - val_loss: 0.4433 - val_mae: 0.5002\n",
      "Epoch 178/300\n",
      "11834/11834 [==============================] - 0s 11us/step - loss: 0.4236 - mae: 0.4918 - val_loss: 0.4402 - val_mae: 0.4981\n",
      "Epoch 179/300\n",
      "11834/11834 [==============================] - 0s 10us/step - loss: 0.4289 - mae: 0.4984 - val_loss: 0.4409 - val_mae: 0.4970\n",
      "Epoch 180/300\n",
      "11834/11834 [==============================] - 0s 10us/step - loss: 0.4250 - mae: 0.4946 - val_loss: 0.4330 - val_mae: 0.4924\n",
      "Epoch 181/300\n",
      "11834/11834 [==============================] - 0s 12us/step - loss: 0.4248 - mae: 0.4934 - val_loss: 0.4472 - val_mae: 0.5029\n",
      "Epoch 182/300\n",
      "11834/11834 [==============================] - 0s 11us/step - loss: 0.4260 - mae: 0.4963 - val_loss: 0.4440 - val_mae: 0.5003\n",
      "Epoch 183/300\n",
      "11834/11834 [==============================] - 0s 10us/step - loss: 0.4301 - mae: 0.4976 - val_loss: 0.4356 - val_mae: 0.4942\n",
      "Epoch 184/300\n",
      "11834/11834 [==============================] - 0s 10us/step - loss: 0.4286 - mae: 0.4958 - val_loss: 0.4433 - val_mae: 0.4999\n",
      "Epoch 185/300\n",
      "11834/11834 [==============================] - 0s 10us/step - loss: 0.4293 - mae: 0.4950 - val_loss: 0.4309 - val_mae: 0.4900\n",
      "Epoch 186/300\n",
      "11834/11834 [==============================] - 0s 10us/step - loss: 0.4263 - mae: 0.4956 - val_loss: 0.4444 - val_mae: 0.5006\n",
      "Epoch 187/300\n",
      "11834/11834 [==============================] - 0s 11us/step - loss: 0.4275 - mae: 0.4955 - val_loss: 0.4371 - val_mae: 0.4956\n",
      "Epoch 188/300\n",
      "11834/11834 [==============================] - 0s 10us/step - loss: 0.4258 - mae: 0.4948 - val_loss: 0.4418 - val_mae: 0.4994\n",
      "Epoch 189/300\n",
      "11834/11834 [==============================] - 0s 10us/step - loss: 0.4212 - mae: 0.4925 - val_loss: 0.4326 - val_mae: 0.4926\n",
      "Epoch 190/300\n",
      "11834/11834 [==============================] - 0s 10us/step - loss: 0.4347 - mae: 0.5000 - val_loss: 0.4448 - val_mae: 0.5022\n",
      "Epoch 191/300\n",
      "11834/11834 [==============================] - 0s 10us/step - loss: 0.4257 - mae: 0.4948 - val_loss: 0.4349 - val_mae: 0.4940\n",
      "Epoch 192/300\n",
      "11834/11834 [==============================] - 0s 13us/step - loss: 0.4255 - mae: 0.4933 - val_loss: 0.4347 - val_mae: 0.4951\n",
      "Epoch 193/300\n",
      "11834/11834 [==============================] - 0s 11us/step - loss: 0.4272 - mae: 0.4959 - val_loss: 0.4344 - val_mae: 0.4925\n",
      "Epoch 194/300\n",
      "11834/11834 [==============================] - 0s 10us/step - loss: 0.4317 - mae: 0.4972 - val_loss: 0.4469 - val_mae: 0.5018\n",
      "Epoch 195/300\n",
      "11834/11834 [==============================] - 0s 10us/step - loss: 0.4281 - mae: 0.4966 - val_loss: 0.4322 - val_mae: 0.4899\n",
      "Epoch 196/300\n",
      "11834/11834 [==============================] - 0s 10us/step - loss: 0.4262 - mae: 0.4946 - val_loss: 0.4344 - val_mae: 0.4939\n",
      "Epoch 197/300\n",
      "11834/11834 [==============================] - 0s 10us/step - loss: 0.4259 - mae: 0.4962 - val_loss: 0.4345 - val_mae: 0.4935\n",
      "Epoch 198/300\n",
      "11834/11834 [==============================] - 0s 12us/step - loss: 0.4244 - mae: 0.4944 - val_loss: 0.4341 - val_mae: 0.4934\n",
      "Epoch 199/300\n",
      "11834/11834 [==============================] - 0s 12us/step - loss: 0.4251 - mae: 0.4937 - val_loss: 0.4349 - val_mae: 0.4936\n",
      "Epoch 200/300\n",
      "11834/11834 [==============================] - 0s 16us/step - loss: 0.4305 - mae: 0.4988 - val_loss: 0.4359 - val_mae: 0.4938\n",
      "Epoch 201/300\n",
      "11834/11834 [==============================] - 0s 11us/step - loss: 0.4214 - mae: 0.4937 - val_loss: 0.4332 - val_mae: 0.4908\n",
      "Epoch 202/300\n",
      "11834/11834 [==============================] - 0s 10us/step - loss: 0.4259 - mae: 0.4950 - val_loss: 0.4431 - val_mae: 0.5016\n",
      "Epoch 203/300\n",
      "11834/11834 [==============================] - 0s 11us/step - loss: 0.4163 - mae: 0.4891 - val_loss: 0.4409 - val_mae: 0.4998\n",
      "Epoch 204/300\n",
      "11834/11834 [==============================] - 0s 12us/step - loss: 0.4245 - mae: 0.4941 - val_loss: 0.4437 - val_mae: 0.5003\n",
      "Epoch 205/300\n",
      "11834/11834 [==============================] - 0s 13us/step - loss: 0.4284 - mae: 0.4969 - val_loss: 0.4380 - val_mae: 0.4963\n",
      "Epoch 206/300\n",
      "11834/11834 [==============================] - 0s 10us/step - loss: 0.4276 - mae: 0.4964 - val_loss: 0.4355 - val_mae: 0.4938\n",
      "Epoch 207/300\n",
      "11834/11834 [==============================] - 0s 12us/step - loss: 0.4194 - mae: 0.4928 - val_loss: 0.4358 - val_mae: 0.4946\n",
      "Epoch 208/300\n",
      "11834/11834 [==============================] - 0s 11us/step - loss: 0.4295 - mae: 0.4948 - val_loss: 0.4318 - val_mae: 0.4915\n",
      "Epoch 209/300\n",
      "11834/11834 [==============================] - 0s 11us/step - loss: 0.4224 - mae: 0.4935 - val_loss: 0.4315 - val_mae: 0.4908\n",
      "Epoch 210/300\n",
      "11834/11834 [==============================] - 0s 11us/step - loss: 0.4199 - mae: 0.4907 - val_loss: 0.4397 - val_mae: 0.4978\n",
      "Epoch 211/300\n",
      "11834/11834 [==============================] - 0s 11us/step - loss: 0.4268 - mae: 0.4960 - val_loss: 0.4283 - val_mae: 0.4899\n",
      "Epoch 212/300\n",
      "11834/11834 [==============================] - 0s 11us/step - loss: 0.4224 - mae: 0.4915 - val_loss: 0.4291 - val_mae: 0.4908\n",
      "Epoch 213/300\n",
      "11834/11834 [==============================] - 0s 11us/step - loss: 0.4318 - mae: 0.4997 - val_loss: 0.4435 - val_mae: 0.5015\n",
      "Epoch 214/300\n",
      "11834/11834 [==============================] - 0s 11us/step - loss: 0.4245 - mae: 0.4936 - val_loss: 0.4297 - val_mae: 0.4907\n",
      "Epoch 215/300\n",
      "11834/11834 [==============================] - 0s 13us/step - loss: 0.4244 - mae: 0.4915 - val_loss: 0.4313 - val_mae: 0.4912\n",
      "Epoch 216/300\n",
      "11834/11834 [==============================] - 0s 11us/step - loss: 0.4216 - mae: 0.4939 - val_loss: 0.4527 - val_mae: 0.5079\n",
      "Epoch 217/300\n",
      "11834/11834 [==============================] - 0s 10us/step - loss: 0.4249 - mae: 0.4956 - val_loss: 0.4440 - val_mae: 0.5004\n",
      "Epoch 218/300\n",
      "11834/11834 [==============================] - 0s 10us/step - loss: 0.4233 - mae: 0.4943 - val_loss: 0.4402 - val_mae: 0.4955\n",
      "Epoch 219/300\n",
      "11834/11834 [==============================] - 0s 10us/step - loss: 0.4309 - mae: 0.4966 - val_loss: 0.4387 - val_mae: 0.4963\n",
      "Epoch 220/300\n",
      "11834/11834 [==============================] - 0s 11us/step - loss: 0.4225 - mae: 0.4939 - val_loss: 0.4342 - val_mae: 0.4932\n",
      "Epoch 221/300\n",
      "11834/11834 [==============================] - 0s 11us/step - loss: 0.4241 - mae: 0.4953 - val_loss: 0.4290 - val_mae: 0.4891\n",
      "Epoch 222/300\n",
      "11834/11834 [==============================] - 0s 10us/step - loss: 0.4248 - mae: 0.4947 - val_loss: 0.4320 - val_mae: 0.4926\n",
      "Epoch 223/300\n",
      "11834/11834 [==============================] - 0s 13us/step - loss: 0.4231 - mae: 0.4950 - val_loss: 0.4324 - val_mae: 0.4913\n",
      "Epoch 224/300\n",
      "11834/11834 [==============================] - 0s 11us/step - loss: 0.4232 - mae: 0.4926 - val_loss: 0.4335 - val_mae: 0.4948\n",
      "Epoch 225/300\n",
      "11834/11834 [==============================] - 0s 11us/step - loss: 0.4259 - mae: 0.4950 - val_loss: 0.4291 - val_mae: 0.4907\n",
      "Epoch 226/300\n",
      "11834/11834 [==============================] - 0s 11us/step - loss: 0.4259 - mae: 0.4955 - val_loss: 0.4276 - val_mae: 0.4904\n",
      "Epoch 227/300\n",
      "11834/11834 [==============================] - 0s 10us/step - loss: 0.4272 - mae: 0.4960 - val_loss: 0.4313 - val_mae: 0.4922\n",
      "Epoch 228/300\n",
      "11834/11834 [==============================] - 0s 10us/step - loss: 0.4260 - mae: 0.4948 - val_loss: 0.4368 - val_mae: 0.4945\n",
      "Epoch 229/300\n",
      "11834/11834 [==============================] - 0s 10us/step - loss: 0.4237 - mae: 0.4948 - val_loss: 0.4288 - val_mae: 0.4907\n",
      "Epoch 230/300\n",
      "11834/11834 [==============================] - 0s 10us/step - loss: 0.4256 - mae: 0.4949 - val_loss: 0.4382 - val_mae: 0.4971\n",
      "Epoch 231/300\n",
      "11834/11834 [==============================] - 0s 12us/step - loss: 0.4231 - mae: 0.4934 - val_loss: 0.4395 - val_mae: 0.4986\n",
      "Epoch 232/300\n",
      "11834/11834 [==============================] - 0s 10us/step - loss: 0.4244 - mae: 0.4937 - val_loss: 0.4281 - val_mae: 0.4896\n",
      "Epoch 233/300\n",
      "11834/11834 [==============================] - 0s 11us/step - loss: 0.4246 - mae: 0.4951 - val_loss: 0.4319 - val_mae: 0.4933\n",
      "Epoch 234/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11834/11834 [==============================] - 0s 11us/step - loss: 0.4235 - mae: 0.4939 - val_loss: 0.4364 - val_mae: 0.4953\n",
      "Epoch 235/300\n",
      "11834/11834 [==============================] - 0s 11us/step - loss: 0.4271 - mae: 0.4957 - val_loss: 0.4356 - val_mae: 0.4947\n",
      "Epoch 236/300\n",
      "11834/11834 [==============================] - 0s 12us/step - loss: 0.4237 - mae: 0.4936 - val_loss: 0.4354 - val_mae: 0.4946\n",
      "Epoch 237/300\n",
      "11834/11834 [==============================] - 0s 10us/step - loss: 0.4219 - mae: 0.4900 - val_loss: 0.4305 - val_mae: 0.4912\n",
      "Epoch 238/300\n",
      "11834/11834 [==============================] - 0s 11us/step - loss: 0.4263 - mae: 0.4943 - val_loss: 0.4313 - val_mae: 0.4907\n",
      "Epoch 239/300\n",
      "11834/11834 [==============================] - 0s 11us/step - loss: 0.4254 - mae: 0.4942 - val_loss: 0.4310 - val_mae: 0.4915\n",
      "Epoch 240/300\n",
      "11834/11834 [==============================] - 0s 10us/step - loss: 0.4257 - mae: 0.4960 - val_loss: 0.4314 - val_mae: 0.4925\n",
      "Epoch 241/300\n",
      "11834/11834 [==============================] - 0s 10us/step - loss: 0.4276 - mae: 0.4965 - val_loss: 0.4381 - val_mae: 0.4980\n",
      "Epoch 242/300\n",
      "11834/11834 [==============================] - 0s 10us/step - loss: 0.4254 - mae: 0.4952 - val_loss: 0.4490 - val_mae: 0.5055\n",
      "Epoch 243/300\n",
      "11834/11834 [==============================] - 0s 10us/step - loss: 0.4210 - mae: 0.4920 - val_loss: 0.4261 - val_mae: 0.4884\n",
      "Epoch 244/300\n",
      "11834/11834 [==============================] - 0s 11us/step - loss: 0.4251 - mae: 0.4957 - val_loss: 0.4260 - val_mae: 0.4876\n",
      "Epoch 245/300\n",
      "11834/11834 [==============================] - 0s 11us/step - loss: 0.4255 - mae: 0.4931 - val_loss: 0.4337 - val_mae: 0.4933\n",
      "Epoch 246/300\n",
      "11834/11834 [==============================] - 0s 12us/step - loss: 0.4232 - mae: 0.4939 - val_loss: 0.4320 - val_mae: 0.4912\n",
      "Epoch 247/300\n",
      "11834/11834 [==============================] - 0s 11us/step - loss: 0.4193 - mae: 0.4908 - val_loss: 0.4336 - val_mae: 0.4929\n",
      "Epoch 248/300\n",
      "11834/11834 [==============================] - 0s 10us/step - loss: 0.4174 - mae: 0.4913 - val_loss: 0.4322 - val_mae: 0.4939\n",
      "Epoch 249/300\n",
      "11834/11834 [==============================] - 0s 11us/step - loss: 0.4224 - mae: 0.4923 - val_loss: 0.4247 - val_mae: 0.4870\n",
      "Epoch 250/300\n",
      "11834/11834 [==============================] - 0s 10us/step - loss: 0.4219 - mae: 0.4926 - val_loss: 0.4338 - val_mae: 0.4942\n",
      "Epoch 251/300\n",
      "11834/11834 [==============================] - 0s 11us/step - loss: 0.4238 - mae: 0.4930 - val_loss: 0.4301 - val_mae: 0.4912\n",
      "Epoch 252/300\n",
      "11834/11834 [==============================] - 0s 10us/step - loss: 0.4268 - mae: 0.4944 - val_loss: 0.4352 - val_mae: 0.4944\n",
      "Epoch 253/300\n",
      "11834/11834 [==============================] - 0s 10us/step - loss: 0.4261 - mae: 0.4920 - val_loss: 0.4347 - val_mae: 0.4938\n",
      "Epoch 254/300\n",
      "11834/11834 [==============================] - 0s 10us/step - loss: 0.4300 - mae: 0.4955 - val_loss: 0.4325 - val_mae: 0.4913\n",
      "Epoch 255/300\n",
      "11834/11834 [==============================] - 0s 12us/step - loss: 0.4250 - mae: 0.4952 - val_loss: 0.4380 - val_mae: 0.4970\n",
      "Epoch 256/300\n",
      "11834/11834 [==============================] - 0s 17us/step - loss: 0.4266 - mae: 0.4959 - val_loss: 0.4302 - val_mae: 0.4928\n",
      "Epoch 257/300\n",
      "11834/11834 [==============================] - 0s 11us/step - loss: 0.4228 - mae: 0.4919 - val_loss: 0.4366 - val_mae: 0.4967\n",
      "Epoch 258/300\n",
      "11834/11834 [==============================] - 0s 10us/step - loss: 0.4268 - mae: 0.4983 - val_loss: 0.4278 - val_mae: 0.4904\n",
      "Epoch 259/300\n",
      "11834/11834 [==============================] - 0s 10us/step - loss: 0.4250 - mae: 0.4943 - val_loss: 0.4400 - val_mae: 0.4986\n",
      "Epoch 260/300\n",
      "11834/11834 [==============================] - 0s 10us/step - loss: 0.4246 - mae: 0.4943 - val_loss: 0.4244 - val_mae: 0.4879\n",
      "Epoch 261/300\n",
      "11834/11834 [==============================] - 0s 10us/step - loss: 0.4193 - mae: 0.4930 - val_loss: 0.4316 - val_mae: 0.4921\n",
      "Epoch 262/300\n",
      "11834/11834 [==============================] - 0s 15us/step - loss: 0.4232 - mae: 0.4917 - val_loss: 0.4369 - val_mae: 0.4960\n",
      "Epoch 263/300\n",
      "11834/11834 [==============================] - 0s 11us/step - loss: 0.4195 - mae: 0.4923 - val_loss: 0.4372 - val_mae: 0.4977\n",
      "Epoch 264/300\n",
      "11834/11834 [==============================] - 0s 12us/step - loss: 0.4292 - mae: 0.4988 - val_loss: 0.4291 - val_mae: 0.4909\n",
      "Epoch 265/300\n",
      "11834/11834 [==============================] - 0s 10us/step - loss: 0.4193 - mae: 0.4903 - val_loss: 0.4324 - val_mae: 0.4915\n",
      "Epoch 266/300\n",
      "11834/11834 [==============================] - 0s 11us/step - loss: 0.4302 - mae: 0.4970 - val_loss: 0.4308 - val_mae: 0.4917\n",
      "Epoch 267/300\n",
      "11834/11834 [==============================] - 0s 11us/step - loss: 0.4166 - mae: 0.4900 - val_loss: 0.4313 - val_mae: 0.4924\n",
      "Epoch 268/300\n",
      "11834/11834 [==============================] - 0s 11us/step - loss: 0.4206 - mae: 0.4912 - val_loss: 0.4412 - val_mae: 0.4994\n",
      "Epoch 269/300\n",
      "11834/11834 [==============================] - 0s 11us/step - loss: 0.4246 - mae: 0.4932 - val_loss: 0.4380 - val_mae: 0.4964\n",
      "Epoch 270/300\n",
      "11834/11834 [==============================] - 0s 13us/step - loss: 0.4212 - mae: 0.4937 - val_loss: 0.4342 - val_mae: 0.4933\n",
      "Epoch 271/300\n",
      "11834/11834 [==============================] - 0s 12us/step - loss: 0.4239 - mae: 0.4940 - val_loss: 0.4344 - val_mae: 0.4933\n",
      "Epoch 272/300\n",
      "11834/11834 [==============================] - 0s 13us/step - loss: 0.4235 - mae: 0.4947 - val_loss: 0.4272 - val_mae: 0.4896\n",
      "Epoch 273/300\n",
      "11834/11834 [==============================] - 0s 11us/step - loss: 0.4206 - mae: 0.4926 - val_loss: 0.4268 - val_mae: 0.4896\n",
      "Epoch 274/300\n",
      "11834/11834 [==============================] - 0s 11us/step - loss: 0.4250 - mae: 0.4944 - val_loss: 0.4337 - val_mae: 0.4947\n",
      "Epoch 275/300\n",
      "11834/11834 [==============================] - 0s 11us/step - loss: 0.4214 - mae: 0.4923 - val_loss: 0.4334 - val_mae: 0.4943\n",
      "Epoch 276/300\n",
      "11834/11834 [==============================] - 0s 11us/step - loss: 0.4260 - mae: 0.4945 - val_loss: 0.4359 - val_mae: 0.4962\n",
      "Epoch 277/300\n",
      "11834/11834 [==============================] - 0s 11us/step - loss: 0.4232 - mae: 0.4940 - val_loss: 0.4406 - val_mae: 0.4987\n",
      "Epoch 278/300\n",
      "11834/11834 [==============================] - 0s 12us/step - loss: 0.4239 - mae: 0.4950 - val_loss: 0.4320 - val_mae: 0.4924\n",
      "Epoch 279/300\n",
      "11834/11834 [==============================] - 0s 11us/step - loss: 0.4256 - mae: 0.4945 - val_loss: 0.4323 - val_mae: 0.4946\n",
      "Epoch 280/300\n",
      "11834/11834 [==============================] - 0s 12us/step - loss: 0.4205 - mae: 0.4921 - val_loss: 0.4275 - val_mae: 0.4908\n",
      "Epoch 281/300\n",
      "11834/11834 [==============================] - 0s 13us/step - loss: 0.4255 - mae: 0.4960 - val_loss: 0.4306 - val_mae: 0.4921\n",
      "Epoch 282/300\n",
      "11834/11834 [==============================] - 0s 12us/step - loss: 0.4211 - mae: 0.4925 - val_loss: 0.4282 - val_mae: 0.4889\n",
      "Epoch 283/300\n",
      "11834/11834 [==============================] - 0s 12us/step - loss: 0.4270 - mae: 0.4953 - val_loss: 0.4310 - val_mae: 0.4925\n",
      "Epoch 284/300\n",
      "11834/11834 [==============================] - 0s 12us/step - loss: 0.4189 - mae: 0.4901 - val_loss: 0.4360 - val_mae: 0.4956\n",
      "Epoch 285/300\n",
      "11834/11834 [==============================] - 0s 12us/step - loss: 0.4247 - mae: 0.4933 - val_loss: 0.4375 - val_mae: 0.4959\n",
      "Epoch 286/300\n",
      "11834/11834 [==============================] - 0s 12us/step - loss: 0.4194 - mae: 0.4910 - val_loss: 0.4266 - val_mae: 0.4895\n",
      "Epoch 287/300\n",
      "11834/11834 [==============================] - 0s 11us/step - loss: 0.4203 - mae: 0.4903 - val_loss: 0.4327 - val_mae: 0.4922\n",
      "Epoch 288/300\n",
      "11834/11834 [==============================] - 0s 11us/step - loss: 0.4216 - mae: 0.4924 - val_loss: 0.4296 - val_mae: 0.4914\n",
      "Epoch 289/300\n",
      "11834/11834 [==============================] - 0s 11us/step - loss: 0.4258 - mae: 0.4937 - val_loss: 0.4338 - val_mae: 0.4924\n",
      "Epoch 290/300\n",
      "11834/11834 [==============================] - 0s 11us/step - loss: 0.4171 - mae: 0.4898 - val_loss: 0.4330 - val_mae: 0.4928\n",
      "Epoch 291/300\n",
      "11834/11834 [==============================] - 0s 13us/step - loss: 0.4216 - mae: 0.4920 - val_loss: 0.4341 - val_mae: 0.4947\n",
      "Epoch 292/300\n",
      "11834/11834 [==============================] - 0s 12us/step - loss: 0.4195 - mae: 0.4934 - val_loss: 0.4272 - val_mae: 0.4883\n",
      "Epoch 293/300\n",
      "11834/11834 [==============================] - 0s 12us/step - loss: 0.4213 - mae: 0.4935 - val_loss: 0.4309 - val_mae: 0.4903\n",
      "Epoch 294/300\n",
      "11834/11834 [==============================] - 0s 12us/step - loss: 0.4244 - mae: 0.4960 - val_loss: 0.4317 - val_mae: 0.4925\n",
      "Epoch 295/300\n",
      "11834/11834 [==============================] - 0s 12us/step - loss: 0.4268 - mae: 0.4966 - val_loss: 0.4232 - val_mae: 0.4854\n",
      "Epoch 296/300\n",
      "11834/11834 [==============================] - 0s 13us/step - loss: 0.4188 - mae: 0.4899 - val_loss: 0.4334 - val_mae: 0.4948\n",
      "Epoch 297/300\n",
      "11834/11834 [==============================] - 0s 18us/step - loss: 0.4242 - mae: 0.4947 - val_loss: 0.4347 - val_mae: 0.4953\n",
      "Epoch 298/300\n",
      "11834/11834 [==============================] - 0s 13us/step - loss: 0.4258 - mae: 0.4959 - val_loss: 0.4391 - val_mae: 0.4979\n",
      "Epoch 299/300\n",
      "11834/11834 [==============================] - 0s 12us/step - loss: 0.4202 - mae: 0.4918 - val_loss: 0.4329 - val_mae: 0.4945\n",
      "Epoch 300/300\n",
      "11834/11834 [==============================] - 0s 11us/step - loss: 0.4224 - mae: 0.4947 - val_loss: 0.4357 - val_mae: 0.4963\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss=\"mse\", optimizer=\"adam\", metrics=[\"mae\"])\n",
    "history = model.fit(X_train, y_train, epochs=300, batch_size=256, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1461/1461 [==============================] - 0s 40us/step\n",
      "[0.3881460028425787, 0.4773012399673462]\n"
     ]
    }
   ],
   "source": [
    "results = model.evaluate(X_test, y_test)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['val_loss', 'val_mae', 'loss', 'mae'])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "history_dict = history.history\n",
    "print(history_dict.keys())\n",
    "\n",
    "loss_values = history_dict['loss']\n",
    "val_loss_values = history_dict['val_loss']\n",
    "\n",
    "epochs = range(1, len(loss_values) + 1)\n",
    "\n",
    "plt.plot(epochs, loss_values, 'b', color='k', label='Training loss')\n",
    "plt.plot(epochs, val_loss_values, 'b', color='b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SMILES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>smiles</th>\n",
       "      <th>logP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C[C@H]([C@@H](C)Cl)Cl</td>\n",
       "      <td>2.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C(C=CBr)N</td>\n",
       "      <td>0.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CCC(CO)Br</td>\n",
       "      <td>1.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[13CH3][13CH2][13CH2][13CH2][13CH2][13CH2]O</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CCCOCCP</td>\n",
       "      <td>0.6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        smiles  logP\n",
       "0                        C[C@H]([C@@H](C)Cl)Cl   2.3\n",
       "1                                    C(C=CBr)N   0.3\n",
       "2                                    CCC(CO)Br   1.3\n",
       "3  [13CH3][13CH2][13CH2][13CH2][13CH2][13CH2]O   2.0\n",
       "4                                      CCCOCCP   0.6"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df= pd.read_csv('logP_dataset.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.3 0.3 1.3 ... 0.4 3.4 1. ]\n"
     ]
    }
   ],
   "source": [
    "y = df['logP'].values\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "138\n"
     ]
    }
   ],
   "source": [
    "a = []\n",
    "for i in df['smiles']:\n",
    "    a.append(len(i))\n",
    "max_length = max(a)\n",
    "print(max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word_index = {'C': 1, '(': 2, ')': 3, '[': 4, ']': 5, 'O': 6, 'H': 7, 'N': 8, '2': 9, 'S': 10, '1': 11, '=': 12, 'l': 13, 'P': 14, 'F': 15, 'I': 16, 'B': 17, 'r': 18, '@': 19, '+': 20, '-': 21, '3': 22, '/': 23, '\\\\': 24, '4': 25, '5': 26, '#': 27, '8': 28, '7': 29, '9': 30, '0': 31, '.': 32, '6': 33}\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing import text, sequence\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "tokenizer = Tokenizer(lower=False, char_level=True)\n",
    "tokenizer.fit_on_texts(df.values[:,0])\n",
    "\n",
    "X = tokenizer.texts_to_sequences(df.values[:,0])\n",
    "X = sequence.pad_sequences(X, dtype='int32', truncating='pre', value=0., maxlen=max_length)\n",
    "\n",
    "print(\"word_index =\", tokenizer.word_index)\n",
    "with open('word_index.txt', 'w') as output:\n",
    "    output.write(str(tokenizer.word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 138, 138)          4692      \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 32)                21888     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 26,613\n",
      "Trainable params: 26,613\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Input, Embedding, LSTM, Dense, concatenate\n",
    "from keras.models import Model\n",
    "from keras import regularizers\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=len(tokenizer.word_index)+1, output_dim=max_length, input_length=max_length))\n",
    "model.add(LSTM(32))\n",
    "model.add(Dropout(0.19))\n",
    "model.add(Dense(1))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.1, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PPCC\\anaconda3\\envs\\my-rdkit-env\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 11834 samples, validate on 1315 samples\n",
      "Epoch 1/200\n",
      "11834/11834 [==============================] - 11s 943us/step - loss: 0.9921 - mae: 0.7438 - val_loss: 0.3756 - val_mae: 0.4700\n",
      "Epoch 2/200\n",
      "11834/11834 [==============================] - 10s 882us/step - loss: 0.3506 - mae: 0.4524 - val_loss: 0.3158 - val_mae: 0.4277\n",
      "Epoch 3/200\n",
      "11834/11834 [==============================] - 11s 916us/step - loss: 0.3093 - mae: 0.4248 - val_loss: 0.2865 - val_mae: 0.4028\n",
      "Epoch 4/200\n",
      "11834/11834 [==============================] - 10s 874us/step - loss: 0.2733 - mae: 0.3982 - val_loss: 0.2478 - val_mae: 0.3729\n",
      "Epoch 5/200\n",
      "11834/11834 [==============================] - 10s 875us/step - loss: 0.2349 - mae: 0.3708 - val_loss: 0.2081 - val_mae: 0.3427\n",
      "Epoch 6/200\n",
      "11834/11834 [==============================] - 11s 908us/step - loss: 0.2126 - mae: 0.3518 - val_loss: 0.1862 - val_mae: 0.3180\n",
      "Epoch 7/200\n",
      "11834/11834 [==============================] - 10s 868us/step - loss: 0.1895 - mae: 0.3297 - val_loss: 0.1626 - val_mae: 0.2929\n",
      "Epoch 8/200\n",
      "11834/11834 [==============================] - 11s 895us/step - loss: 0.1691 - mae: 0.3073 - val_loss: 0.1469 - val_mae: 0.2713\n",
      "Epoch 9/200\n",
      "11834/11834 [==============================] - 10s 873us/step - loss: 0.1593 - mae: 0.2970 - val_loss: 0.1348 - val_mae: 0.2588\n",
      "Epoch 10/200\n",
      "11834/11834 [==============================] - 10s 869us/step - loss: 0.1498 - mae: 0.2858 - val_loss: 0.1352 - val_mae: 0.2578\n",
      "Epoch 11/200\n",
      "11834/11834 [==============================] - 10s 858us/step - loss: 0.1487 - mae: 0.2836 - val_loss: 0.1266 - val_mae: 0.2462\n",
      "Epoch 12/200\n",
      "11834/11834 [==============================] - 10s 861us/step - loss: 0.1399 - mae: 0.2757 - val_loss: 0.1303 - val_mae: 0.2495\n",
      "Epoch 13/200\n",
      "11834/11834 [==============================] - 10s 855us/step - loss: 0.1435 - mae: 0.2784 - val_loss: 0.1364 - val_mae: 0.2506\n",
      "Epoch 14/200\n",
      "11834/11834 [==============================] - 10s 860us/step - loss: 0.1335 - mae: 0.2684 - val_loss: 0.1193 - val_mae: 0.2344\n",
      "Epoch 15/200\n",
      "11834/11834 [==============================] - 10s 857us/step - loss: 0.1305 - mae: 0.2655 - val_loss: 0.1118 - val_mae: 0.2312\n",
      "Epoch 16/200\n",
      "11834/11834 [==============================] - 10s 859us/step - loss: 0.1320 - mae: 0.2642 - val_loss: 0.1257 - val_mae: 0.2447\n",
      "Epoch 17/200\n",
      "11834/11834 [==============================] - 10s 872us/step - loss: 0.1248 - mae: 0.2587 - val_loss: 0.1217 - val_mae: 0.2409\n",
      "Epoch 18/200\n",
      "11834/11834 [==============================] - 10s 862us/step - loss: 0.1210 - mae: 0.2537 - val_loss: 0.1040 - val_mae: 0.2190\n",
      "Epoch 19/200\n",
      "11834/11834 [==============================] - 10s 855us/step - loss: 0.1186 - mae: 0.2525 - val_loss: 0.1091 - val_mae: 0.2266\n",
      "Epoch 20/200\n",
      "11834/11834 [==============================] - 10s 860us/step - loss: 0.1219 - mae: 0.2559 - val_loss: 0.1127 - val_mae: 0.2380\n",
      "Epoch 21/200\n",
      "11834/11834 [==============================] - 10s 859us/step - loss: 0.1167 - mae: 0.2491 - val_loss: 0.1004 - val_mae: 0.2208\n",
      "Epoch 22/200\n",
      "11834/11834 [==============================] - 10s 858us/step - loss: 0.1143 - mae: 0.2474 - val_loss: 0.1003 - val_mae: 0.2150\n",
      "Epoch 23/200\n",
      "11834/11834 [==============================] - 10s 855us/step - loss: 0.1096 - mae: 0.2411 - val_loss: 0.0944 - val_mae: 0.2101\n",
      "Epoch 24/200\n",
      "11834/11834 [==============================] - 10s 856us/step - loss: 0.1094 - mae: 0.2426 - val_loss: 0.0931 - val_mae: 0.2068\n",
      "Epoch 25/200\n",
      "11834/11834 [==============================] - 10s 854us/step - loss: 0.1092 - mae: 0.2417 - val_loss: 0.0969 - val_mae: 0.2111\n",
      "Epoch 26/200\n",
      "11834/11834 [==============================] - 10s 855us/step - loss: 0.1134 - mae: 0.2461 - val_loss: 0.0925 - val_mae: 0.2082\n",
      "Epoch 27/200\n",
      "11834/11834 [==============================] - 10s 860us/step - loss: 0.1052 - mae: 0.2374 - val_loss: 0.0892 - val_mae: 0.2049\n",
      "Epoch 28/200\n",
      "11834/11834 [==============================] - 10s 871us/step - loss: 0.1045 - mae: 0.2351 - val_loss: 0.0868 - val_mae: 0.1995\n",
      "Epoch 29/200\n",
      "11834/11834 [==============================] - 10s 871us/step - loss: 0.1018 - mae: 0.2339 - val_loss: 0.0853 - val_mae: 0.2008\n",
      "Epoch 30/200\n",
      "11834/11834 [==============================] - 10s 850us/step - loss: 0.1044 - mae: 0.2359 - val_loss: 0.0979 - val_mae: 0.2164\n",
      "Epoch 31/200\n",
      "11834/11834 [==============================] - 10s 862us/step - loss: 0.0977 - mae: 0.2272 - val_loss: 0.0834 - val_mae: 0.1983\n",
      "Epoch 32/200\n",
      "11834/11834 [==============================] - 10s 872us/step - loss: 0.0978 - mae: 0.2284 - val_loss: 0.0817 - val_mae: 0.1992\n",
      "Epoch 33/200\n",
      "11834/11834 [==============================] - 10s 855us/step - loss: 0.0994 - mae: 0.2302 - val_loss: 0.0786 - val_mae: 0.1935\n",
      "Epoch 34/200\n",
      "11834/11834 [==============================] - 10s 867us/step - loss: 0.0974 - mae: 0.2292 - val_loss: 0.0812 - val_mae: 0.1952\n",
      "Epoch 35/200\n",
      "11834/11834 [==============================] - 10s 860us/step - loss: 0.0954 - mae: 0.2262 - val_loss: 0.0774 - val_mae: 0.1933\n",
      "Epoch 36/200\n",
      "11834/11834 [==============================] - 10s 857us/step - loss: 0.0929 - mae: 0.2233 - val_loss: 0.0751 - val_mae: 0.1890\n",
      "Epoch 37/200\n",
      "11834/11834 [==============================] - 10s 861us/step - loss: 0.0926 - mae: 0.2236 - val_loss: 0.0802 - val_mae: 0.1954\n",
      "Epoch 38/200\n",
      "11834/11834 [==============================] - 10s 850us/step - loss: 0.0926 - mae: 0.2223 - val_loss: 0.0822 - val_mae: 0.2054\n",
      "Epoch 39/200\n",
      "11834/11834 [==============================] - 10s 860us/step - loss: 0.0916 - mae: 0.2218 - val_loss: 0.0757 - val_mae: 0.1893\n",
      "Epoch 40/200\n",
      "11834/11834 [==============================] - 10s 863us/step - loss: 0.0863 - mae: 0.2159 - val_loss: 0.0749 - val_mae: 0.1898\n",
      "Epoch 41/200\n",
      "11834/11834 [==============================] - 10s 851us/step - loss: 0.0871 - mae: 0.2163 - val_loss: 0.0685 - val_mae: 0.1792\n",
      "Epoch 42/200\n",
      "11834/11834 [==============================] - 10s 859us/step - loss: 0.0856 - mae: 0.2139 - val_loss: 0.0720 - val_mae: 0.1893\n",
      "Epoch 43/200\n",
      "11834/11834 [==============================] - 10s 856us/step - loss: 0.0857 - mae: 0.2153 - val_loss: 0.0712 - val_mae: 0.1831\n",
      "Epoch 44/200\n",
      "11834/11834 [==============================] - 10s 868us/step - loss: 0.0844 - mae: 0.2130 - val_loss: 0.0686 - val_mae: 0.1802\n",
      "Epoch 45/200\n",
      "11834/11834 [==============================] - 10s 856us/step - loss: 0.0866 - mae: 0.2162 - val_loss: 0.0683 - val_mae: 0.1833\n",
      "Epoch 46/200\n",
      "11834/11834 [==============================] - 10s 856us/step - loss: 0.0820 - mae: 0.2109 - val_loss: 0.0710 - val_mae: 0.1834\n",
      "Epoch 47/200\n",
      "11834/11834 [==============================] - 10s 856us/step - loss: 0.0815 - mae: 0.2106 - val_loss: 0.0651 - val_mae: 0.1752\n",
      "Epoch 48/200\n",
      "11834/11834 [==============================] - 10s 858us/step - loss: 0.0798 - mae: 0.2070 - val_loss: 0.0681 - val_mae: 0.1833\n",
      "Epoch 49/200\n",
      "11834/11834 [==============================] - 10s 855us/step - loss: 0.0798 - mae: 0.2077 - val_loss: 0.0676 - val_mae: 0.1838\n",
      "Epoch 50/200\n",
      "11834/11834 [==============================] - 10s 861us/step - loss: 0.0771 - mae: 0.2047 - val_loss: 0.0651 - val_mae: 0.1761\n",
      "Epoch 51/200\n",
      "11834/11834 [==============================] - 10s 855us/step - loss: 0.0800 - mae: 0.2063 - val_loss: 0.0698 - val_mae: 0.1938\n",
      "Epoch 52/200\n",
      "11834/11834 [==============================] - 10s 868us/step - loss: 0.0780 - mae: 0.2064 - val_loss: 0.0585 - val_mae: 0.1679\n",
      "Epoch 53/200\n",
      "11834/11834 [==============================] - 10s 858us/step - loss: 0.0754 - mae: 0.2012 - val_loss: 0.0607 - val_mae: 0.1706\n",
      "Epoch 54/200\n",
      "11834/11834 [==============================] - 10s 863us/step - loss: 0.0761 - mae: 0.2025 - val_loss: 0.0638 - val_mae: 0.1756\n",
      "Epoch 55/200\n",
      "11834/11834 [==============================] - 10s 852us/step - loss: 0.0742 - mae: 0.2023 - val_loss: 0.0613 - val_mae: 0.1718\n",
      "Epoch 56/200\n",
      "11834/11834 [==============================] - 10s 864us/step - loss: 0.0728 - mae: 0.1986 - val_loss: 0.0617 - val_mae: 0.1723\n",
      "Epoch 57/200\n",
      "11834/11834 [==============================] - 10s 852us/step - loss: 0.0735 - mae: 0.1992 - val_loss: 0.0601 - val_mae: 0.1713\n",
      "Epoch 58/200\n",
      "11834/11834 [==============================] - 10s 848us/step - loss: 0.0711 - mae: 0.1974 - val_loss: 0.0575 - val_mae: 0.1669\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59/200\n",
      "11834/11834 [==============================] - 10s 853us/step - loss: 0.0706 - mae: 0.1966 - val_loss: 0.0595 - val_mae: 0.1739\n",
      "Epoch 60/200\n",
      "11834/11834 [==============================] - 10s 866us/step - loss: 0.0707 - mae: 0.1973 - val_loss: 0.0564 - val_mae: 0.1644\n",
      "Epoch 61/200\n",
      "11834/11834 [==============================] - 10s 849us/step - loss: 0.0745 - mae: 0.2021 - val_loss: 0.0626 - val_mae: 0.1737\n",
      "Epoch 62/200\n",
      "11834/11834 [==============================] - 11s 899us/step - loss: 0.0691 - mae: 0.1945 - val_loss: 0.0593 - val_mae: 0.1743\n",
      "Epoch 63/200\n",
      "11834/11834 [==============================] - 10s 873us/step - loss: 0.0680 - mae: 0.1932 - val_loss: 0.0554 - val_mae: 0.1635\n",
      "Epoch 64/200\n",
      "11834/11834 [==============================] - 10s 866us/step - loss: 0.0690 - mae: 0.1957 - val_loss: 0.0556 - val_mae: 0.1658\n",
      "Epoch 65/200\n",
      "11834/11834 [==============================] - 10s 858us/step - loss: 0.0665 - mae: 0.1918 - val_loss: 0.0539 - val_mae: 0.1607\n",
      "Epoch 66/200\n",
      "11834/11834 [==============================] - 10s 864us/step - loss: 0.0664 - mae: 0.1905 - val_loss: 0.0552 - val_mae: 0.1653\n",
      "Epoch 67/200\n",
      "11834/11834 [==============================] - 10s 855us/step - loss: 0.0660 - mae: 0.1900 - val_loss: 0.0537 - val_mae: 0.1609\n",
      "Epoch 68/200\n",
      "11834/11834 [==============================] - 10s 865us/step - loss: 0.0669 - mae: 0.1910 - val_loss: 0.0528 - val_mae: 0.1618\n",
      "Epoch 69/200\n",
      "11834/11834 [==============================] - 10s 849us/step - loss: 0.0654 - mae: 0.1896 - val_loss: 0.0531 - val_mae: 0.1608\n",
      "Epoch 70/200\n",
      "11834/11834 [==============================] - 10s 859us/step - loss: 0.0652 - mae: 0.1897 - val_loss: 0.0578 - val_mae: 0.1687\n",
      "Epoch 71/200\n",
      "11834/11834 [==============================] - 10s 852us/step - loss: 0.0633 - mae: 0.1875 - val_loss: 0.0551 - val_mae: 0.1630\n",
      "Epoch 72/200\n",
      "11834/11834 [==============================] - 10s 862us/step - loss: 0.0636 - mae: 0.1867 - val_loss: 0.0543 - val_mae: 0.1628\n",
      "Epoch 73/200\n",
      "11834/11834 [==============================] - 10s 856us/step - loss: 0.0617 - mae: 0.1865 - val_loss: 0.0514 - val_mae: 0.1588\n",
      "Epoch 74/200\n",
      "11834/11834 [==============================] - 10s 858us/step - loss: 0.0603 - mae: 0.1830 - val_loss: 0.0490 - val_mae: 0.1527\n",
      "Epoch 75/200\n",
      "11834/11834 [==============================] - 10s 871us/step - loss: 0.0623 - mae: 0.1846 - val_loss: 0.0495 - val_mae: 0.1554\n",
      "Epoch 76/200\n",
      "11834/11834 [==============================] - 10s 868us/step - loss: 0.0625 - mae: 0.1862 - val_loss: 0.0494 - val_mae: 0.1547\n",
      "Epoch 77/200\n",
      "11834/11834 [==============================] - 10s 852us/step - loss: 0.0604 - mae: 0.1815 - val_loss: 0.0492 - val_mae: 0.1536\n",
      "Epoch 78/200\n",
      "11834/11834 [==============================] - 10s 851us/step - loss: 0.0623 - mae: 0.1858 - val_loss: 0.0518 - val_mae: 0.1596\n",
      "Epoch 79/200\n",
      "11834/11834 [==============================] - 10s 856us/step - loss: 0.0606 - mae: 0.1826 - val_loss: 0.0503 - val_mae: 0.1572\n",
      "Epoch 80/200\n",
      "11834/11834 [==============================] - 10s 851us/step - loss: 0.0592 - mae: 0.1803 - val_loss: 0.0494 - val_mae: 0.1544\n",
      "Epoch 81/200\n",
      "11834/11834 [==============================] - 10s 854us/step - loss: 0.0590 - mae: 0.1807 - val_loss: 0.0496 - val_mae: 0.1537\n",
      "Epoch 82/200\n",
      "11834/11834 [==============================] - 10s 846us/step - loss: 0.0606 - mae: 0.1822 - val_loss: 0.0506 - val_mae: 0.1600\n",
      "Epoch 83/200\n",
      "11834/11834 [==============================] - 10s 852us/step - loss: 0.0596 - mae: 0.1812 - val_loss: 0.0458 - val_mae: 0.1492\n",
      "Epoch 84/200\n",
      "11834/11834 [==============================] - 10s 859us/step - loss: 0.0586 - mae: 0.1799 - val_loss: 0.0452 - val_mae: 0.1477\n",
      "Epoch 85/200\n",
      "11834/11834 [==============================] - 10s 859us/step - loss: 0.0577 - mae: 0.1789 - val_loss: 0.0507 - val_mae: 0.1571\n",
      "Epoch 86/200\n",
      "11834/11834 [==============================] - 10s 873us/step - loss: 0.0586 - mae: 0.1798 - val_loss: 0.0449 - val_mae: 0.1484\n",
      "Epoch 87/200\n",
      "11834/11834 [==============================] - 10s 866us/step - loss: 0.0572 - mae: 0.1772 - val_loss: 0.0498 - val_mae: 0.1573\n",
      "Epoch 88/200\n",
      "11834/11834 [==============================] - 10s 855us/step - loss: 0.0572 - mae: 0.1778 - val_loss: 0.0496 - val_mae: 0.1544\n",
      "Epoch 89/200\n",
      "11834/11834 [==============================] - 10s 853us/step - loss: 0.0579 - mae: 0.1783 - val_loss: 0.0490 - val_mae: 0.1566\n",
      "Epoch 90/200\n",
      "11834/11834 [==============================] - 10s 858us/step - loss: 0.0560 - mae: 0.1764 - val_loss: 0.0427 - val_mae: 0.1452\n",
      "Epoch 91/200\n",
      "11834/11834 [==============================] - 10s 855us/step - loss: 0.0548 - mae: 0.1742 - val_loss: 0.0462 - val_mae: 0.1523\n",
      "Epoch 92/200\n",
      "11834/11834 [==============================] - 10s 860us/step - loss: 0.0544 - mae: 0.1736 - val_loss: 0.0433 - val_mae: 0.1433\n",
      "Epoch 93/200\n",
      "11834/11834 [==============================] - 10s 847us/step - loss: 0.0550 - mae: 0.1730 - val_loss: 0.0431 - val_mae: 0.1440\n",
      "Epoch 94/200\n",
      "11834/11834 [==============================] - 10s 862us/step - loss: 0.0543 - mae: 0.1722 - val_loss: 0.0464 - val_mae: 0.1518\n",
      "Epoch 95/200\n",
      "11834/11834 [==============================] - 10s 872us/step - loss: 0.0534 - mae: 0.1712 - val_loss: 0.0442 - val_mae: 0.1451\n",
      "Epoch 96/200\n",
      "11834/11834 [==============================] - 11s 888us/step - loss: 0.0542 - mae: 0.1738 - val_loss: 0.0430 - val_mae: 0.1439\n",
      "Epoch 97/200\n",
      "11834/11834 [==============================] - 11s 918us/step - loss: 0.0530 - mae: 0.1704 - val_loss: 0.0450 - val_mae: 0.1489\n",
      "Epoch 98/200\n",
      "11834/11834 [==============================] - 11s 898us/step - loss: 0.0530 - mae: 0.1717 - val_loss: 0.0440 - val_mae: 0.1463\n",
      "Epoch 99/200\n",
      "11834/11834 [==============================] - 10s 876us/step - loss: 0.0529 - mae: 0.1704 - val_loss: 0.0426 - val_mae: 0.1430\n",
      "Epoch 100/200\n",
      "11834/11834 [==============================] - 10s 852us/step - loss: 0.0516 - mae: 0.1696 - val_loss: 0.0422 - val_mae: 0.1409\n",
      "Epoch 101/200\n",
      "11834/11834 [==============================] - 10s 884us/step - loss: 0.0526 - mae: 0.1710 - val_loss: 0.0476 - val_mae: 0.1574\n",
      "Epoch 102/200\n",
      "11834/11834 [==============================] - 10s 856us/step - loss: 0.0528 - mae: 0.1699 - val_loss: 0.0421 - val_mae: 0.1419\n",
      "Epoch 103/200\n",
      "11834/11834 [==============================] - 10s 854us/step - loss: 0.0519 - mae: 0.1702 - val_loss: 0.0419 - val_mae: 0.1432\n",
      "Epoch 104/200\n",
      "11834/11834 [==============================] - 10s 839us/step - loss: 0.0506 - mae: 0.1672 - val_loss: 0.0420 - val_mae: 0.1426\n",
      "Epoch 105/200\n",
      "11834/11834 [==============================] - 11s 928us/step - loss: 0.0526 - mae: 0.1704 - val_loss: 0.0454 - val_mae: 0.1480\n",
      "Epoch 106/200\n",
      "11834/11834 [==============================] - 12s 1ms/step - loss: 0.0515 - mae: 0.1691 - val_loss: 0.0472 - val_mae: 0.1518\n",
      "Epoch 107/200\n",
      "11834/11834 [==============================] - 10s 842us/step - loss: 0.0502 - mae: 0.1680 - val_loss: 0.0416 - val_mae: 0.1403\n",
      "Epoch 108/200\n",
      "11834/11834 [==============================] - 10s 841us/step - loss: 0.0505 - mae: 0.1678 - val_loss: 0.0410 - val_mae: 0.1397\n",
      "Epoch 109/200\n",
      "11834/11834 [==============================] - 10s 843us/step - loss: 0.0508 - mae: 0.1677 - val_loss: 0.0469 - val_mae: 0.1487\n",
      "Epoch 110/200\n",
      "11834/11834 [==============================] - 10s 858us/step - loss: 0.0517 - mae: 0.1685 - val_loss: 0.0544 - val_mae: 0.1652\n",
      "Epoch 111/200\n",
      "11834/11834 [==============================] - 10s 868us/step - loss: 0.0508 - mae: 0.1676 - val_loss: 0.0413 - val_mae: 0.1395\n",
      "Epoch 112/200\n",
      "11834/11834 [==============================] - 10s 851us/step - loss: 0.0506 - mae: 0.1679 - val_loss: 0.0418 - val_mae: 0.1430\n",
      "Epoch 113/200\n",
      "11834/11834 [==============================] - 11s 910us/step - loss: 0.0493 - mae: 0.1644 - val_loss: 0.0412 - val_mae: 0.1415\n",
      "Epoch 114/200\n",
      "11834/11834 [==============================] - 10s 869us/step - loss: 0.0465 - mae: 0.1607 - val_loss: 0.0434 - val_mae: 0.1443\n",
      "Epoch 115/200\n",
      "11834/11834 [==============================] - 11s 943us/step - loss: 0.0478 - mae: 0.1642 - val_loss: 0.0403 - val_mae: 0.1401\n",
      "Epoch 116/200\n",
      "11834/11834 [==============================] - 10s 842us/step - loss: 0.0479 - mae: 0.1619 - val_loss: 0.0401 - val_mae: 0.1361\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 117/200\n",
      "11834/11834 [==============================] - 10s 841us/step - loss: 0.0480 - mae: 0.1626 - val_loss: 0.0393 - val_mae: 0.1347\n",
      "Epoch 118/200\n",
      "11834/11834 [==============================] - 10s 861us/step - loss: 0.0463 - mae: 0.1605 - val_loss: 0.0402 - val_mae: 0.1383\n",
      "Epoch 119/200\n",
      "11834/11834 [==============================] - 10s 869us/step - loss: 0.0483 - mae: 0.1639 - val_loss: 0.0381 - val_mae: 0.1343\n",
      "Epoch 120/200\n",
      "11834/11834 [==============================] - 10s 843us/step - loss: 0.0468 - mae: 0.1612 - val_loss: 0.0426 - val_mae: 0.1397\n",
      "Epoch 121/200\n",
      "11834/11834 [==============================] - 10s 883us/step - loss: 0.0473 - mae: 0.1617 - val_loss: 0.0433 - val_mae: 0.1429\n",
      "Epoch 122/200\n",
      "11834/11834 [==============================] - 10s 868us/step - loss: 0.0483 - mae: 0.1637 - val_loss: 0.0435 - val_mae: 0.1446\n",
      "Epoch 123/200\n",
      "11834/11834 [==============================] - 11s 888us/step - loss: 0.0485 - mae: 0.1642 - val_loss: 0.0420 - val_mae: 0.1373\n",
      "Epoch 124/200\n",
      "11834/11834 [==============================] - 11s 898us/step - loss: 0.0464 - mae: 0.1595 - val_loss: 0.0385 - val_mae: 0.1335\n",
      "Epoch 125/200\n",
      "11834/11834 [==============================] - 11s 919us/step - loss: 0.0460 - mae: 0.1604 - val_loss: 0.0375 - val_mae: 0.1318\n",
      "Epoch 126/200\n",
      "11834/11834 [==============================] - 10s 881us/step - loss: 0.0459 - mae: 0.1594 - val_loss: 0.0393 - val_mae: 0.1338\n",
      "Epoch 127/200\n",
      "11834/11834 [==============================] - 11s 918us/step - loss: 0.0455 - mae: 0.1580 - val_loss: 0.0398 - val_mae: 0.1337\n",
      "Epoch 128/200\n",
      "11834/11834 [==============================] - 10s 882us/step - loss: 0.0455 - mae: 0.1587 - val_loss: 0.0405 - val_mae: 0.1364\n",
      "Epoch 129/200\n",
      "11834/11834 [==============================] - 10s 859us/step - loss: 0.0454 - mae: 0.1586 - val_loss: 0.0423 - val_mae: 0.1431\n",
      "Epoch 130/200\n",
      "11834/11834 [==============================] - 10s 868us/step - loss: 0.0445 - mae: 0.1575 - val_loss: 0.0394 - val_mae: 0.1368\n",
      "Epoch 131/200\n",
      "11834/11834 [==============================] - 10s 881us/step - loss: 0.0448 - mae: 0.1572 - val_loss: 0.0423 - val_mae: 0.1404\n",
      "Epoch 132/200\n",
      "11834/11834 [==============================] - 11s 889us/step - loss: 0.0463 - mae: 0.1594 - val_loss: 0.0405 - val_mae: 0.1380\n",
      "Epoch 133/200\n",
      "11834/11834 [==============================] - 11s 919us/step - loss: 0.0453 - mae: 0.1585 - val_loss: 0.0388 - val_mae: 0.1348\n",
      "Epoch 134/200\n",
      "11834/11834 [==============================] - 11s 910us/step - loss: 0.0430 - mae: 0.1545 - val_loss: 0.0366 - val_mae: 0.1301\n",
      "Epoch 135/200\n",
      "11834/11834 [==============================] - 11s 888us/step - loss: 0.0442 - mae: 0.1564 - val_loss: 0.0378 - val_mae: 0.1323\n",
      "Epoch 136/200\n",
      "11834/11834 [==============================] - 10s 886us/step - loss: 0.0441 - mae: 0.1550 - val_loss: 0.0414 - val_mae: 0.1416\n",
      "Epoch 137/200\n",
      "11834/11834 [==============================] - 10s 877us/step - loss: 0.0450 - mae: 0.1574 - val_loss: 0.0369 - val_mae: 0.1324\n",
      "Epoch 138/200\n",
      "11834/11834 [==============================] - 10s 863us/step - loss: 0.0443 - mae: 0.1566 - val_loss: 0.0367 - val_mae: 0.1305\n",
      "Epoch 139/200\n",
      "11834/11834 [==============================] - 10s 875us/step - loss: 0.0433 - mae: 0.1548 - val_loss: 0.0410 - val_mae: 0.1398\n",
      "Epoch 140/200\n",
      "11834/11834 [==============================] - 10s 861us/step - loss: 0.0431 - mae: 0.1552 - val_loss: 0.0402 - val_mae: 0.1433\n",
      "Epoch 141/200\n",
      "11834/11834 [==============================] - 10s 862us/step - loss: 0.0444 - mae: 0.1571 - val_loss: 0.0412 - val_mae: 0.1385\n",
      "Epoch 142/200\n",
      "11834/11834 [==============================] - 10s 853us/step - loss: 0.0442 - mae: 0.1569 - val_loss: 0.0357 - val_mae: 0.1295\n",
      "Epoch 143/200\n",
      "11834/11834 [==============================] - 10s 871us/step - loss: 0.0429 - mae: 0.1539 - val_loss: 0.0362 - val_mae: 0.1304\n",
      "Epoch 144/200\n",
      "11834/11834 [==============================] - 10s 859us/step - loss: 0.0430 - mae: 0.1540 - val_loss: 0.0358 - val_mae: 0.1290\n",
      "Epoch 145/200\n",
      "11834/11834 [==============================] - 10s 883us/step - loss: 0.0422 - mae: 0.1526 - val_loss: 0.0392 - val_mae: 0.1376\n",
      "Epoch 146/200\n",
      "11834/11834 [==============================] - 10s 865us/step - loss: 0.0437 - mae: 0.1555 - val_loss: 0.0440 - val_mae: 0.1435\n",
      "Epoch 147/200\n",
      "11834/11834 [==============================] - 10s 863us/step - loss: 0.0465 - mae: 0.1595 - val_loss: 0.0373 - val_mae: 0.1306\n",
      "Epoch 148/200\n",
      "11834/11834 [==============================] - 10s 855us/step - loss: 0.0428 - mae: 0.1535 - val_loss: 0.0355 - val_mae: 0.1285\n",
      "Epoch 149/200\n",
      "11834/11834 [==============================] - 10s 862us/step - loss: 0.0419 - mae: 0.1522 - val_loss: 0.0359 - val_mae: 0.1296\n",
      "Epoch 150/200\n",
      "11834/11834 [==============================] - 11s 897us/step - loss: 0.0421 - mae: 0.1524 - val_loss: 0.0368 - val_mae: 0.1323\n",
      "Epoch 151/200\n",
      "11834/11834 [==============================] - 11s 924us/step - loss: 0.0420 - mae: 0.1517 - val_loss: 0.0366 - val_mae: 0.1320\n",
      "Epoch 152/200\n",
      "11834/11834 [==============================] - 11s 930us/step - loss: 0.0404 - mae: 0.1500 - val_loss: 0.0365 - val_mae: 0.1320\n",
      "Epoch 153/200\n",
      "11834/11834 [==============================] - 12s 1ms/step - loss: 0.0400 - mae: 0.1478 - val_loss: 0.0366 - val_mae: 0.1321\n",
      "Epoch 154/200\n",
      "11834/11834 [==============================] - 11s 927us/step - loss: 0.0420 - mae: 0.1527 - val_loss: 0.0459 - val_mae: 0.1469\n",
      "Epoch 155/200\n",
      "11834/11834 [==============================] - 11s 915us/step - loss: 0.0442 - mae: 0.1563 - val_loss: 0.0400 - val_mae: 0.1368\n",
      "Epoch 156/200\n",
      "11834/11834 [==============================] - 11s 907us/step - loss: 0.0416 - mae: 0.1513 - val_loss: 0.0354 - val_mae: 0.1307\n",
      "Epoch 157/200\n",
      "11834/11834 [==============================] - 10s 882us/step - loss: 0.0421 - mae: 0.1532 - val_loss: 0.0373 - val_mae: 0.1347\n",
      "Epoch 158/200\n",
      "11834/11834 [==============================] - 10s 879us/step - loss: 0.0412 - mae: 0.1511 - val_loss: 0.0361 - val_mae: 0.1298\n",
      "Epoch 159/200\n",
      "11834/11834 [==============================] - 10s 861us/step - loss: 0.0406 - mae: 0.1485 - val_loss: 0.0367 - val_mae: 0.1292\n",
      "Epoch 160/200\n",
      "11834/11834 [==============================] - 10s 875us/step - loss: 0.0397 - mae: 0.1488 - val_loss: 0.0355 - val_mae: 0.1273\n",
      "Epoch 161/200\n",
      "11834/11834 [==============================] - 10s 862us/step - loss: 0.0406 - mae: 0.1499 - val_loss: 0.0352 - val_mae: 0.1286\n",
      "Epoch 162/200\n",
      "11834/11834 [==============================] - 11s 888us/step - loss: 0.0412 - mae: 0.1503 - val_loss: 0.0338 - val_mae: 0.1262\n",
      "Epoch 163/200\n",
      "11834/11834 [==============================] - 10s 887us/step - loss: 0.0396 - mae: 0.1476 - val_loss: 0.0350 - val_mae: 0.1294\n",
      "Epoch 164/200\n",
      "11834/11834 [==============================] - 10s 886us/step - loss: 0.0404 - mae: 0.1492 - val_loss: 0.0357 - val_mae: 0.1305\n",
      "Epoch 165/200\n",
      "11834/11834 [==============================] - 10s 886us/step - loss: 0.0403 - mae: 0.1501 - val_loss: 0.0360 - val_mae: 0.1319\n",
      "Epoch 166/200\n",
      "11834/11834 [==============================] - 10s 851us/step - loss: 0.0402 - mae: 0.1488 - val_loss: 0.0331 - val_mae: 0.1251\n",
      "Epoch 167/200\n",
      "11834/11834 [==============================] - 10s 853us/step - loss: 0.0417 - mae: 0.1517 - val_loss: 0.0345 - val_mae: 0.1289\n",
      "Epoch 168/200\n",
      "11834/11834 [==============================] - 10s 870us/step - loss: 0.0405 - mae: 0.1493 - val_loss: 0.0337 - val_mae: 0.1246\n",
      "Epoch 169/200\n",
      "11834/11834 [==============================] - 10s 867us/step - loss: 0.0390 - mae: 0.1465 - val_loss: 0.0327 - val_mae: 0.1234\n",
      "Epoch 170/200\n",
      "11834/11834 [==============================] - 10s 866us/step - loss: 0.0385 - mae: 0.1464 - val_loss: 0.0330 - val_mae: 0.1230\n",
      "Epoch 171/200\n",
      "11834/11834 [==============================] - 10s 877us/step - loss: 0.0402 - mae: 0.1485 - val_loss: 0.0347 - val_mae: 0.1255\n",
      "Epoch 172/200\n",
      "11834/11834 [==============================] - 11s 968us/step - loss: 0.0404 - mae: 0.1494 - val_loss: 0.0337 - val_mae: 0.1263\n",
      "Epoch 173/200\n",
      "11834/11834 [==============================] - 10s 855us/step - loss: 0.0380 - mae: 0.1452 - val_loss: 0.0380 - val_mae: 0.1378\n",
      "Epoch 174/200\n",
      "11834/11834 [==============================] - 10s 885us/step - loss: 0.0393 - mae: 0.1477 - val_loss: 0.0332 - val_mae: 0.1237\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 175/200\n",
      "11834/11834 [==============================] - 11s 891us/step - loss: 0.0394 - mae: 0.1470 - val_loss: 0.0359 - val_mae: 0.1276\n",
      "Epoch 176/200\n",
      "11834/11834 [==============================] - 10s 867us/step - loss: 0.0391 - mae: 0.1465 - val_loss: 0.0327 - val_mae: 0.1260\n",
      "Epoch 177/200\n",
      "11834/11834 [==============================] - 10s 846us/step - loss: 0.0391 - mae: 0.1477 - val_loss: 0.0319 - val_mae: 0.1214\n",
      "Epoch 178/200\n",
      "11834/11834 [==============================] - 10s 874us/step - loss: 0.0388 - mae: 0.1470 - val_loss: 0.0347 - val_mae: 0.1274\n",
      "Epoch 179/200\n",
      "11834/11834 [==============================] - 11s 892us/step - loss: 0.0367 - mae: 0.1431 - val_loss: 0.0317 - val_mae: 0.1203\n",
      "Epoch 180/200\n",
      "11834/11834 [==============================] - 11s 914us/step - loss: 0.0382 - mae: 0.1451 - val_loss: 0.0344 - val_mae: 0.1273\n",
      "Epoch 181/200\n",
      "11834/11834 [==============================] - 10s 876us/step - loss: 0.0391 - mae: 0.1471 - val_loss: 0.0327 - val_mae: 0.1251\n",
      "Epoch 182/200\n",
      "11834/11834 [==============================] - 10s 874us/step - loss: 0.0383 - mae: 0.1450 - val_loss: 0.0319 - val_mae: 0.1206\n",
      "Epoch 183/200\n",
      "11834/11834 [==============================] - 10s 865us/step - loss: 0.0373 - mae: 0.1441 - val_loss: 0.0347 - val_mae: 0.1311\n",
      "Epoch 184/200\n",
      "11834/11834 [==============================] - 10s 867us/step - loss: 0.0381 - mae: 0.1459 - val_loss: 0.0317 - val_mae: 0.1201\n",
      "Epoch 185/200\n",
      "11834/11834 [==============================] - 11s 889us/step - loss: 0.0381 - mae: 0.1449 - val_loss: 0.0313 - val_mae: 0.1203\n",
      "Epoch 186/200\n",
      "11834/11834 [==============================] - 10s 865us/step - loss: 0.0378 - mae: 0.1438 - val_loss: 0.0324 - val_mae: 0.1260\n",
      "Epoch 187/200\n",
      "11834/11834 [==============================] - 11s 953us/step - loss: 0.0377 - mae: 0.1442 - val_loss: 0.0321 - val_mae: 0.1220\n",
      "Epoch 188/200\n",
      "11834/11834 [==============================] - 10s 865us/step - loss: 0.0378 - mae: 0.1450 - val_loss: 0.0327 - val_mae: 0.1243\n",
      "Epoch 189/200\n",
      "11834/11834 [==============================] - 10s 865us/step - loss: 0.0377 - mae: 0.1442 - val_loss: 0.0314 - val_mae: 0.1207\n",
      "Epoch 190/200\n",
      "11834/11834 [==============================] - 10s 877us/step - loss: 0.0375 - mae: 0.1440 - val_loss: 0.0309 - val_mae: 0.1217\n",
      "Epoch 191/200\n",
      "11834/11834 [==============================] - 10s 879us/step - loss: 0.0377 - mae: 0.1433 - val_loss: 0.0317 - val_mae: 0.1191\n",
      "Epoch 192/200\n",
      "11834/11834 [==============================] - 10s 860us/step - loss: 0.0376 - mae: 0.1446 - val_loss: 0.0322 - val_mae: 0.1263\n",
      "Epoch 193/200\n",
      "11834/11834 [==============================] - 10s 867us/step - loss: 0.0379 - mae: 0.1436 - val_loss: 0.0341 - val_mae: 0.1260\n",
      "Epoch 194/200\n",
      "11834/11834 [==============================] - 10s 860us/step - loss: 0.0375 - mae: 0.1446 - val_loss: 0.0397 - val_mae: 0.1357\n",
      "Epoch 195/200\n",
      "11834/11834 [==============================] - 10s 886us/step - loss: 0.0381 - mae: 0.1455 - val_loss: 0.0319 - val_mae: 0.1212\n",
      "Epoch 196/200\n",
      "11834/11834 [==============================] - 10s 884us/step - loss: 0.0386 - mae: 0.1461 - val_loss: 0.0313 - val_mae: 0.1204\n",
      "Epoch 197/200\n",
      "11834/11834 [==============================] - 10s 862us/step - loss: 0.0358 - mae: 0.1415 - val_loss: 0.0323 - val_mae: 0.1199\n",
      "Epoch 198/200\n",
      "11834/11834 [==============================] - 10s 883us/step - loss: 0.0364 - mae: 0.1416 - val_loss: 0.0337 - val_mae: 0.1268\n",
      "Epoch 199/200\n",
      "11834/11834 [==============================] - 10s 884us/step - loss: 0.0376 - mae: 0.1440 - val_loss: 0.0334 - val_mae: 0.1240\n",
      "Epoch 200/200\n",
      "11834/11834 [==============================] - 10s 868us/step - loss: 0.0369 - mae: 0.1426 - val_loss: 0.0334 - val_mae: 0.1265\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss=\"mse\", optimizer=\"adam\", metrics=[\"mae\"])\n",
    "history = model.fit(X_train, y_train, epochs=200, batch_size=128, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1461/1461 [==============================] - 2s 2ms/step\n",
      "[0.02756046939228166, 0.11915440112352371]\n"
     ]
    }
   ],
   "source": [
    "results = model.evaluate(X_test, y_test)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['val_loss', 'val_mae', 'loss', 'mae'])\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3deXxU9dn//9eVyUYWlpCELUASdhQJEJACIohVQIuIWqX8VMRbi616W1urX63K7XJ/26/WenO73ViXuhVsXW6ruAEiKi5syr4JiYQ1RLJAEshy/f74nIQhG2GZTGCu5+ORR2bOnHPmmjPJvOfzOed8jqgqxhhjQldYsAswxhgTXBYExhgT4iwIjDEmxFkQGGNMiLMgMMaYEGdBYIwxIc6CwJxUIvK+iFx7sucNJhHJEpHzA7BeFZHu3u1nROTexsx7HM8zRUQ+Ot46G1jvKBHJOdnrNU0vPNgFmOATkf1+d2OAg0CFd/+XqvpqY9elquMCMe/pTlWnn4z1iEgqsBWIUNVyb92vAo1+D03osSAwqGpc1W0RyQL+TVXn1ZxPRMKrPlyMMacP6xoy9apq+ovInSKyC3hBRNqIyLsikisi+7zbKX7LLBSRf/NuTxWRz0XkUW/erSIy7jjnTRORRSJSJCLzRORJEXmlnrobU+ODIvKFt76PRCTR7/GrRSRbRPJE5J4Gts9QEdklIj6/aZeKyErv9hAR+VJE8kVkp4g8ISKR9azrRRF5yO/+Hd4yO0RkWo15LxKRFSJSKCLbRGSG38OLvN/5IrJfRH5StW39lh8mIktEpMD7Payx26YhItLHWz5fRNaIyAS/x8aLyFpvndtF5Hfe9ETv/ckXkR9F5DMRsc+lJmYb3BxNeyAB6ArciPubecG73wUoAZ5oYPmzgQ1AIvD/gOdERI5j3teAb4C2wAzg6gaeszE1/gK4DkgGIoGqD6a+wNPe+jt6z5dCHVT1K+AAcF6N9b7m3a4AfuO9np8AY4BfNVA3Xg1jvXp+CvQAau6fOABcA7QGLgJuEpGJ3mMjvd+tVTVOVb+sse4E4D1gpvfaHgPeE5G2NV5DrW1zlJojgH8BH3nL3QK8KiK9vFmew3UzxgNnAgu86b8FcoAkoB1wN2Dj3jQxCwJzNJXA/ap6UFVLVDVPVd9Q1WJVLQIeBs5tYPlsVX1WVSuAvwEdcP/wjZ5XRLoAg4H7VPWQqn4OvFPfEzayxhdUdaOqlgCvAxne9MuBd1V1kaoeBO71tkF9/g5MBhCReGC8Nw1VXaaqX6lquapmAf9TRx11+blX32pVPYALPv/Xt1BVV6lqpaqu9J6vMesFFxybVPVlr66/A+uBn/nNU9+2achQIA74o/ceLQDexds2QBnQV0Raquo+VV3uN70D0FVVy1T1M7UB0JqcBYE5mlxVLa26IyIxIvI/XtdJIa4rorV/90gNu6puqGqxdzPuGOftCPzoNw1gW30FN7LGXX63i/1q6ui/bu+DOK++58J9+58kIlHAJGC5qmZ7dfT0uj12eXX8J651cDRH1ABk13h9Z4vIJ17XVwEwvZHrrVp3do1p2UAnv/v1bZuj1qyq/qHpv97LcCGZLSKfishPvOmPAJuBj0Rki4jc1biXYU4mCwJzNDW/nf0W6AWcraotOdwVUV93z8mwE0gQkRi/aZ0bmP9Eatzpv27vOdvWN7OqrsV94I3jyG4hcF1M64EeXh13H08NuO4tf6/hWkSdVbUV8Izfeo/2bXoHrsvMXxdgeyPqOtp6O9fo369er6ouUdVLcN1Gb+NaGqhqkar+VlXTca2S20VkzAnWYo6RBYE5VvG4Pvd8r7/5/kA/ofcNeykwQ0QivW+TP2tgkROp8Z/AxSIywtux+wBH/z95DbgVFzj/qFFHIbBfRHoDNzWyhteBqSLS1wuimvXH41pIpSIyBBdAVXJxXVnp9ax7LtBTRH4hIuEiciXQF9eNcyK+xu27+L2IRIjIKNx7NNt7z6aISCtVLcNtkwoAEblYRLp7+4KqplfU/RQmUCwIzLF6HGgB7AW+Aj5oouedgtvhmgc8BMzBne9Ql+OuUVXXAL/GfbjvBPbhdmY25O/AKGCBqu71m/473Id0EfCsV3Njanjfew0LcN0mC2rM8ivgAREpAu7D+3btLVuM2yfyhXckztAa684DLsa1mvKA3wMX16j7mKnqIWACrmW0F3gKuEZV13uzXA1keV1k04H/z5veA5gH7Ae+BJ5S1YUnUos5dmL7ZcypSETmAOtVNeAtEmNOd9YiMKcEERksIt1EJMw7vPISXF+zMeYE2ZnF5lTRHngTt+M2B7hJVVcEtyRjTg/WNWSMMSHOuoaMMSbEnXJdQ4mJiZqamhrsMowx5pSybNmyvaqaVNdjp1wQpKamsnTp0mCXYYwxpxQRqXlGeTXrGjLGmBBnQWCMMSHOgsAYY0JcwPYRiMjzuFPZ96jqmXU8LsB/4UYkLAam+g1Na4xpRsrKysjJyaG0tPToM5ugio6OJiUlhYiIiEYvE8idxS/iLgbyUj2Pj8ONM9IDd0GSp73fxphmJicnh/j4eFJTU6n/ukIm2FSVvLw8cnJySEtLa/RyAesaUtVFwI8NzHIJ8JI6X+HGi+8QqHqMMcevtLSUtm3bWgg0cyJC27Ztj7nlFsx9BJ048uIbORx5cQxjTDNiIXBqOJ73KZhBUFe1dY53ISI3ishSEVmam5t7XE+2evVq7r33Xvbs2XNcyxtjzOkqmEGQw5FXYUrBXeWoFlWdpaqZqpqZlFTniXFHtW7dOh566CELAmNOQXl5eWRkZJCRkUH79u3p1KlT9f1Dhw41uOzSpUu59dZbj/ocw4YNOym1Lly4kIsvvvikrKupBPPM4neAm0VkNm4ncYGq7gzUk/l87nK1FRV28SNjTjVt27bl22+/BWDGjBnExcXxu9/9rvrx8vJywsPr/jjLzMwkMzPzqM+xePHik1PsKShgLQIR+TvuikO9RCRHRK4XkekiMt2bZS6wBXcFpmdxV10KGAsCY04vU6dO5fbbb2f06NHceeedfPPNNwwbNowBAwYwbNgwNmzYABz5DX3GjBlMmzaNUaNGkZ6ezsyZM6vXFxcXVz3/qFGjuPzyy+nduzdTpkyhapTmuXPn0rt3b0aMGMGtt9561G/+P/74IxMnTuSss85i6NChrFy5EoBPP/20ukUzYMAAioqK2LlzJyNHjiQjI4MzzzyTzz777KRvs/oErEWgqpOP8rjiLgnYJCwIjDk5brvttupv5ydLRkYGjz/++DEvt3HjRubNm4fP56OwsJBFixYRHh7OvHnzuPvuu3njjTdqLbN+/Xo++eQTioqK6NWrFzfddFOtY+5XrFjBmjVr6NixI8OHD+eLL74gMzOTX/7ylyxatIi0tDQmT27wIw6A+++/nwEDBvD222+zYMECrrnmGr799lseffRRnnzySYYPH87+/fuJjo5m1qxZXHjhhdxzzz1UVFRQXFx8zNvjeJ1yg84dLwsCY04/V1xxRfX/dkFBAddeey2bNm1CRCgrK6tzmYsuuoioqCiioqJITk5m9+7dpKSkHDHPkCFDqqdlZGSQlZVFXFwc6enp1cfnT548mVmzZjVY3+eff14dRueddx55eXkUFBQwfPhwbr/9dqZMmcKkSZNISUlh8ODBTJs2jbKyMiZOnEhGRsYJbZtjYUFgjDkmx/PNPVBiY2Orb997772MHj2at956i6ysLEaNGlXnMlFRUdW3fT4f5eXljZrneC7iVdcyIsJdd93FRRddxNy5cxk6dCjz5s1j5MiRLFq0iPfee4+rr76aO+64g2uuueaYn/N4hMxYQxYExpzeCgoK6NTJnYr04osvnvT19+7dmy1btpCVlQXAnDlzjrrMyJEjefXVVwG37yExMZGWLVvy/fff069fP+68804yMzNZv3492dnZJCcnc8MNN3D99dezfHnTjbhjLQJjzGnh97//Pddeey2PPfYY55133klff4sWLXjqqacYO3YsiYmJDBky5KjLzJgxg+uuu46zzjqLmJgY/va3vwGuVfXJJ5/g8/no27cv48aNY/bs2TzyyCNEREQQFxfHSy/VNzrPyXfKXbM4MzNTj+fCNJ999hkjR47k448/5vzzzw9AZcacvtatW0efPn2CXUbQ7d+/n7i4OFSVX//61/To0YPf/OY3wS6rlrreLxFZpqp1HkdrXUPGGNNIzz77LBkZGZxxxhkUFBTwy1/+MtglnRTWNWSMMY30m9/8plm2AE6UtQiMMSbEWRAYY0yIsyAwxpgQF3JBUFlZGeRKjDGmeQm5ILAWgTGnnlGjRvHhhx8eMe3xxx/nV7+qf6zKUaNGUXWo+fjx48nPz681z4wZM3j00UcbfO63336btWvXVt+/7777mDdv3rGUX6fmNFx1yARBWJh7qRYExpx6Jk+ezOzZs4+YNnv27EYN/AZu1NDWrVsf13PXDIIHHnjgtDsXKWSCwFoExpy6Lr/8ct59910OHjwIQFZWFjt27GDEiBHcdNNNZGZmcsYZZ3D//ffXuXxqaip79+4F4OGHH6ZXr16cf/751UNVgztHYPDgwfTv35/LLruM4uJiFi9ezDvvvMMdd9xBRkYG33//PVOnTuWf//wnAPPnz2fAgAH069ePadOmVdeXmprK/fffz8CBA+nXrx/r169v8PUFe7hqO4/AGHNMbrsNTvIo1GRkQENj2bVt25YhQ4bwwQcfcMkllzB79myuvPJKRISHH36YhIQEKioqGDNmDCtXruSss86qcz3Lli1j9uzZrFixgvLycgYOHMigQYMAmDRpEjfccAMAf/jDH3juuee45ZZbmDBhAhdffDGXX375EesqLS1l6tSpzJ8/n549e3LNNdfw9NNPc9tttwGQmJjI8uXLeeqpp3j00Uf561//Wu/rC/Zw1dYiMMacEvy7h/y7hV5//XUGDhzIgAEDWLNmzRHdODV99tlnXHrppcTExNCyZUsmTJhQ/djq1as555xz6NevH6+++ipr1qxpsJ4NGzaQlpZGz549Abj22mtZtGhR9eOTJk0CYNCgQdUD1dXn888/5+qrrwbqHq565syZ5OfnEx4ezuDBg3nhhReYMWMGq1atIj4+vsF1N4a1CIwxxyRYo1BPnDiR22+/neXLl1NSUsLAgQPZunUrjz76KEuWLKFNmzZMnTqV0tLSBtcjInVOnzp1Km+//Tb9+/fnxRdfZOHChQ2u52jjtFUNZV3fUNdHW1dTDldtLQJjzCkhLi6OUaNGMW3atOrWQGFhIbGxsbRq1Yrdu3fz/vvvN7iOkSNH8tZbb1FSUkJRURH/+te/qh8rKiqiQ4cOlJWVVQ8dDRAfH09RUVGtdfXu3ZusrCw2b94MwMsvv8y55557XK8t2MNVW4vAGHPKmDx5MpMmTaruIurfvz8DBgzgjDPOID09neHDhze4/MCBA7nyyivJyMiga9eunHPOOdWPPfjgg5x99tl07dqVfv36VX/4X3XVVdxwww3MnDmzeicxQHR0NC+88AJXXHEF5eXlDB48mOnTp9d6zsYI9nDVITMM9b59+0hISOAvf/lL9c4cY0zj2DDUpxYbhroe1iIwxpi6WRAYY0yIsyAwxjTKqdaNHKqO532yIDDGHFV0dDR5eXkWBs2cqpKXl0d0dPQxLWdHDRljjiolJYWcnBxyc3ODXYo5iujoaFJSUo5pmZAJAht0zpjjFxERQVpaWrDLMAESMl1D4FoFFgTGGHMkCwJjjAlxFgTGGBPiLAiMMSbEWRAYY0yIsyAwxpgQZ0FgjDEhLqBBICJjRWSDiGwWkbvqeLyViPxLRL4TkTUicl0g67EgMMaY2gIWBCLiA54ExgF9gcki0rfGbL8G1qpqf2AU8GcRiQxUTRYExhhTWyBbBEOAzaq6RVUPAbOBS2rMo0C8uGvHxQE/Ag1f0+0EWBAYY0xtgQyCTsA2v/s53jR/TwB9gB3AKuDfVbWy5opE5EYRWSoiS09krBMLAmOMqS2QQVDXFaJrDl14IfAt0BHIAJ4QkZa1FlKdpaqZqpqZlJR03AX5fD4qK2vljDHGhLRABkEO0Nnvfgrum7+/64A31dkMbAV6B6ogaxEYY0xtgQyCJUAPEUnzdgBfBbxTY54fgDEAItIO6AVsCVRBFgTGGFNbwIahVtVyEbkZ+BDwAc+r6hoRme49/gzwIPCiiKzCdSXdqap7A1WTBYExxtQW0OsRqOpcYG6Nac/43d4BXBDIGvxZEBhjTG12ZrExxoQ4CwJjjAlxFgTGGBPiQioIwsLCLAiMMaaGkAoCaxEYY0xtFgTGGBPiLAiMMSbEWRAYY0yIsyAwxpgQZ0FgjDEhzoLAGGNCnAWBMcaEOAsCY4wJcRYExhgT4iwIjDEmxFkQGGNMiLMgMMaYEGdBYIwxIc6CwBhjQpwFgTHGhDgLAmOMCXEWBMYYE+IsCIwxJsSFXBBUVlYGuwxjjGlWQi4IrEVgjDFHsiAwxpgQF3JBoKqoarBLMcaYZiPkggCwVoExxvixIDDGmBBnQWCMMSHOgsAYY0KcBYExxoS4gAaBiIwVkQ0isllE7qpnnlEi8q2IrBGRTwNZjwWBMcbUFh6oFYuID3gS+CmQAywRkXdUda3fPK2Bp4CxqvqDiCQHqh6wIDDGmLoEskUwBNisqltU9RAwG7ikxjy/AN5U1R8AVHVPAOuxIDDGmDoEMgg6Adv87ud40/z1BNqIyEIRWSYi19S1IhG5UUSWisjS3Nzc4y7IgsAYY2oLZBBIHdNqntIbDgwCLgIuBO4VkZ61FlKdpaqZqpqZlJR03AWFhbmXa0FgjDGHBWwfAa4F0Nnvfgqwo4559qrqAeCAiCwC+gMbA1GQtQiMMaa2QLYIlgA9RCRNRCKBq4B3aszzv8A5IhIuIjHA2cC6QBVkQWCMMbUFrEWgquUicjPwIeADnlfVNSIy3Xv8GVVdJyIfACuBSuCvqro6UDVZEBhjTG2B7BpCVecCc2tMe6bG/UeARwJZRxULAmOMqc3OLDbGmBBnQWCMMSHOgsAYY0KcBYExxoS4RgWBiMSKSJh3u6eITBCRiMCWdvJZEBhjTG2NbREsAqJFpBMwH7gOeDFQRQWKBYExxtTW2CAQVS0GJgH/raqXAn0DV1ZgWBAYY0xtjQ4CEfkJMAV4z5sW0HMQAsGCwBhjamtsENwG/B/gLe/s4HTgk8CVFRgWBMYYU1ujvtWr6qfApwDeTuO9qnprIAsLhKogqKysDHIlxhjTfDT2qKHXRKSliMQCa4ENInJHYEs7+axFYIwxtTW2a6ivqhYCE3FjB3UBrg5YVQFiQWCMMbU1NggivPMGJgL/q6pl1L7ITLNnQWCMMbU1Ngj+B8gCYoFFItIVKAxUUYFiQWCMMbU1dmfxTGCm36RsERkdmJICx4LAGGNqa+zO4lYi8ljVBeRF5M+41sEpxYLAGGNqa2zX0PNAEfBz76cQeCFQRQWKBYExxtTW2LODu6nqZX73/0NEvg1EQYFkQWCMMbU1tkVQIiIjqu6IyHCgJDAlBY4FgTHG1NbYFsF04CURaeXd3wdcG5iSAseCwBhjamvsUUPfAf1FpKV3v1BEbgNWBrK4k82CwBhjajumK5SpaqF3hjHA7QGoJ6AsCIwxprYTuVSlnLQqmogFgTHG1HYiQWBDTBhjzGmgwX0EIlJE3R/4ArQISEUBZEFgjDG1NRgEqhrfVIU0BQsCY4yp7US6hk45FgTGGFObBYExxoS4kAoCEXegkwWBMcYcFnJBEBYWZkFgjDF+QioIwHUPWRAYY8xhFgTGGBPiAhoEIjJWRDaIyGYRuauB+QaLSIWIXB7IesCCwBhjagpYEIiID3gSGAf0BSaLSN965vsT8GGgavFnQWCMMUcKZItgCLBZVbeo6iFgNnBJHfPdArwB7AlgLdUsCIwx5kiBDIJOwDa/+znetGoi0gm4FHimoRWJyI1V10vOzc09oaIsCIwx5kiBDIK6RietOW7R48CdqtrgJ7OqzlLVTFXNTEpKOqGifD4flZWVJ7QOY4w5nTT2CmXHIwfo7Hc/BdhRY55MYLZ3olciMF5EylX17UAVZS0CY4w5UiCDYAnQQ0TSgO3AVcAv/GdQ1bSq2yLyIvBuIEMALAiMMaamgHUNqWo5cDPuaKB1wOuqukZEpovI9EA9b30WLoQxY0C1vQWBMcb4CWSLAFWdC8ytMa3OHcOqOjWQtZSWwoIF0LFjqgWBMcb4CZkzi1NT3e/Kyi4WBMYY4ydkgqBrV/e7oqKzBYExxvgJmSBo0QLatYPy8k4WBMYY4ydkggBcq6CsLMWCwBhj/IRUEKSmQkVFCid6drIxxpxOQi4IDh5sx8qVq1GteZKzMcaEppALgsrKCPbvjyU7OzvY5RhjTLMQckHg3WLlypVBrMQYY5qPEA2CNL777rsgVmKMMc1HSAVBly7ud0LCQGsRGGOMJ6SCIDYWkpKgZcuzLAiMMcYTUkEA0KMHlJf3ZtOmTRQXFwe7HGOMCbqQC4JzzoGdOzuh2oJVq1YFuxxjjAm6kAuC0aOhoiIMGM7y5cuDXY4xxgRdQIehbo6GD4fwcCU8fDzLli0LdjnGGBN0IdciiIuDIUOEqKgLLQiMMYYQDAJw3UOFhT1ZtSqL0tLSYJdjjDFBFZJBcO65oOqjoiLTdhgbY0JeSAbBoEFVtzKse8gYE/JCMggSEqBLFyUy8myWLl0a7HKMMSaoQjIIADIyhIiIIdYiMMaEvJANggED4MCBFFat2mI7jI0xIS1kgyAjAyCMioo+tsPYGBPSQjYIBgyoumU7jI0xoS1kg6BLF2jTRomKGmpBYIwJaSEbBCJuh3FU1FA7csgYE9JCNgjA7ScoLu7GqlXrbIexMSZkhXQQDBgA5eURVFSk24VqjDEhK+SDAEBkEO+++25wizHGmCAJ6SDo1QuioiAl5WLmzJmDqga7JGOMaXIhHQQREdCvH8TGDmfjxo3WPWSMCUkhHQTgdhjv3t2RsDAfc+bMCXY5xhjT5AIaBCIyVkQ2iMhmEbmrjseniMhK72exiPQPZD11GTAA9u0LY8SIq6x7yBgTkgIWBCLiA54ExgF9gcki0rfGbFuBc1X1LOBBYFag6qnPwIHud58+17NlyxY7ucwYE3IC2SIYAmxW1S2qegiYDVziP4OqLlbVfd7dr4CUANZTp8GDISkJ9uwZRnh4OK+//npTl2CMMUEVyCDoBGzzu5/jTavP9cD7dT0gIjeKyFIRWZqbm3sSSwSfDyZNgo8+imLMmIt5/fXXrXvIGBNSAhkEUse0Oj9hRWQ0LgjurOtxVZ2lqpmqmpmUlHQSS3QuuwwOHIBevW4lOzubr7/++qQ/hzHGNFeBDIIcoLPf/RRgR82ZROQs4K/AJaqaF8B66jVqlLtq2a5dw4mMjLSjh4wxISWQQbAE6CEiaSISCVwFvOM/g4h0Ad4ErlbVjQGspUERETBxInzwQSQXXPAz/vGPf1BZWRmscowxpkkFLAhUtRy4GfgQWAe8rqprRGS6iEz3ZrsPaAs8JSLfikjQhgG9/HIoLITevW9m+/btLF68OFilGGNMkwoP5MpVdS4wt8a0Z/xu/xvwb4GsobHGjIFWrWDnzuFER0czZ84cRowYEeyyjDEm4EL+zOIqkZEwYQLMnRvBRRdN5KWXXmLLli3BLssYYwLOgsDP5ZfDvn0wfvxf8Pl8XH755ZSUlAS7LGOMCSgLAj8XXggdO8Irr7Tn5ZdfZsWKFdxyyy3BLssYYwLKgsBPVBTcfjt88gkkJl7E3XffzXPPPcdf/vIX9uzZE+zyjDEmICwIarjxRmjTBh5+GB544AHGjBnD7bffTrt27bjvvvuCXZ4xxpx0FgQ1xMfD734H//oXzJ/v47333uP999/nqquu4sEHH2TatGl06dKFhx9+ONilGmPMSSGn2rg6mZmZunRpYE83OHjQXbBGFZ59Fjp3hi5dyhg/fjzz5s0nIeE2fvzxLT7++FnOP//8gNZijDEng4gsU9XMOh+zIKjbvHnw05+625GRMH8+dO9eyuTJ5SxcGEdc3KdERFzKzTffTHx8PAcOHOCOO+4gNjY24LUZY8yxsiA4TosXw/79cMstkJvrWghFRTBsGHz2GZxzzg18/vlzqIYDZzN8uDB37ru0bNmySeozxpjGaigIbB9BA4YNgwsugPfec2cdn3suLFsGr7/uWglnnfUs27dv55Zb8oHP+OKL/6ZHj6t4+umn2bdvHytXrmTmzJmc7KGzjTHmZLIWwXG69lr45z9hzhy44gp3gZu1aw9SUpJNcXEvfD4fFRUVACQnJ/PHP/6RSy+9lNatWwe5cmNMKLIWQQA8+CAkJsLPfgaVlfDSS/Cf/xlFcXFPZs1aw5133sWNN77Oz3++neTkXkybNo127dpx7733UlpaWr2eBQsWMH78eD7++OMgvhpjTCizFsEJ2LIFxo+HK6+E//gPd3GbTp1g6FAoLYVPP3Xz3XCDMm3a1zzxxBO8+uqrAERGRhIVFUVRURFhYWFER0fz4YcfMmzYMMLCLJ+NMSeX7SwOIFUQv2ux3XYb/Nd/QcuW8H//L6xbB08+Ce+/D4MGwapVn/Dpp59y8OBBSktLSU1NZeLEiYwePZqtW7cSERHBqFGj6NOnD/Pnz2fixIk88MADFg7GmBNiQdCEdu2Cxx6Dm2+GLl3cNQ769IEd3rXZfvtb+OMfITwc1qyBggK3U3rHjh288cYbZGVl8dZbb7FtWw7t2z9ITs7fGDu2K+np6axZs4YffviBc889l/Hjx3PBBRfQqlWr4L5gY8wpwYIgyLKy3PhFixfDX/8KPXq4LqSFC93j48fDQw/BgAHuvqryj3+Uc+WVEXTsuJOSkjMRgW7dutGpUycWLlxIfn4+4eHhDB8+nAEDBtCqVavqn+TkZC688EIiIiKC9ZKNMc2MBUEz8tpr8OqrLhwuu8wdlvrAA67lMHy4Gwp7xAh3JNLeve48hjffhEsvPbyO8vJyvvrqK+bOncvcuXPZvHkzBw4cOOJ5hgwZwt13301RURHdu3enW7duqCqJiYlUVFSwevVqevbsaSfAGRMiLAiaufx8N5TFS0Y8wVYAABOSSURBVC/B6tWHp3/8Mfz6124/xB/+AOPGQVKSu2ZCixYQHX143qKiChYsKKZVq0K2b1/IzTffTH5+fq3nio2NxefzUVhYSOfOnbn//vtJT0+nb9++tGvXrglerTEmGCwITiFbt8JXX0FYmDsa6YMP4Be/cB/+kZHQv787qS0pCX7/e/j5z10LY8YMd6RSVBS8/DKMGpXLxo0bSUhIYP369Wzfvh1VZfPmzZSWljJ48GBmzpzJqlWrqp+7Y8eOhIeHk5GRwaBBg3jrrbeIiIjg/PPP58orr6RXr14UFBSQnJyM+O8hN8Y0exYEp7jKSvjuO3jxRfjyS3d95W++gQULDs9z6aUwdSr86U9uX0RcnDsr+vnnobgYNm6ElBRITz98lFNZWTnr1q0jN3cPy5cvZ+3atZSVlbFgwQJ27tzJ2WefTUREBF9++WX1yXHgTpD7yU9+wsCBAzlw4ACtW7dmwoQJpKenEx0dTV5eHo8++ihZWVk89thjdOzYsUm3lzGmNguC09Tq1e6w1G7dXBCIuFbBs8/C2rVux3T79rBnDxw65JZJT3cBERHhWhv5+S4skpLg88/dYa8DBlRQWbmPP/85kREjYPLkvbzxxhvk5eURGxvLihUrWLx4MZs2bSIiIoKysrJatYkIkZGRxMfHc9VVV9GmTRuys7Pp06cPF110EW3atKFFixYA5OTk0KpVK7p27WotDWMCxIIgRC1YANOnw/nnwyWXuB3Ub7wBS5a4YBg6FPLyXGujJhG3byI83M3fv78Lig8+gJ/8xA3TXVBQSu/eUeTm7uCjjz5i9+7dFBcXExUVxciRV5CVFcnzz09j6dIlHDhwgPbt27Nz5856623ZsiVxcXGkpqYyfPhwRISEhASGDh1KZmYmqsrXX3/N/v37adu2LUOGDOGHH35g9+7d9O/fn7i4uMBtTGNOcRYEpl4lJfD445CQAJMmuW6kjz+GTZtgyhTXeoiJcYGwaVPt5Vu0gJEjYcIE9/ihQ3DOOe6Snzt3upPo7rlHGTeujOjoSLKzs/niiy84cOAAJSUlVFRUkJKSQl5eHmvXriUnpyUbN2azbt3fCQ/3cchryvh8PsLCwo5ofYSHh1NeXg5AWFgYffr04YwzziA6OpqNGzeSnZ1N586dSUtLIz09nbS0NA4ePEheXh69e/emY8eOVFZWsnv3blq1asXw4cMtTMxpy4LAHLd33oGrr3atgCuucIe8fvUV5OS4cFixwl3NbetWdxRTWJgLk9RUN3z3U0/B99+7rqeICNc1lZ4Omze7S4IOHAhdu7plFy+Gp592LZH0dOWMM4TevYtJT/+GnJx5VFRUMGrUKJKTk6sDpWvX7iQkdGHjxq9ZsmQJmzZtqj5ju1u3buTk5LB161ays7OrQ6M+Pp+P7t2707VrV2JiYigoKEBVSUlJISUlhZiYGHbt2kViYiJdunQhPDycsLAwYmJi6NOnD4cOHSInJ4dDhw7Rtm1bevbsSfv27cnPz2fFihXVoRQeHn7E886ZU8qWLRXcdVeMdY2ZgLEgMAGl6nZGd+3qru72ySeuVdC2LZSXuxFa581z3U1r18IPP7iT6vLy3BAclZVuPSLujOx+/dzQ31u2uP0gqu5kuyFD3PUgRFyolJXBhx+68y1SU6FnTxc4JSWHz+R++GE47zz4/vtyrruujM6dlT/9yce99+azcaMQE1NBYmIkPXtuQ/VNVq9ezfbt2ykpKam+rsT27dvZvn07ZWVltGnThoKCAiqrij5CONADWFc9JS4ujuLiYm/+NkREVNCtm9t5XlJSQl7ehezf/xTgo0WLC2jVahUJCQn069ePcePGMXLkSLZs2UJhYSHl5eVUVFSQmppKWloaX375JeHh4fTt25eSkhJycnLIzs6muLiYDh06MGbMGNq2bWvhYgALAtOMHTrkPrTLylwLITHxyMd37HDXf3jtNdfqaNXKBUNZmQuQESMgIwO+/da1PH780bUuOnRwQbJ1qwuIXbugosK1VsLC3LJnnOEGCvzxRzfUxw03uGE/8vNdt1hamjtK6+WXoVs3ZfToCnr3DmflynLWrz9E+/ZldOxYRuvWhezbt4V//nMg33+fwKRJe/nZz9aRnb2FLVu2smvX2WzYMILs7HhiYwsZOPDPtGu3jq1bL2P58itITd3Gjz8mEB29j3Hj/siePXl8992XbN/eHmgPfAgcqmcLCjAU6AzEe9PWAiuAUnw+H3FxccTGxhIbG0t0dDRhYWGoxhEV1Zpu3eKprPSRnX2AdesWVLduxo4dS0REBD/88AMiQl5eHnv27CEhIYF27dqRnJxMu3bt2L9/Pxs2bKg+WbGqay46OprExETeffdHPvhgK6tW/ZYhQwZz22230bdvX2JiYqj67ElKSqo+C/6jj5Rbbill5Mj3GDgwlwkTJiAi+Hw+kpKSjmvMrbKyMjvLHgsCE6Kq9n+sWOE+/B9+2O3HeOIJuOsuFyLgQuXmm2HWLNdt1bGj2zEOrvvr0ktdV9g337gwiYpyR2pt3+4CpEqbNm6n/N/+5sLK3/DhMHasO+dj/XoXVqWl7kzyF190Z49fc83h+X0+paLCfZNv1eoQPXocIjZWadeujF27isjLK2PEiHAWLWrLunXx1NSqVRmZmd+QmLiB/fsr2batI4WFcRQXx1Fa2obCwi6AEB39NWVlPaioSCQuLo/27ddx8OD7bNv2EiBERV1CWdl0IiP3kpb2DCUla9ix42IOHZoKFADhiCQTE/PfFBc/gmpVS6k9cA/wKyCMbt0+Yf/+a9m9e5tfldGAEhUVwZln9iU6ujeLF/8XqlWv5yHgP4FIIJKIiCI6dOhISkoK7du3o6CggF27dpGbm0t6ejpdunRh9+7dlJeXVwfGpk2b2LVrFykpKfTo0YPu3bvTvXt3du3axXfffUdxcTGpqd3p0WMy0dEr2bcvD5/Ph8/nIzw8nIiICCIiIujQoQOjR4+msLCQtWvXsmHDBuLj40lLS6Nr164cOHCA3NxcRISwsDDCwsKq92sd7f6+fZG88koy55+/lejoHAoKChgxYgS9evVCVatfY1JSEklJSbW6FhvLgsCYo1B13VS9eoHP51oQ+fmuhVLVSjl0yHVrpaQcPqs7Px+ys11QDBrkDtddssS1LFTdT79+7sJF4IYMefZZN3+fPnD99a6rq7ISZs50LZbwcHcC4Zlnup34L7/sQqewELZtc+eIREa6UGvf3gXckCGutVReDitXwnPPuX03VSIiXCspOdn9DBrkavvHP1wdw4e71s/Cha6F5C8jw73GffsOTxsxooKwsIOEh4dRVhbFZ58JXbpUUlJSQVRUBbt3R1JRIVxxRS5durTlkUd8tGihdOq0nwMHKti3L4bS0sha70Nk5H7+8IeP+fbbC3nzzRhiY4spLo5GNYyoqBLKyiKAMiIisomK2k/LliW0a7eX3buLyc9vhUgqFRUtCQsrJTl5Pm3bxhMR0YWysiwKC1exa9e3FBSUER6eQdu2w4mN3U929lAqKvoAX+DzfUll5UjgY1Rfw4XdpUAb4J/AWUA68A2wEUgALgO2AK8DxbjgaodrxbUAYoHvgcPXIQEfkAKMxoVdB2AbcA6QDbQkOXkkBQXtOHhQgXKgnClTBvDKK787yl9z3SwIjDkN7dzpPvxjYup+fNcut+8mPNztlPcfkqQ+lZUuSBYtcmGTkQFnn+3258yZ48IoMxN++tMjl3nqKbdMfLzbT5Sc7IZH6dbNzfPhh+6cl02bXL0dO7oQE3GtrMpK9zNxojtUGdzRa7NmuaBq3dp19cXHu5bUpk2uNbZtmwupiAi3jyo11e2bys52BzWAC3a/8yGrxce7fU5pacrYsXuYPTuJwsIw+vd3rUj/j0YRRbXhfS1hYUpMTAXFxT4qK6XWY23aHCIqqoKionCKig6HYEpKET//+XpmzcrgwIFwYmMr2b/fV+dzTJmyjVde6dxgHfWxIDDGnLb273eHMftqfHZmZbnwS052J1VmZ7vfMTHQvbsbJr6kxHX1+XxuPQcPuiDZssUFyd69MGqUa5m9/bZrpfXvD0uXuvWJuO7Adevgo49cOCUkuFbjoUOHxwRbt87t7zpwwD2enOyC8Oyz4ayzXNflmjVuf1h+vgvK3r3d/q3YWNfSq6hwgZiUdHzbyYLAGGNCnF2z2BhjTL0CGgQiMlZENojIZhG5q47HRURmeo+vFJGBgazHGGNMbQELAhHxAU8C44C+wGQR6VtjtnG4M3B6ADcCTweqHmOMMXULZItgCLBZVbeo6iFgNnBJjXkuAV5S5yugtYh0CGBNxhhjaghkEHTCHRhbJcebdqzzICI3ishSEVmam5t70gs1xphQFsggqOug25qHKDVmHlR1lqpmqmpm0vEeO2WMMaZOgQyCHNwAKFVSgB3HMY8xxpgACmQQLAF6iEiaiEQCVwHv1JjnHeAa7+ihoUCBqtZ/5RJjjDEn3fGNXtQIqlouIjfjhk70Ac+r6hoRme49/gwwFxgPbMYN0nHd0da7bNmyvSKSfRwlJQJ7j2O5QLO6jl1zrc3qOjbNtS5ovrWdSF1d63vglDuz+HiJyNL6zqoLJqvr2DXX2qyuY9Nc64LmW1ug6rIzi40xJsRZEBhjTIgLpSCYFewC6mF1HbvmWpvVdWyaa13QfGsLSF0hs4/AGGNM3UKpRWCMMaYOFgTGGBPiTvsgONpQ2E1cS2cR+URE1onIGhH5d2/6DBHZLiLfej/jg1Bblois8p5/qTctQUQ+FpFN3u82TVxTL79t8q2IFIrIbcHYXiLyvIjsEZHVftPq3T4i8n+8v7kNInJhEGp7RETWe8O7vyUirb3pqSJS4rftnmniuup975pqm9VT1xy/mrJE5FtvelNur/o+HwL/d6aqp+0P7kS273FXm44EvgP6BrGeDsBA73Y87urXfYEZwO+CvK2ygMQa0/4fcJd3+y7gT0F+L3fhTopp8u0FjAQGAquPtn289/Q7IApI8/4GfU1c2wVAuHf7T361pfrPF4RtVud715TbrK66ajz+Z+C+IGyv+j4fAv53drq3CBozFHaTUdWdqrrcu10ErKOO0VabkUuAv3m3/wZMDGItY4DvVfV4zio/Yaq6CPixxuT6ts8lwGxVPaiqW3Fnzg9pytpU9SNVLffufoUbx6tJ1bPN6tNk26yhukREgJ8Dfw/Eczekgc+HgP+dne5B0KhhroNBRFKBAcDX3qSbvWb8803dBeNR4CMRWSYiN3rT2qk39pP3OzkIdVW5iiP/OYO9vaD+7dPc/u6mAe/73U8TkRUi8qmInBOEeup675rLNjsH2K2qm/ymNfn2qvH5EPC/s9M9CBo1zHVTE5E44A3gNlUtxF2ZrRuQAezENU2b2nBVHYi7atyvRWRkEGqok7hBCycA//AmNYft1ZBm83cnIvcA5cCr3qSdQBdVHQDcDrwmIi2bsKT63rvmss0mc+QXjibfXnV8PtQ7ax3Tjmubne5B0OyGuRaRCNyb/KqqvgmgqrtVtUJVK4FnCWA3Qn1UdYf3ew/wllfDbvGuGOf93tPUdXnGActVdbdXY9C3l6e+7dMs/u5E5FrgYmCKep3KXjdCnnd7Ga5fuWdT1dTAexf0bSYi4cAkYE7VtKbeXnV9PtAEf2enexA0ZijsJuP1Pz4HrFPVx/ym+1+e81Jgdc1lA1xXrIjEV93G7WhcjdtW13qzXQv8b1PW5eeIb2nB3l5+6ts+7wBXiUiUiKThrsn9TVMWJiJjgTuBCapa7Dc9Sdz1xBGRdK+2LU1YV33vXdC3GXA+sF5Vc6omNOX2qu/zgab4O2uKveHB/MENc70Rl+T3BLmWEbim20rgW+9nPPAysMqb/g7QoYnrSscdffAdsKZqOwFtgfnAJu93QhC2WQyQB7Tym9bk2wsXRDuBMtw3sesb2j7APd7f3AZgXBBq24zrP676O3vGm/cy7z3+DlgO/KyJ66r3vWuqbVZXXd70F4HpNeZtyu1V3+dDwP/ObIgJY4wJcad715AxxpijsCAwxpgQZ0FgjDEhzoLAGGNCnAWBMcaEOAsCYzwiUiFHjnZ60kar9UaxDNb5DsY0KDzYBRjTjJSoakawizCmqVmLwJij8Man/5OIfOP9dPemdxWR+d4AavNFpIs3vZ24awB85/0M81blE5FnvbHmPxKRFt78t4rIWm89s4P0Mk0IsyAw5rAWNbqGrvR7rFBVhwBPAI97054AXlLVs3CDus30ps8EPlXV/rhx79d403sAT6rqGUA+7qxVcGPMD/DWMz1QL86Y+tiZxcZ4RGS/qsbVMT0LOE9Vt3iDgu1S1bYishc3REKZN32nqiaKSC6QoqoH/daRCnysqj28+3cCEar6kIh8AOwH3gbeVtX9AX6pxhzBWgTGNI7Wc7u+eepy0O92BYf30V0EPAkMApZ5o2Aa02QsCIxpnCv9fn/p3V6MG9EWYArwuXd7PnATgIj4Ghq/XkTCgM6q+gnwe6A1UKtVYkwg2TcPYw5rId5Fyz0fqGrVIaRRIvI17svTZG/arcDzInIHkAtc503/d2CWiFyP++Z/E260y7r4gFdEpBXuQiN/UdX8k/aKjGkE20dgzFF4+wgyVXVvsGsxJhCsa8gYY0KctQiMMSbEWYvAGGNCnAWBMcaEOAsCY4wJcRYExhgT4iwIjDEmxP3/67ZkxAKaWfkAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "history_dict = history.history\n",
    "print(history_dict.keys())\n",
    "\n",
    "loss_values = history_dict['loss']\n",
    "val_loss_values = history_dict['val_loss']\n",
    "\n",
    "epochs = range(1, len(loss_values) + 1)\n",
    "\n",
    "plt.plot(epochs, loss_values, 'b', color='k', label='Training loss')\n",
    "plt.plot(epochs, val_loss_values, 'b', color='b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
